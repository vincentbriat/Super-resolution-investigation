{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2b9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the code is in a colab notebook\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624efb2d",
   "metadata": {},
   "source": [
    "Run on Colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a87d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install basicsr\n",
    "  drive.mount('/content/drive/')\n",
    "  !unzip /content/drive/MyDrive/Datasets/DIV2K_valid_HR.zip\n",
    "  !unzip /content/drive/MyDrive/Datasets/DIV2K_valid_LR_clean.zip\n",
    "  !unzip /content/drive/MyDrive/Datasets/DIV2K_train_LR_clean.zip\n",
    "  #!unzip /content/drive/MyDrive/Datasets/DIV2K_train_HR.zip\n",
    "  !unzip /content/drive/MyDrive/ML/Indiv_Project/Second_Year/Pruning/80_percent/gen_images_0.zip\n",
    "  FOLDER_LR_TEST = 'DIV2K_valid_LR_clean'\n",
    "  FOLDER_HR_TEST = 'DIV2K_valid_HR'\n",
    "  FOLDER_LR_TRAIN = 'DIV2K_train_LR_clean'\n",
    "  #FOLDER_HR_TRAIN = 'DIV2K_train_HR'\n",
    "  FOLDER_GEN_IMAGES = 'gen_images_0'\n",
    "\n",
    "  STUDENT_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/student.pth'\n",
    "  STUDENT_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/student.csv'\n",
    "  GENERATOR_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/generator.pth'\n",
    "  GENERATOR_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/generator.csv'\n",
    "  DISCRIMINATOR_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/Pruning/80_percent/discriminator.pth'\n",
    "  DISCRIMINATOR_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/Pruning/80_percent/discriminator.csv'\n",
    "  \n",
    "  PRUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son.pth'\n",
    "  FINE_TUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son_fine.pth'\n",
    "\n",
    "  TEACHER_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/ESRGAN_models/RealESRGAN_x4plus.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ace5b6",
   "metadata": {
    "id": "GqUMHJXSUzfi"
   },
   "source": [
    "Run on my Windows desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897a0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "  FOLDER_LR_TEST = 'D:\\Downloads\\Div2k\\DIV2K_valid_LR_clean'\n",
    "  FOLDER_HR_TEST = 'D:\\Downloads\\Div2k\\DIV2K_valid_HR'\n",
    "  FOLDER_LR_TRAIN = 'D:\\Downloads\\Div2k\\DIV2K_train_LR_clean'\n",
    "  FOLDER_HR_TRAIN = 'D:\\Downloads\\Div2k\\DIV2K_train_HR'\n",
    "  \n",
    "  STUDENT_MODEL_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\student.pth'\n",
    "  STUDENT_RECORDS_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\student.csv'\n",
    "  GENERATOR_MODEL_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\generator.pth'\n",
    "  GENERATOR_RECORDS_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\generator.csv'\n",
    "  DISCRIMINATOR_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\discriminator.pth'\n",
    "  DISCRIMINATOR_MODEL_PATH_24 = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\discriminator_24.pth'\n",
    "  DISCRIMINATOR_RECORDS_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\discriminator.csv'\n",
    "  FOLDER_GEN_IMAGES = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\gen_images_0'\n",
    "  \n",
    "  PRUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son.pth'\n",
    "  FINE_TUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son_fine.pth'\n",
    "  \n",
    "  TEACHER_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\ESRGAN_models\\RealESRGAN_x4plus.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395215a",
   "metadata": {
    "id": "PLmP_9ta1bdM"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b8843d",
   "metadata": {
    "id": "PLmP_9ta1bdM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\anaconda3\\envs\\ai-robotics\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "import torchvision\n",
    "from os import listdir, environ, path\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "# Making sure to use the gpu, if available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_device(torch.device(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ccdcb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, use_act: bool, **kwargs):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "    self.activation = nn.LeakyReLU(.2, inplace=True) if use_act else nn.Identity()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.activation(self.conv(x))\n",
    "\n",
    "class RDB(nn.Module):\n",
    "  def __init__(self, in_channels, middle_channels = 32, residual_scale = .2):\n",
    "    super().__init__()\n",
    "    self.residual_scale = residual_scale\n",
    "    self.block = nn.ModuleList([ConvBlock(in_channels + i * middle_channels,\n",
    "                                  middle_channels if i<4 else in_channels,\n",
    "                                  3,\n",
    "                                  stride=1,\n",
    "                                  padding=1,\n",
    "                                  use_act=i<4) for i in range(5)])\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = x\n",
    "    for conv in self.block:\n",
    "      out = conv(input)\n",
    "      input = torch.cat([input, out], dim=1)\n",
    "    return self.residual_scale * out + x\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "  def __init__(self, in_channels, mid_channels = 32, residual_scale = .2):\n",
    "    super().__init__()\n",
    "    self.residual_scale = residual_scale\n",
    "    self.model = nn.Sequential(*[RDB(in_channels, middle_channels = mid_channels, residual_scale = residual_scale) for _ in range(3)])\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.model(x) * self.residual_scale + x\n",
    "\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, out_channels = 64) -> None:\n",
    "    super().__init__()\n",
    "    self.model = nn.Conv2d(3, out_channels, 3, stride=1, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "class Tail(nn.Module):\n",
    "  def __init__(self, in_channels = 64) -> None:\n",
    "    super().__init__()\n",
    "    self.model = nn.Sequential(nn.Conv2d(in_channels, 256, 3, stride=1, padding=1),\n",
    "                               nn.Upsample(scale_factor=4, mode='nearest'),\n",
    "                               nn.LeakyReLU(.2, inplace=True),\n",
    "                               nn.Conv2d(256, 3, 3, stride=1, padding=1))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "# Original code for ESRGAN:\n",
    "# https://github.com/xinntao/ESRGAN/blob/master/RRDBNet_arch.py\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_layer(block, n_layers, **kwargs):\n",
    "    layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(block(**kwargs))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResidualDenseBlock_5C(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32, bias=True):\n",
    "        super(ResidualDenseBlock_5C, self).__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # initialization\n",
    "        # mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    '''Residual in Residual Dense Block'''\n",
    "\n",
    "    def __init__(self, nf, gc=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.RDB1 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB2 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB3 = ResidualDenseBlock_5C(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.RDB1(x)\n",
    "        out = self.RDB2(out)\n",
    "        out = self.RDB3(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=23, gc=32):\n",
    "        super(RRDBNet, self).__init__()\n",
    "\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
    "        self.RRDB_trunk = make_layer(RRDB, nb, nf=nf, gc=gc)\n",
    "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        #### upsampling\n",
    "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv_first(x)\n",
    "        trunk = self.trunk_conv(self.RRDB_trunk(feat))\n",
    "        feat = feat + trunk\n",
    "\n",
    "        feat = self.lrelu(self.upconv1(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        feat = self.lrelu(self.upconv2(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        out = self.conv_last(self.lrelu(self.HRconv(feat)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf3b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KEval(object):\n",
    "    def __init__(self, folder, res_size: int = 48):\n",
    "        self.folder = folder\n",
    "        self.image_names = sorted(listdir(folder))\n",
    "        self.len = len(self.image_names)\n",
    "        self.resize = torchvision.transforms.Resize(res_size)\n",
    "        \n",
    "    def get_image(self, index: int):\n",
    "        image = Image.open(Path(self.folder).joinpath(self.image_names[index]))\n",
    "        return torchvision.transforms.ToTensor()(image)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        # Get the right target image because we will use it resized version as input for the model\n",
    "        image = self.get_image(index)\n",
    "        return (self.resize(image),)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def restrict_size(self, size):\n",
    "        if size < len(self.image_names) and size > 0:\n",
    "            self.len = size\n",
    "        else:\n",
    "            self.len = len(self.image_names)\n",
    "            print(f\"Size must be between 0 and {len(self.image_names)}\")\n",
    "\n",
    "class DIV2KTrain(DIV2KEval):\n",
    "    def __init__(self,  high_res_folder, low_res_size: int = 48, hr_resize_factor = 3):\n",
    "        # We setup the HR folder and and (possible) resize as the basis for the parent class\n",
    "        #\n",
    "        # smallest dim: ~600 - We don't want to resize over 648 pixels because the smallest dimension of our\n",
    "        # training HR images is 648px. We don't want the target image to have experienced any kind of super-resolution.\n",
    "        crop_size = low_res_size*4\n",
    "        \n",
    "        # hr_target_size\n",
    "        super().__init__(high_res_folder, int(crop_size * hr_resize_factor))\n",
    "        \n",
    "        self.crop_size = crop_size\n",
    "        \n",
    "        # smallest dim:: 48\n",
    "        self.resize_lr = torchvision.transforms.Resize(low_res_size)\n",
    "\n",
    "  \n",
    "    def __getitem__(self, i):\n",
    "        # Get the right target image because we will use it resized version as input for the model\n",
    "        target = self.get_image(i)\n",
    "\n",
    "        # The images having a resolution of 2040 pixels make it hard for our cropping of 48*4=192 pixels\n",
    "        # to mimic a genuine image despite its advantage of showing great detailing.\n",
    "        # Mainly because we only use a very small area of the image:\n",
    "        #     (192*192)/(2040*1300) = 2%, in the case of an image with a smaller dimension of 1300 pixels.\n",
    "        # This is why we introduce croppings of resized images that will capture more the elements of the HR image.\n",
    "        #     (192*192)/(582*371) = 17%, if we divide the original proportions by 3.5\n",
    "        # However, there is still a compromise between the loss of information induced when resizing the HR image down\n",
    "        # and the amount of elements we retain when cropping the original image.\n",
    "        # Hence why we use multiple scales when extracting these croppins.\n",
    "\n",
    "        # This probablity is prone to decrease as the model gains more performance (with smaller learning rate)\n",
    "        if torch.rand(1).item() < .99:\n",
    "            # Resize the full image\n",
    "            target = self.resize(target)\n",
    "\n",
    "        # Crop of target image\n",
    "        x, y, h, w = torchvision.transforms.RandomCrop.get_params(target, output_size=(self.crop_size, self.crop_size))\n",
    "        target = torchvision.transforms.functional.crop(target, x, y, h, w)\n",
    "\n",
    "        # Resizing for input image\n",
    "        img = self.resize_lr(target)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e62934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.L1Loss()\n",
    "        self.vgg = vgg19(weights='DEFAULT').features[:35].eval().to(DEVICE)\n",
    "        \n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        vgg_input_features = self.vgg(X)\n",
    "        vgg_target_features = self.vgg(y)\n",
    "        return self.loss(vgg_input_features, vgg_target_features)\n",
    "    \n",
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = VGGLoss()\n",
    "        self.pixel_loss = nn.L1Loss()\n",
    "        self.cur_epoch = 0\n",
    "        self.writer = SummaryWriter(f'runs/ESRGAN_Scratch/tensorboard')\n",
    "        self.step_size = 30\n",
    "        self.opt_gamma = .9\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, step_size=self.step_size, gamma = self.opt_gamma)\n",
    "        \n",
    "    # Creates a new subfolder and assigns it for incoming metrics\n",
    "    def update_writer(self):\n",
    "        folders = sorted(listdir('runs/ESRGAN_Scratch'))\n",
    "        next_id = 0\n",
    "        for name in folders:\n",
    "            if name[:4] == 'run_' and  int(name[4:]) >= next_id:\n",
    "                next_id = int(name[4:]) + 1\n",
    "        self.writer = SummaryWriter(f'runs/ESRGAN_Scratch/run_{next_id}')\n",
    "    \n",
    "    # This function shows the super-resolution of an image from the dataloader.\n",
    "    def display_result(self, data: DIV2KEval, index:int = -1):\n",
    "        if index < 0:\n",
    "            index = torch.randint(len(data), (1,1))\n",
    "        if index > len(data):\n",
    "            print(f'Error: the given index {index} is greater than number of images {len(data)}')\n",
    "            return\n",
    "        \n",
    "        display(torchvision.transforms.ToPILImage()(self.model(data[index][0].unsqueeze(dim=0).to(DEVICE))[0].clamp_(min=0., max=1.)))\n",
    "\n",
    "    def run(self, dataloader, epochs, path_record: str = None, path_model: str = None, load_epoch: bool = False):\n",
    "        # Initializes the records file if not existant\n",
    "        if path_record and not Path(path_record).is_file():\n",
    "            self.initialize_csv(path_record, nb_batches=len(dataloader))\n",
    "            self.cur_epoch = 0\n",
    "        elif load_epoch:\n",
    "            # Loads the next epoch to continue training\n",
    "            self.load_cur_epoch(path_record)\n",
    "        \n",
    "        # Initializes the summary writer to a new subfolder \n",
    "        self.update_writer()\n",
    "\n",
    "        while self.cur_epoch < epochs:\n",
    "            print(f'Epoch: {self.cur_epoch}')\n",
    "            result = self.train_one_epoch(dataloader)\n",
    "            loss, losses = result['epoch_loss'], result['batch_losses']\n",
    "            print('****************************************\\n')\n",
    "            print(f'Total Loss: {loss}')\n",
    "\n",
    "            self.writer.add_scalar('Training Loss - Epoch', loss, global_step = self.cur_epoch)\n",
    "            \n",
    "            if path_record:\n",
    "                self.record(path_record, [self.cur_epoch, loss] + losses)\n",
    "            if path_model:\n",
    "                self.save(path_model)\n",
    "            \n",
    "            self.cur_epoch += 1\n",
    "\n",
    "    def load_cur_epoch(self, path):\n",
    "        with open(path) as records_file:\n",
    "            epoch = records_file.readlines()[-1].split(',')[0]\n",
    "            self.cur_epoch = int(epoch) + 1\n",
    "\n",
    "    def load(self, path):\n",
    "        saved_info = torch.load(path)\n",
    "        self.model.load_state_dict(saved_info['model_state_dict'])\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=saved_info['lr'])\n",
    "        self.cur_epoch = saved_info['epoch'] + 1\n",
    "        if self.scheduler:\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, step_size=self.step_size, gamma = self.opt_gamma)\n",
    "        self.loss_fn = saved_info['loss_fn']\n",
    "        self.model.to(DEVICE)\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({'model_state_dict': self.model.state_dict(),\n",
    "                'lr': self.opt.param_groups[0]['lr'],\n",
    "                'loss_fn': self.loss_fn,\n",
    "                'epoch': self.cur_epoch,\n",
    "                'lr_scheduler_state_dict': 0},\n",
    "               path)\n",
    "        print(\"Model saved successfully!\")\n",
    "\n",
    "    def initialize_csv(self, path, nb_batches: int = 0):\n",
    "        columns = ['Epoch', 'Loss'] + [f'Batch Loss {i}' for i in range(nb_batches)]\n",
    "        self.record(path, columns)\n",
    "        print(\"Records created Successfully!\")\n",
    "\n",
    "    def record(self, path, row):\n",
    "        with open(path, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow(row)\n",
    "        print(\"Model recorded successfully!\")\n",
    "\n",
    "    def train_one_epoch(self, dataloader):\n",
    "        cumu_loss = 0\n",
    "        losses = [0]*len(dataloader)\n",
    "        self.model.train()\n",
    "        self.model.to(DEVICE)\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            print('------------------------------------------')\n",
    "            out = self.model(X.to(DEVICE))\n",
    "            loss = self.learn(out, y)\n",
    "            losses[batch] = loss\n",
    "            cumu_loss += loss\n",
    "            \n",
    "            print(\n",
    "                f'Training batch {batch} with loss {loss:.5f}')\n",
    "            \n",
    "            # Plot metrics to tensorboard\n",
    "            self.writer.add_scalar('Training Loss - Batches', loss, global_step = self.cur_epoch * len(dataloader) + batch)\n",
    "            self.writer.add_scalar(f'Training Loss - Batch n{batch}', loss, global_step = self.cur_epoch)\n",
    "            out_grid = torchvision.utils.make_grid(out.clamp_(0.,1.))\n",
    "            self.writer.add_image(f'Images batch {batch}', out_grid, global_step=self.cur_epoch)\n",
    "        \n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "            \n",
    "        return {'epoch_loss': cumu_loss / len(dataloader), 'batch_losses': losses}\n",
    "\n",
    "    def learn(self, out, y):\n",
    "        y_cuda = y.to(DEVICE)\n",
    "        p_w_loss = self.pixel_loss(out, y_cuda)\n",
    "        loss = self.loss_fn(out, y_cuda) + p_w_loss\n",
    "        \n",
    "        print(f'L1 loss: {p_w_loss.item()}')\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba3972",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6760c872",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    RECORDS_PATH = f'drive/MyDrive/ML/Indiv_Project/Second_Year/ESRGAN_Scratch/training.csv'\n",
    "    MODEL_PATH = f'drive/MyDrive/ML/Indiv_Project/Second_Year/ESRGAN_Scratch/model.pth'\n",
    "else:\n",
    "    RECORDS_PATH = f'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\ESRGAN_Scratch\\\\training.csv'\n",
    "    MODEL_PATH = f'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\ESRGAN_Scratch\\model.pth'\n",
    "\n",
    "model = RRDBNet()\n",
    "trainer = Trainer(model)\n",
    "trainer.load(MODEL_PATH)\n",
    "\n",
    "data_train = DIV2KTrain(FOLDER_HR_TRAIN, hr_resize_factor=1)\n",
    "data_train.restrict_size(400)\n",
    "dl_train = DataLoader(data_train, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09705e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4807\n",
      "------------------------------------------\n",
      "L1 loss: 0.06509442627429962\n",
      "Training batch 0 with loss 0.26414\n",
      "------------------------------------------\n",
      "L1 loss: 0.08383459597826004\n",
      "Training batch 1 with loss 0.29857\n",
      "------------------------------------------\n",
      "L1 loss: 0.09010085463523865\n",
      "Training batch 2 with loss 0.32528\n",
      "------------------------------------------\n",
      "L1 loss: 0.08077043294906616\n",
      "Training batch 3 with loss 0.27770\n",
      "------------------------------------------\n",
      "L1 loss: 0.08297618478536606\n",
      "Training batch 4 with loss 0.33494\n",
      "------------------------------------------\n",
      "L1 loss: 0.06570187956094742\n",
      "Training batch 5 with loss 0.32207\n",
      "------------------------------------------\n",
      "L1 loss: 0.08031147718429565\n",
      "Training batch 6 with loss 0.27018\n",
      "------------------------------------------\n",
      "L1 loss: 0.08301092684268951\n",
      "Training batch 7 with loss 0.33289\n",
      "------------------------------------------\n",
      "L1 loss: 0.0782976970076561\n",
      "Training batch 8 with loss 0.32060\n",
      "------------------------------------------\n",
      "L1 loss: 0.05820775777101517\n",
      "Training batch 9 with loss 0.26512\n",
      "------------------------------------------\n",
      "L1 loss: 0.08113402128219604\n",
      "Training batch 10 with loss 0.29268\n",
      "------------------------------------------\n",
      "L1 loss: 0.05862007662653923\n",
      "Training batch 11 with loss 0.28324\n",
      "------------------------------------------\n",
      "L1 loss: 0.07302679121494293\n",
      "Training batch 12 with loss 0.29886\n",
      "------------------------------------------\n",
      "L1 loss: 0.06779586523771286\n",
      "Training batch 13 with loss 0.23505\n",
      "------------------------------------------\n",
      "L1 loss: 0.05109914019703865\n",
      "Training batch 14 with loss 0.24032\n",
      "------------------------------------------\n",
      "L1 loss: 0.08286327123641968\n",
      "Training batch 15 with loss 0.28136\n",
      "------------------------------------------\n",
      "L1 loss: 0.06305037438869476\n",
      "Training batch 16 with loss 0.25697\n",
      "------------------------------------------\n",
      "L1 loss: 0.08170648664236069\n",
      "Training batch 17 with loss 0.31870\n",
      "------------------------------------------\n",
      "L1 loss: 0.07926171272993088\n",
      "Training batch 18 with loss 0.30541\n",
      "------------------------------------------\n",
      "L1 loss: 0.06175427511334419\n",
      "Training batch 19 with loss 0.23369\n",
      "------------------------------------------\n",
      "L1 loss: 0.0735066682100296\n",
      "Training batch 20 with loss 0.32691\n",
      "------------------------------------------\n",
      "L1 loss: 0.0721101388335228\n",
      "Training batch 21 with loss 0.28530\n",
      "------------------------------------------\n",
      "L1 loss: 0.06802894175052643\n",
      "Training batch 22 with loss 0.29244\n",
      "------------------------------------------\n",
      "L1 loss: 0.09172118455171585\n",
      "Training batch 23 with loss 0.35996\n",
      "------------------------------------------\n",
      "L1 loss: 0.06834207475185394\n",
      "Training batch 24 with loss 0.28951\n",
      "------------------------------------------\n",
      "L1 loss: 0.07746849954128265\n",
      "Training batch 25 with loss 0.30854\n",
      "------------------------------------------\n",
      "L1 loss: 0.07758734375238419\n",
      "Training batch 26 with loss 0.29089\n",
      "------------------------------------------\n",
      "L1 loss: 0.05740664526820183\n",
      "Training batch 27 with loss 0.29580\n",
      "------------------------------------------\n",
      "L1 loss: 0.05735582113265991\n",
      "Training batch 28 with loss 0.28871\n",
      "------------------------------------------\n",
      "L1 loss: 0.056017469614744186\n",
      "Training batch 29 with loss 0.25491\n",
      "------------------------------------------\n",
      "L1 loss: 0.06264343857765198\n",
      "Training batch 30 with loss 0.26879\n",
      "------------------------------------------\n",
      "L1 loss: 0.06533103436231613\n",
      "Training batch 31 with loss 0.30725\n",
      "------------------------------------------\n",
      "L1 loss: 0.06984671205282211\n",
      "Training batch 32 with loss 0.28402\n",
      "------------------------------------------\n",
      "L1 loss: 0.06401897221803665\n",
      "Training batch 33 with loss 0.25874\n",
      "------------------------------------------\n",
      "L1 loss: 0.07401325553655624\n",
      "Training batch 34 with loss 0.30461\n",
      "------------------------------------------\n",
      "L1 loss: 0.09510896354913712\n",
      "Training batch 35 with loss 0.36203\n",
      "------------------------------------------\n",
      "L1 loss: 0.08324947953224182\n",
      "Training batch 36 with loss 0.28417\n",
      "------------------------------------------\n",
      "L1 loss: 0.06631206721067429\n",
      "Training batch 37 with loss 0.25561\n",
      "------------------------------------------\n",
      "L1 loss: 0.06353505700826645\n",
      "Training batch 38 with loss 0.21953\n",
      "------------------------------------------\n",
      "L1 loss: 0.06827749311923981\n",
      "Training batch 39 with loss 0.31694\n",
      "------------------------------------------\n",
      "L1 loss: 0.06723106652498245\n",
      "Training batch 40 with loss 0.24593\n",
      "------------------------------------------\n",
      "L1 loss: 0.059461791068315506\n",
      "Training batch 41 with loss 0.29327\n",
      "------------------------------------------\n",
      "L1 loss: 0.0757472887635231\n",
      "Training batch 42 with loss 0.27413\n",
      "------------------------------------------\n",
      "L1 loss: 0.07019921392202377\n",
      "Training batch 43 with loss 0.27578\n",
      "------------------------------------------\n",
      "L1 loss: 0.05470902845263481\n",
      "Training batch 44 with loss 0.26461\n",
      "------------------------------------------\n",
      "L1 loss: 0.0916358157992363\n",
      "Training batch 45 with loss 0.35761\n",
      "------------------------------------------\n",
      "L1 loss: 0.07394459843635559\n",
      "Training batch 46 with loss 0.27194\n",
      "------------------------------------------\n",
      "L1 loss: 0.10943414270877838\n",
      "Training batch 47 with loss 0.35577\n",
      "------------------------------------------\n",
      "L1 loss: 0.04901904612779617\n",
      "Training batch 48 with loss 0.25213\n",
      "------------------------------------------\n",
      "L1 loss: 0.0814211443066597\n",
      "Training batch 49 with loss 0.36422\n",
      "------------------------------------------\n",
      "L1 loss: 0.07360640913248062\n",
      "Training batch 50 with loss 0.28415\n",
      "------------------------------------------\n",
      "L1 loss: 0.08349131792783737\n",
      "Training batch 51 with loss 0.31854\n",
      "------------------------------------------\n",
      "L1 loss: 0.09138578176498413\n",
      "Training batch 52 with loss 0.29160\n",
      "------------------------------------------\n",
      "L1 loss: 0.07781524956226349\n",
      "Training batch 53 with loss 0.30836\n",
      "------------------------------------------\n",
      "L1 loss: 0.0767776370048523\n",
      "Training batch 54 with loss 0.28761\n",
      "------------------------------------------\n",
      "L1 loss: 0.0608513318002224\n",
      "Training batch 55 with loss 0.29802\n",
      "------------------------------------------\n",
      "L1 loss: 0.05577617138624191\n",
      "Training batch 56 with loss 0.25116\n",
      "------------------------------------------\n",
      "L1 loss: 0.06560272723436356\n",
      "Training batch 57 with loss 0.27512\n",
      "------------------------------------------\n",
      "L1 loss: 0.05592438951134682\n",
      "Training batch 58 with loss 0.38848\n",
      "------------------------------------------\n",
      "L1 loss: 0.08080070465803146\n",
      "Training batch 59 with loss 0.28341\n",
      "------------------------------------------\n",
      "L1 loss: 0.062178995460271835\n",
      "Training batch 60 with loss 0.26076\n",
      "------------------------------------------\n",
      "L1 loss: 0.09971431642770767\n",
      "Training batch 61 with loss 0.36132\n",
      "------------------------------------------\n",
      "L1 loss: 0.05755271762609482\n",
      "Training batch 62 with loss 0.25051\n",
      "------------------------------------------\n",
      "L1 loss: 0.0589018389582634\n",
      "Training batch 63 with loss 0.25395\n",
      "------------------------------------------\n",
      "L1 loss: 0.0701831504702568\n",
      "Training batch 64 with loss 0.27904\n",
      "------------------------------------------\n",
      "L1 loss: 0.05781374126672745\n",
      "Training batch 65 with loss 0.22766\n",
      "------------------------------------------\n",
      "L1 loss: 0.06799139082431793\n",
      "Training batch 66 with loss 0.27982\n",
      "------------------------------------------\n",
      "L1 loss: 0.06322880834341049\n",
      "Training batch 67 with loss 0.26317\n",
      "------------------------------------------\n",
      "L1 loss: 0.0770564079284668\n",
      "Training batch 68 with loss 0.31098\n",
      "------------------------------------------\n",
      "L1 loss: 0.048938509076833725\n",
      "Training batch 69 with loss 0.25802\n",
      "------------------------------------------\n",
      "L1 loss: 0.076413594186306\n",
      "Training batch 70 with loss 0.30015\n",
      "------------------------------------------\n",
      "L1 loss: 0.06239129602909088\n",
      "Training batch 71 with loss 0.38589\n",
      "------------------------------------------\n",
      "L1 loss: 0.08608578145503998\n",
      "Training batch 72 with loss 0.30829\n",
      "------------------------------------------\n",
      "L1 loss: 0.07323464751243591\n",
      "Training batch 73 with loss 0.30698\n",
      "------------------------------------------\n",
      "L1 loss: 0.09740567207336426\n",
      "Training batch 74 with loss 0.31094\n",
      "------------------------------------------\n",
      "L1 loss: 0.07513962686061859\n",
      "Training batch 75 with loss 0.27533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.08652082085609436\n",
      "Training batch 76 with loss 0.29416\n",
      "------------------------------------------\n",
      "L1 loss: 0.07711320370435715\n",
      "Training batch 77 with loss 0.26428\n",
      "------------------------------------------\n",
      "L1 loss: 0.08095569908618927\n",
      "Training batch 78 with loss 0.30701\n",
      "------------------------------------------\n",
      "L1 loss: 0.10517521947622299\n",
      "Training batch 79 with loss 0.37598\n",
      "------------------------------------------\n",
      "L1 loss: 0.07772774249315262\n",
      "Training batch 80 with loss 0.29322\n",
      "------------------------------------------\n",
      "L1 loss: 0.06844371557235718\n",
      "Training batch 81 with loss 0.26604\n",
      "------------------------------------------\n",
      "L1 loss: 0.08346202969551086\n",
      "Training batch 82 with loss 0.38315\n",
      "------------------------------------------\n",
      "L1 loss: 0.06421758979558945\n",
      "Training batch 83 with loss 0.34683\n",
      "------------------------------------------\n",
      "L1 loss: 0.07290466874837875\n",
      "Training batch 84 with loss 0.32999\n",
      "------------------------------------------\n",
      "L1 loss: 0.0594346709549427\n",
      "Training batch 85 with loss 0.28118\n",
      "------------------------------------------\n",
      "L1 loss: 0.08233218640089035\n",
      "Training batch 86 with loss 0.30609\n",
      "------------------------------------------\n",
      "L1 loss: 0.10068976879119873\n",
      "Training batch 87 with loss 0.39253\n",
      "------------------------------------------\n",
      "L1 loss: 0.07774828374385834\n",
      "Training batch 88 with loss 0.33333\n",
      "------------------------------------------\n",
      "L1 loss: 0.06909921765327454\n",
      "Training batch 89 with loss 0.28560\n",
      "------------------------------------------\n",
      "L1 loss: 0.07427927106618881\n",
      "Training batch 90 with loss 0.28399\n",
      "------------------------------------------\n",
      "L1 loss: 0.0541846938431263\n",
      "Training batch 91 with loss 0.27505\n",
      "------------------------------------------\n",
      "L1 loss: 0.09078411012887955\n",
      "Training batch 92 with loss 0.29892\n",
      "------------------------------------------\n",
      "L1 loss: 0.056447532027959824\n",
      "Training batch 93 with loss 0.24042\n",
      "------------------------------------------\n",
      "L1 loss: 0.0852113664150238\n",
      "Training batch 94 with loss 0.30285\n",
      "------------------------------------------\n",
      "L1 loss: 0.07090826332569122\n",
      "Training batch 95 with loss 0.25414\n",
      "------------------------------------------\n",
      "L1 loss: 0.07320159673690796\n",
      "Training batch 96 with loss 0.31101\n",
      "------------------------------------------\n",
      "L1 loss: 0.08683212101459503\n",
      "Training batch 97 with loss 0.34393\n",
      "------------------------------------------\n",
      "L1 loss: 0.0595950186252594\n",
      "Training batch 98 with loss 0.25494\n",
      "------------------------------------------\n",
      "L1 loss: 0.06358161568641663\n",
      "Training batch 99 with loss 0.26055\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29472236707806587\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4808\n",
      "------------------------------------------\n",
      "L1 loss: 0.05474061518907547\n",
      "Training batch 0 with loss 0.21105\n",
      "------------------------------------------\n",
      "L1 loss: 0.08435067534446716\n",
      "Training batch 1 with loss 0.28918\n",
      "------------------------------------------\n",
      "L1 loss: 0.09421853721141815\n",
      "Training batch 2 with loss 0.32963\n",
      "------------------------------------------\n",
      "L1 loss: 0.07680148631334305\n",
      "Training batch 3 with loss 0.25877\n",
      "------------------------------------------\n",
      "L1 loss: 0.08428255468606949\n",
      "Training batch 4 with loss 0.33905\n",
      "------------------------------------------\n",
      "L1 loss: 0.06635768711566925\n",
      "Training batch 5 with loss 0.32652\n",
      "------------------------------------------\n",
      "L1 loss: 0.08110180497169495\n",
      "Training batch 6 with loss 0.27265\n",
      "------------------------------------------\n",
      "L1 loss: 0.06640520691871643\n",
      "Training batch 7 with loss 0.44452\n",
      "------------------------------------------\n",
      "L1 loss: 0.07626113295555115\n",
      "Training batch 8 with loss 0.31181\n",
      "------------------------------------------\n",
      "L1 loss: 0.05930020287632942\n",
      "Training batch 9 with loss 0.25633\n",
      "------------------------------------------\n",
      "L1 loss: 0.08177909255027771\n",
      "Training batch 10 with loss 0.29641\n",
      "------------------------------------------\n",
      "L1 loss: 0.06411302834749222\n",
      "Training batch 11 with loss 0.30670\n",
      "------------------------------------------\n",
      "L1 loss: 0.07561898976564407\n",
      "Training batch 12 with loss 0.30236\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990975141525269\n",
      "Training batch 13 with loss 0.25293\n",
      "------------------------------------------\n",
      "L1 loss: 0.05393841862678528\n",
      "Training batch 14 with loss 0.26846\n",
      "------------------------------------------\n",
      "L1 loss: 0.08644213527441025\n",
      "Training batch 15 with loss 0.27373\n",
      "------------------------------------------\n",
      "L1 loss: 0.06291349232196808\n",
      "Training batch 16 with loss 0.25733\n",
      "------------------------------------------\n",
      "L1 loss: 0.08806765824556351\n",
      "Training batch 17 with loss 0.35469\n",
      "------------------------------------------\n",
      "L1 loss: 0.08098211884498596\n",
      "Training batch 18 with loss 0.29306\n",
      "------------------------------------------\n",
      "L1 loss: 0.061617664992809296\n",
      "Training batch 19 with loss 0.23985\n",
      "------------------------------------------\n",
      "L1 loss: 0.07307807356119156\n",
      "Training batch 20 with loss 0.31851\n",
      "------------------------------------------\n",
      "L1 loss: 0.0729508325457573\n",
      "Training batch 21 with loss 0.27597\n",
      "------------------------------------------\n",
      "L1 loss: 0.06682426482439041\n",
      "Training batch 22 with loss 0.29008\n",
      "------------------------------------------\n",
      "L1 loss: 0.09026069194078445\n",
      "Training batch 23 with loss 0.35698\n",
      "------------------------------------------\n",
      "L1 loss: 0.06689797341823578\n",
      "Training batch 24 with loss 0.29728\n",
      "------------------------------------------\n",
      "L1 loss: 0.07582495361566544\n",
      "Training batch 25 with loss 0.31861\n",
      "------------------------------------------\n",
      "L1 loss: 0.07968729734420776\n",
      "Training batch 26 with loss 0.28978\n",
      "------------------------------------------\n",
      "L1 loss: 0.05726427584886551\n",
      "Training batch 27 with loss 0.30372\n",
      "------------------------------------------\n",
      "L1 loss: 0.059107761830091476\n",
      "Training batch 28 with loss 0.29783\n",
      "------------------------------------------\n",
      "L1 loss: 0.054381925612688065\n",
      "Training batch 29 with loss 0.22885\n",
      "------------------------------------------\n",
      "L1 loss: 0.06164710596203804\n",
      "Training batch 30 with loss 0.26172\n",
      "------------------------------------------\n",
      "L1 loss: 0.06238752603530884\n",
      "Training batch 31 with loss 0.29747\n",
      "------------------------------------------\n",
      "L1 loss: 0.07009270787239075\n",
      "Training batch 32 with loss 0.27778\n",
      "------------------------------------------\n",
      "L1 loss: 0.05480610951781273\n",
      "Training batch 33 with loss 0.32435\n",
      "------------------------------------------\n",
      "L1 loss: 0.07434839755296707\n",
      "Training batch 34 with loss 0.31559\n",
      "------------------------------------------\n",
      "L1 loss: 0.09620779007673264\n",
      "Training batch 35 with loss 0.34855\n",
      "------------------------------------------\n",
      "L1 loss: 0.08232694119215012\n",
      "Training batch 36 with loss 0.29455\n",
      "------------------------------------------\n",
      "L1 loss: 0.06879647821187973\n",
      "Training batch 37 with loss 0.26599\n",
      "------------------------------------------\n",
      "L1 loss: 0.06277672946453094\n",
      "Training batch 38 with loss 0.22006\n",
      "------------------------------------------\n",
      "L1 loss: 0.06759604811668396\n",
      "Training batch 39 with loss 0.30077\n",
      "------------------------------------------\n",
      "L1 loss: 0.0698767602443695\n",
      "Training batch 40 with loss 0.25724\n",
      "------------------------------------------\n",
      "L1 loss: 0.060724690556526184\n",
      "Training batch 41 with loss 0.29893\n",
      "------------------------------------------\n",
      "L1 loss: 0.08348821848630905\n",
      "Training batch 42 with loss 0.29978\n",
      "------------------------------------------\n",
      "L1 loss: 0.07225726544857025\n",
      "Training batch 43 with loss 0.28299\n",
      "------------------------------------------\n",
      "L1 loss: 0.05751675367355347\n",
      "Training batch 44 with loss 0.26345\n",
      "------------------------------------------\n",
      "L1 loss: 0.09109721332788467\n",
      "Training batch 45 with loss 0.36139\n",
      "------------------------------------------\n",
      "L1 loss: 0.07212266325950623\n",
      "Training batch 46 with loss 0.26481\n",
      "------------------------------------------\n",
      "L1 loss: 0.1096201241016388\n",
      "Training batch 47 with loss 0.36085\n",
      "------------------------------------------\n",
      "L1 loss: 0.04759388417005539\n",
      "Training batch 48 with loss 0.24537\n",
      "------------------------------------------\n",
      "L1 loss: 0.08154848217964172\n",
      "Training batch 49 with loss 0.35196\n",
      "------------------------------------------\n",
      "L1 loss: 0.07182949036359787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 50 with loss 0.28705\n",
      "------------------------------------------\n",
      "L1 loss: 0.08236269652843475\n",
      "Training batch 51 with loss 0.33127\n",
      "------------------------------------------\n",
      "L1 loss: 0.08883439004421234\n",
      "Training batch 52 with loss 0.28652\n",
      "------------------------------------------\n",
      "L1 loss: 0.0796620324254036\n",
      "Training batch 53 with loss 0.28585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07087530195713043\n",
      "Training batch 54 with loss 0.27476\n",
      "------------------------------------------\n",
      "L1 loss: 0.06171967461705208\n",
      "Training batch 55 with loss 0.29563\n",
      "------------------------------------------\n",
      "L1 loss: 0.05414500832557678\n",
      "Training batch 56 with loss 0.25206\n",
      "------------------------------------------\n",
      "L1 loss: 0.06793998926877975\n",
      "Training batch 57 with loss 0.28652\n",
      "------------------------------------------\n",
      "L1 loss: 0.06755390018224716\n",
      "Training batch 58 with loss 0.26153\n",
      "------------------------------------------\n",
      "L1 loss: 0.0825200229883194\n",
      "Training batch 59 with loss 0.29082\n",
      "------------------------------------------\n",
      "L1 loss: 0.06680933386087418\n",
      "Training batch 60 with loss 0.27827\n",
      "------------------------------------------\n",
      "L1 loss: 0.09468359500169754\n",
      "Training batch 61 with loss 0.35213\n",
      "------------------------------------------\n",
      "L1 loss: 0.05715322494506836\n",
      "Training batch 62 with loss 0.25484\n",
      "------------------------------------------\n",
      "L1 loss: 0.06143779307603836\n",
      "Training batch 63 with loss 0.24419\n",
      "------------------------------------------\n",
      "L1 loss: 0.06927992403507233\n",
      "Training batch 64 with loss 0.27547\n",
      "------------------------------------------\n",
      "L1 loss: 0.05922200530767441\n",
      "Training batch 65 with loss 0.21871\n",
      "------------------------------------------\n",
      "L1 loss: 0.06538928300142288\n",
      "Training batch 66 with loss 0.27205\n",
      "------------------------------------------\n",
      "L1 loss: 0.06572111696004868\n",
      "Training batch 67 with loss 0.25834\n",
      "------------------------------------------\n",
      "L1 loss: 0.07315588742494583\n",
      "Training batch 68 with loss 0.29799\n",
      "------------------------------------------\n",
      "L1 loss: 0.052806440740823746\n",
      "Training batch 69 with loss 0.27268\n",
      "------------------------------------------\n",
      "L1 loss: 0.0783381313085556\n",
      "Training batch 70 with loss 0.30914\n",
      "------------------------------------------\n",
      "L1 loss: 0.07171284407377243\n",
      "Training batch 71 with loss 0.30021\n",
      "------------------------------------------\n",
      "L1 loss: 0.08427122235298157\n",
      "Training batch 72 with loss 0.30585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07304435223340988\n",
      "Training batch 73 with loss 0.30705\n",
      "------------------------------------------\n",
      "L1 loss: 0.09798348695039749\n",
      "Training batch 74 with loss 0.33500\n",
      "------------------------------------------\n",
      "L1 loss: 0.07381201535463333\n",
      "Training batch 75 with loss 0.29183\n",
      "------------------------------------------\n",
      "L1 loss: 0.08625441789627075\n",
      "Training batch 76 with loss 0.29277\n",
      "------------------------------------------\n",
      "L1 loss: 0.07757478207349777\n",
      "Training batch 77 with loss 0.25994\n",
      "------------------------------------------\n",
      "L1 loss: 0.08208923786878586\n",
      "Training batch 78 with loss 0.32011\n",
      "------------------------------------------\n",
      "L1 loss: 0.10924094170331955\n",
      "Training batch 79 with loss 0.36632\n",
      "------------------------------------------\n",
      "L1 loss: 0.07554257661104202\n",
      "Training batch 80 with loss 0.29718\n",
      "------------------------------------------\n",
      "L1 loss: 0.075248584151268\n",
      "Training batch 81 with loss 0.27711\n",
      "------------------------------------------\n",
      "L1 loss: 0.0833878293633461\n",
      "Training batch 82 with loss 0.35242\n",
      "------------------------------------------\n",
      "L1 loss: 0.07481955736875534\n",
      "Training batch 83 with loss 0.29482\n",
      "------------------------------------------\n",
      "L1 loss: 0.07228000462055206\n",
      "Training batch 84 with loss 0.30966\n",
      "------------------------------------------\n",
      "L1 loss: 0.061129551380872726\n",
      "Training batch 85 with loss 0.26216\n",
      "------------------------------------------\n",
      "L1 loss: 0.07793723046779633\n",
      "Training batch 86 with loss 0.28054\n",
      "------------------------------------------\n",
      "L1 loss: 0.1009206622838974\n",
      "Training batch 87 with loss 0.40490\n",
      "------------------------------------------\n",
      "L1 loss: 0.07541497051715851\n",
      "Training batch 88 with loss 0.32939\n",
      "------------------------------------------\n",
      "L1 loss: 0.06631246954202652\n",
      "Training batch 89 with loss 0.26573\n",
      "------------------------------------------\n",
      "L1 loss: 0.058186035603284836\n",
      "Training batch 90 with loss 0.60430\n",
      "------------------------------------------\n",
      "L1 loss: 0.059977877885103226\n",
      "Training batch 91 with loss 0.26764\n",
      "------------------------------------------\n",
      "L1 loss: 0.08753527700901031\n",
      "Training batch 92 with loss 0.40704\n",
      "------------------------------------------\n",
      "L1 loss: 0.05975374951958656\n",
      "Training batch 93 with loss 0.23163\n",
      "------------------------------------------\n",
      "L1 loss: 0.08585015684366226\n",
      "Training batch 94 with loss 0.30502\n",
      "------------------------------------------\n",
      "L1 loss: 0.0701228678226471\n",
      "Training batch 95 with loss 0.25710\n",
      "------------------------------------------\n",
      "L1 loss: 0.06597653776407242\n",
      "Training batch 96 with loss 0.28026\n",
      "------------------------------------------\n",
      "L1 loss: 0.08651980757713318\n",
      "Training batch 97 with loss 0.35020\n",
      "------------------------------------------\n",
      "L1 loss: 0.055200424045324326\n",
      "Training batch 98 with loss 0.24691\n",
      "------------------------------------------\n",
      "L1 loss: 0.06162673979997635\n",
      "Training batch 99 with loss 0.25429\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29699492916464804\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4809\n",
      "------------------------------------------\n",
      "L1 loss: 0.06299485266208649\n",
      "Training batch 0 with loss 0.25649\n",
      "------------------------------------------\n",
      "L1 loss: 0.08344753831624985\n",
      "Training batch 1 with loss 0.29567\n",
      "------------------------------------------\n",
      "L1 loss: 0.09181248396635056\n",
      "Training batch 2 with loss 0.32036\n",
      "------------------------------------------\n",
      "L1 loss: 0.07418667525053024\n",
      "Training batch 3 with loss 0.26080\n",
      "------------------------------------------\n",
      "L1 loss: 0.08231909573078156\n",
      "Training batch 4 with loss 0.34465\n",
      "------------------------------------------\n",
      "L1 loss: 0.0681871771812439\n",
      "Training batch 5 with loss 0.33167\n",
      "------------------------------------------\n",
      "L1 loss: 0.055039066821336746\n",
      "Training batch 6 with loss 0.37826\n",
      "------------------------------------------\n",
      "L1 loss: 0.08485560864210129\n",
      "Training batch 7 with loss 0.33770\n",
      "------------------------------------------\n",
      "L1 loss: 0.08070376515388489\n",
      "Training batch 8 with loss 0.32624\n",
      "------------------------------------------\n",
      "L1 loss: 0.04763196036219597\n",
      "Training batch 9 with loss 0.21100\n",
      "------------------------------------------\n",
      "L1 loss: 0.082614004611969\n",
      "Training batch 10 with loss 0.29413\n",
      "------------------------------------------\n",
      "L1 loss: 0.06099984422326088\n",
      "Training batch 11 with loss 0.30065\n",
      "------------------------------------------\n",
      "L1 loss: 0.07255178689956665\n",
      "Training batch 12 with loss 0.31592\n",
      "------------------------------------------\n",
      "L1 loss: 0.07028824090957642\n",
      "Training batch 13 with loss 0.25020\n",
      "------------------------------------------\n",
      "L1 loss: 0.05395438149571419\n",
      "Training batch 14 with loss 0.24778\n",
      "------------------------------------------\n",
      "L1 loss: 0.08398917317390442\n",
      "Training batch 15 with loss 0.28590\n",
      "------------------------------------------\n",
      "L1 loss: 0.065752774477005\n",
      "Training batch 16 with loss 0.26739\n",
      "------------------------------------------\n",
      "L1 loss: 0.0831102505326271\n",
      "Training batch 17 with loss 0.32405\n",
      "------------------------------------------\n",
      "L1 loss: 0.08041144162416458\n",
      "Training batch 18 with loss 0.31466\n",
      "------------------------------------------\n",
      "L1 loss: 0.06112944707274437\n",
      "Training batch 19 with loss 0.22182\n",
      "------------------------------------------\n",
      "L1 loss: 0.07228630781173706\n",
      "Training batch 20 with loss 0.32509\n",
      "------------------------------------------\n",
      "L1 loss: 0.07045356184244156\n",
      "Training batch 21 with loss 0.28028\n",
      "------------------------------------------\n",
      "L1 loss: 0.06848662346601486\n",
      "Training batch 22 with loss 0.28896\n",
      "------------------------------------------\n",
      "L1 loss: 0.09151428937911987\n",
      "Training batch 23 with loss 0.37462\n",
      "------------------------------------------\n",
      "L1 loss: 0.06732156872749329\n",
      "Training batch 24 with loss 0.30291\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07634720951318741\n",
      "Training batch 25 with loss 0.29987\n",
      "------------------------------------------\n",
      "L1 loss: 0.07243292778730392\n",
      "Training batch 26 with loss 0.27613\n",
      "------------------------------------------\n",
      "L1 loss: 0.05748949199914932\n",
      "Training batch 27 with loss 0.28969\n",
      "------------------------------------------\n",
      "L1 loss: 0.05879039317369461\n",
      "Training batch 28 with loss 0.31112\n",
      "------------------------------------------\n",
      "L1 loss: 0.05523509159684181\n",
      "Training batch 29 with loss 0.25217\n",
      "------------------------------------------\n",
      "L1 loss: 0.0586579293012619\n",
      "Training batch 30 with loss 0.27240\n",
      "------------------------------------------\n",
      "L1 loss: 0.0628635585308075\n",
      "Training batch 31 with loss 0.29052\n",
      "------------------------------------------\n",
      "L1 loss: 0.0713704451918602\n",
      "Training batch 32 with loss 0.28993\n",
      "------------------------------------------\n",
      "L1 loss: 0.06591164320707321\n",
      "Training batch 33 with loss 0.27379\n",
      "------------------------------------------\n",
      "L1 loss: 0.07491634041070938\n",
      "Training batch 34 with loss 0.32529\n",
      "------------------------------------------\n",
      "L1 loss: 0.09473598003387451\n",
      "Training batch 35 with loss 0.33023\n",
      "------------------------------------------\n",
      "L1 loss: 0.08371702581644058\n",
      "Training batch 36 with loss 0.28129\n",
      "------------------------------------------\n",
      "L1 loss: 0.0676731988787651\n",
      "Training batch 37 with loss 0.26053\n",
      "------------------------------------------\n",
      "L1 loss: 0.06256283819675446\n",
      "Training batch 38 with loss 0.22186\n",
      "------------------------------------------\n",
      "L1 loss: 0.06660383939743042\n",
      "Training batch 39 with loss 0.29662\n",
      "------------------------------------------\n",
      "L1 loss: 0.0682707205414772\n",
      "Training batch 40 with loss 0.25556\n",
      "------------------------------------------\n",
      "L1 loss: 0.06017662584781647\n",
      "Training batch 41 with loss 0.30200\n",
      "------------------------------------------\n",
      "L1 loss: 0.07951842248439789\n",
      "Training batch 42 with loss 0.28300\n",
      "------------------------------------------\n",
      "L1 loss: 0.07024803757667542\n",
      "Training batch 43 with loss 0.28051\n",
      "------------------------------------------\n",
      "L1 loss: 0.05498053506016731\n",
      "Training batch 44 with loss 0.27727\n",
      "------------------------------------------\n",
      "L1 loss: 0.09203682094812393\n",
      "Training batch 45 with loss 0.36118\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018893212080002\n",
      "Training batch 46 with loss 0.26525\n",
      "------------------------------------------\n",
      "L1 loss: 0.10951098054647446\n",
      "Training batch 47 with loss 0.36759\n",
      "------------------------------------------\n",
      "L1 loss: 0.04455343633890152\n",
      "Training batch 48 with loss 0.24437\n",
      "------------------------------------------\n",
      "L1 loss: 0.08304949849843979\n",
      "Training batch 49 with loss 0.36008\n",
      "------------------------------------------\n",
      "L1 loss: 0.07113179564476013\n",
      "Training batch 50 with loss 0.27993\n",
      "------------------------------------------\n",
      "L1 loss: 0.07743407785892487\n",
      "Training batch 51 with loss 0.32504\n",
      "------------------------------------------\n",
      "L1 loss: 0.09078601747751236\n",
      "Training batch 52 with loss 0.29237\n",
      "------------------------------------------\n",
      "L1 loss: 0.08136170357465744\n",
      "Training batch 53 with loss 0.28981\n",
      "------------------------------------------\n",
      "L1 loss: 0.0605769045650959\n",
      "Training batch 54 with loss 0.39145\n",
      "------------------------------------------\n",
      "L1 loss: 0.06035180389881134\n",
      "Training batch 55 with loss 0.30322\n",
      "------------------------------------------\n",
      "L1 loss: 0.05860636383295059\n",
      "Training batch 56 with loss 0.26117\n",
      "------------------------------------------\n",
      "L1 loss: 0.07148018479347229\n",
      "Training batch 57 with loss 0.29454\n",
      "------------------------------------------\n",
      "L1 loss: 0.0657326951622963\n",
      "Training batch 58 with loss 0.25914\n",
      "------------------------------------------\n",
      "L1 loss: 0.07767964899539948\n",
      "Training batch 59 with loss 0.29643\n",
      "------------------------------------------\n",
      "L1 loss: 0.0661701112985611\n",
      "Training batch 60 with loss 0.26773\n",
      "------------------------------------------\n",
      "L1 loss: 0.10119593888521194\n",
      "Training batch 61 with loss 0.36226\n",
      "------------------------------------------\n",
      "L1 loss: 0.05520176887512207\n",
      "Training batch 62 with loss 0.24161\n",
      "------------------------------------------\n",
      "L1 loss: 0.061900656670331955\n",
      "Training batch 63 with loss 0.25558\n",
      "------------------------------------------\n",
      "L1 loss: 0.0690077394247055\n",
      "Training batch 64 with loss 0.26909\n",
      "------------------------------------------\n",
      "L1 loss: 0.060602154582738876\n",
      "Training batch 65 with loss 0.39428\n",
      "------------------------------------------\n",
      "L1 loss: 0.06582611799240112\n",
      "Training batch 66 with loss 0.27980\n",
      "------------------------------------------\n",
      "L1 loss: 0.06520681083202362\n",
      "Training batch 67 with loss 0.26983\n",
      "------------------------------------------\n",
      "L1 loss: 0.07687653601169586\n",
      "Training batch 68 with loss 0.32307\n",
      "------------------------------------------\n",
      "L1 loss: 0.051093243062496185\n",
      "Training batch 69 with loss 0.25448\n",
      "------------------------------------------\n",
      "L1 loss: 0.07818751782178879\n",
      "Training batch 70 with loss 0.30380\n",
      "------------------------------------------\n",
      "L1 loss: 0.07191066443920135\n",
      "Training batch 71 with loss 0.30198\n",
      "------------------------------------------\n",
      "L1 loss: 0.08309648931026459\n",
      "Training batch 72 with loss 0.29372\n",
      "------------------------------------------\n",
      "L1 loss: 0.07147400826215744\n",
      "Training batch 73 with loss 0.30633\n",
      "------------------------------------------\n",
      "L1 loss: 0.0981236919760704\n",
      "Training batch 74 with loss 0.33214\n",
      "------------------------------------------\n",
      "L1 loss: 0.07327962666749954\n",
      "Training batch 75 with loss 0.29356\n",
      "------------------------------------------\n",
      "L1 loss: 0.05909144878387451\n",
      "Training batch 76 with loss 0.46147\n",
      "------------------------------------------\n",
      "L1 loss: 0.07684119790792465\n",
      "Training batch 77 with loss 0.27950\n",
      "------------------------------------------\n",
      "L1 loss: 0.08032690733671188\n",
      "Training batch 78 with loss 0.30641\n",
      "------------------------------------------\n",
      "L1 loss: 0.1069798469543457\n",
      "Training batch 79 with loss 0.36765\n",
      "------------------------------------------\n",
      "L1 loss: 0.07566981017589569\n",
      "Training batch 80 with loss 0.28299\n",
      "------------------------------------------\n",
      "L1 loss: 0.08073490858078003\n",
      "Training batch 81 with loss 0.29907\n",
      "------------------------------------------\n",
      "L1 loss: 0.08126770704984665\n",
      "Training batch 82 with loss 0.36427\n",
      "------------------------------------------\n",
      "L1 loss: 0.07701972126960754\n",
      "Training batch 83 with loss 0.27580\n",
      "------------------------------------------\n",
      "L1 loss: 0.07131167501211166\n",
      "Training batch 84 with loss 0.30684\n",
      "------------------------------------------\n",
      "L1 loss: 0.06522452086210251\n",
      "Training batch 85 with loss 0.27063\n",
      "------------------------------------------\n",
      "L1 loss: 0.08173831552267075\n",
      "Training batch 86 with loss 0.29546\n",
      "------------------------------------------\n",
      "L1 loss: 0.10279688239097595\n",
      "Training batch 87 with loss 0.39879\n",
      "------------------------------------------\n",
      "L1 loss: 0.07545242458581924\n",
      "Training batch 88 with loss 0.30973\n",
      "------------------------------------------\n",
      "L1 loss: 0.06907019019126892\n",
      "Training batch 89 with loss 0.27003\n",
      "------------------------------------------\n",
      "L1 loss: 0.07480474561452866\n",
      "Training batch 90 with loss 0.28027\n",
      "------------------------------------------\n",
      "L1 loss: 0.06415709853172302\n",
      "Training batch 91 with loss 0.26230\n",
      "------------------------------------------\n",
      "L1 loss: 0.09037818014621735\n",
      "Training batch 92 with loss 0.28532\n",
      "------------------------------------------\n",
      "L1 loss: 0.0728221908211708\n",
      "Training batch 93 with loss 0.28228\n",
      "------------------------------------------\n",
      "L1 loss: 0.08532052487134933\n",
      "Training batch 94 with loss 0.31353\n",
      "------------------------------------------\n",
      "L1 loss: 0.069752536714077\n",
      "Training batch 95 with loss 0.25362\n",
      "------------------------------------------\n",
      "L1 loss: 0.061220645904541016\n",
      "Training batch 96 with loss 0.27986\n",
      "------------------------------------------\n",
      "L1 loss: 0.08459003269672394\n",
      "Training batch 97 with loss 0.35674\n",
      "------------------------------------------\n",
      "L1 loss: 0.057723354548215866\n",
      "Training batch 98 with loss 0.24935\n",
      "------------------------------------------\n",
      "L1 loss: 0.06808415055274963\n",
      "Training batch 99 with loss 0.27013\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2978988282382488\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4810\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06018238142132759\n",
      "Training batch 0 with loss 0.24345\n",
      "------------------------------------------\n",
      "L1 loss: 0.08316336572170258\n",
      "Training batch 1 with loss 0.27959\n",
      "------------------------------------------\n",
      "L1 loss: 0.09265345335006714\n",
      "Training batch 2 with loss 0.31936\n",
      "------------------------------------------\n",
      "L1 loss: 0.06922267377376556\n",
      "Training batch 3 with loss 0.26167\n",
      "------------------------------------------\n",
      "L1 loss: 0.0839160904288292\n",
      "Training batch 4 with loss 0.33876\n",
      "------------------------------------------\n",
      "L1 loss: 0.06356543302536011\n",
      "Training batch 5 with loss 0.33077\n",
      "------------------------------------------\n",
      "L1 loss: 0.07824800908565521\n",
      "Training batch 6 with loss 0.26540\n",
      "------------------------------------------\n",
      "L1 loss: 0.08341444283723831\n",
      "Training batch 7 with loss 0.33190\n",
      "------------------------------------------\n",
      "L1 loss: 0.08321832120418549\n",
      "Training batch 8 with loss 0.34235\n",
      "------------------------------------------\n",
      "L1 loss: 0.05875552073121071\n",
      "Training batch 9 with loss 0.26117\n",
      "------------------------------------------\n",
      "L1 loss: 0.08040836453437805\n",
      "Training batch 10 with loss 0.29133\n",
      "------------------------------------------\n",
      "L1 loss: 0.060697268694639206\n",
      "Training batch 11 with loss 0.27327\n",
      "------------------------------------------\n",
      "L1 loss: 0.07679619640111923\n",
      "Training batch 12 with loss 0.31501\n",
      "------------------------------------------\n",
      "L1 loss: 0.0672817975282669\n",
      "Training batch 13 with loss 0.26467\n",
      "------------------------------------------\n",
      "L1 loss: 0.053088217973709106\n",
      "Training batch 14 with loss 0.26575\n",
      "------------------------------------------\n",
      "L1 loss: 0.08276542276144028\n",
      "Training batch 15 with loss 0.27643\n",
      "------------------------------------------\n",
      "L1 loss: 0.06560926139354706\n",
      "Training batch 16 with loss 0.27342\n",
      "------------------------------------------\n",
      "L1 loss: 0.0826312005519867\n",
      "Training batch 17 with loss 0.33796\n",
      "------------------------------------------\n",
      "L1 loss: 0.08127673715353012\n",
      "Training batch 18 with loss 0.31689\n",
      "------------------------------------------\n",
      "L1 loss: 0.061440128833055496\n",
      "Training batch 19 with loss 0.24096\n",
      "------------------------------------------\n",
      "L1 loss: 0.07327339798212051\n",
      "Training batch 20 with loss 0.32264\n",
      "------------------------------------------\n",
      "L1 loss: 0.07250458747148514\n",
      "Training batch 21 with loss 0.27907\n",
      "------------------------------------------\n",
      "L1 loss: 0.06922496110200882\n",
      "Training batch 22 with loss 0.30337\n",
      "------------------------------------------\n",
      "L1 loss: 0.08653233200311661\n",
      "Training batch 23 with loss 0.37106\n",
      "------------------------------------------\n",
      "L1 loss: 0.06673283874988556\n",
      "Training batch 24 with loss 0.31201\n",
      "------------------------------------------\n",
      "L1 loss: 0.07780694216489792\n",
      "Training batch 25 with loss 0.31976\n",
      "------------------------------------------\n",
      "L1 loss: 0.07811663299798965\n",
      "Training batch 26 with loss 0.27742\n",
      "------------------------------------------\n",
      "L1 loss: 0.05621226504445076\n",
      "Training batch 27 with loss 0.29603\n",
      "------------------------------------------\n",
      "L1 loss: 0.05800950154662132\n",
      "Training batch 28 with loss 0.28478\n",
      "------------------------------------------\n",
      "L1 loss: 0.06111591309309006\n",
      "Training batch 29 with loss 0.24218\n",
      "------------------------------------------\n",
      "L1 loss: 0.06558910757303238\n",
      "Training batch 30 with loss 0.28703\n",
      "------------------------------------------\n",
      "L1 loss: 0.06492515653371811\n",
      "Training batch 31 with loss 0.30325\n",
      "------------------------------------------\n",
      "L1 loss: 0.06985701620578766\n",
      "Training batch 32 with loss 0.26682\n",
      "------------------------------------------\n",
      "L1 loss: 0.06267708539962769\n",
      "Training batch 33 with loss 0.25377\n",
      "------------------------------------------\n",
      "L1 loss: 0.07454290241003036\n",
      "Training batch 34 with loss 0.31322\n",
      "------------------------------------------\n",
      "L1 loss: 0.09585794061422348\n",
      "Training batch 35 with loss 0.33296\n",
      "------------------------------------------\n",
      "L1 loss: 0.08443965762853622\n",
      "Training batch 36 with loss 0.28536\n",
      "------------------------------------------\n",
      "L1 loss: 0.06515679508447647\n",
      "Training batch 37 with loss 0.25164\n",
      "------------------------------------------\n",
      "L1 loss: 0.06161338835954666\n",
      "Training batch 38 with loss 0.22491\n",
      "------------------------------------------\n",
      "L1 loss: 0.06530562043190002\n",
      "Training batch 39 with loss 0.29014\n",
      "------------------------------------------\n",
      "L1 loss: 0.07522965967655182\n",
      "Training batch 40 with loss 0.26531\n",
      "------------------------------------------\n",
      "L1 loss: 0.0544796958565712\n",
      "Training batch 41 with loss 0.27084\n",
      "------------------------------------------\n",
      "L1 loss: 0.07933521270751953\n",
      "Training batch 42 with loss 0.26730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07212536036968231\n",
      "Training batch 43 with loss 0.28868\n",
      "------------------------------------------\n",
      "L1 loss: 0.057750727981328964\n",
      "Training batch 44 with loss 0.26884\n",
      "------------------------------------------\n",
      "L1 loss: 0.09191811829805374\n",
      "Training batch 45 with loss 0.37072\n",
      "------------------------------------------\n",
      "L1 loss: 0.07532748579978943\n",
      "Training batch 46 with loss 0.27677\n",
      "------------------------------------------\n",
      "L1 loss: 0.10892699658870697\n",
      "Training batch 47 with loss 0.36472\n",
      "------------------------------------------\n",
      "L1 loss: 0.048631198704242706\n",
      "Training batch 48 with loss 0.24383\n",
      "------------------------------------------\n",
      "L1 loss: 0.07877574115991592\n",
      "Training batch 49 with loss 0.36316\n",
      "------------------------------------------\n",
      "L1 loss: 0.06926213949918747\n",
      "Training batch 50 with loss 0.43984\n",
      "------------------------------------------\n",
      "L1 loss: 0.07757796347141266\n",
      "Training batch 51 with loss 0.32939\n",
      "------------------------------------------\n",
      "L1 loss: 0.09110699594020844\n",
      "Training batch 52 with loss 0.29007\n",
      "------------------------------------------\n",
      "L1 loss: 0.07908495515584946\n",
      "Training batch 53 with loss 0.29500\n",
      "------------------------------------------\n",
      "L1 loss: 0.0780654102563858\n",
      "Training batch 54 with loss 0.27311\n",
      "------------------------------------------\n",
      "L1 loss: 0.05982073023915291\n",
      "Training batch 55 with loss 0.30433\n",
      "------------------------------------------\n",
      "L1 loss: 0.05536046624183655\n",
      "Training batch 56 with loss 0.24771\n",
      "------------------------------------------\n",
      "L1 loss: 0.0691826120018959\n",
      "Training batch 57 with loss 0.29481\n",
      "------------------------------------------\n",
      "L1 loss: 0.06681428849697113\n",
      "Training batch 58 with loss 0.27263\n",
      "------------------------------------------\n",
      "L1 loss: 0.07917794585227966\n",
      "Training batch 59 with loss 0.28880\n",
      "------------------------------------------\n",
      "L1 loss: 0.06711138784885406\n",
      "Training batch 60 with loss 0.28203\n",
      "------------------------------------------\n",
      "L1 loss: 0.1031845435500145\n",
      "Training batch 61 with loss 0.36553\n",
      "------------------------------------------\n",
      "L1 loss: 0.056438762694597244\n",
      "Training batch 62 with loss 0.25013\n",
      "------------------------------------------\n",
      "L1 loss: 0.05841643363237381\n",
      "Training batch 63 with loss 0.25466\n",
      "------------------------------------------\n",
      "L1 loss: 0.07305829972028732\n",
      "Training batch 64 with loss 0.29318\n",
      "------------------------------------------\n",
      "L1 loss: 0.061578184366226196\n",
      "Training batch 65 with loss 0.22478\n",
      "------------------------------------------\n",
      "L1 loss: 0.06683976948261261\n",
      "Training batch 66 with loss 0.27167\n",
      "------------------------------------------\n",
      "L1 loss: 0.06566601246595383\n",
      "Training batch 67 with loss 0.26272\n",
      "------------------------------------------\n",
      "L1 loss: 0.07284002751111984\n",
      "Training batch 68 with loss 0.29877\n",
      "------------------------------------------\n",
      "L1 loss: 0.0504874624311924\n",
      "Training batch 69 with loss 0.25631\n",
      "------------------------------------------\n",
      "L1 loss: 0.08163195848464966\n",
      "Training batch 70 with loss 0.32600\n",
      "------------------------------------------\n",
      "L1 loss: 0.07262998819351196\n",
      "Training batch 71 with loss 0.29278\n",
      "------------------------------------------\n",
      "L1 loss: 0.08071772009134293\n",
      "Training batch 72 with loss 0.31606\n",
      "------------------------------------------\n",
      "L1 loss: 0.07166241854429245\n",
      "Training batch 73 with loss 0.28175\n",
      "------------------------------------------\n",
      "L1 loss: 0.09809593111276627\n",
      "Training batch 74 with loss 0.32602\n",
      "------------------------------------------\n",
      "L1 loss: 0.07619594037532806\n",
      "Training batch 75 with loss 0.28510\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08626290410757065\n",
      "Training batch 76 with loss 0.29411\n",
      "------------------------------------------\n",
      "L1 loss: 0.07629597932100296\n",
      "Training batch 77 with loss 0.24920\n",
      "------------------------------------------\n",
      "L1 loss: 0.07857120782136917\n",
      "Training batch 78 with loss 0.29953\n",
      "------------------------------------------\n",
      "L1 loss: 0.1050955057144165\n",
      "Training batch 79 with loss 0.36748\n",
      "------------------------------------------\n",
      "L1 loss: 0.074784055352211\n",
      "Training batch 80 with loss 0.29046\n",
      "------------------------------------------\n",
      "L1 loss: 0.07330679148435593\n",
      "Training batch 81 with loss 0.28645\n",
      "------------------------------------------\n",
      "L1 loss: 0.08558991551399231\n",
      "Training batch 82 with loss 0.34274\n",
      "------------------------------------------\n",
      "L1 loss: 0.07569586485624313\n",
      "Training batch 83 with loss 0.28769\n",
      "------------------------------------------\n",
      "L1 loss: 0.06923571228981018\n",
      "Training batch 84 with loss 0.28342\n",
      "------------------------------------------\n",
      "L1 loss: 0.0592605397105217\n",
      "Training batch 85 with loss 0.26061\n",
      "------------------------------------------\n",
      "L1 loss: 0.08070686459541321\n",
      "Training batch 86 with loss 0.30643\n",
      "------------------------------------------\n",
      "L1 loss: 0.10072976350784302\n",
      "Training batch 87 with loss 0.39728\n",
      "------------------------------------------\n",
      "L1 loss: 0.07739618420600891\n",
      "Training batch 88 with loss 0.32416\n",
      "------------------------------------------\n",
      "L1 loss: 0.06739059090614319\n",
      "Training batch 89 with loss 0.27530\n",
      "------------------------------------------\n",
      "L1 loss: 0.07415571808815002\n",
      "Training batch 90 with loss 0.28759\n",
      "------------------------------------------\n",
      "L1 loss: 0.0612814724445343\n",
      "Training batch 91 with loss 0.25620\n",
      "------------------------------------------\n",
      "L1 loss: 0.0897456556558609\n",
      "Training batch 92 with loss 0.30012\n",
      "------------------------------------------\n",
      "L1 loss: 0.06835128366947174\n",
      "Training batch 93 with loss 0.26319\n",
      "------------------------------------------\n",
      "L1 loss: 0.08526027947664261\n",
      "Training batch 94 with loss 0.29997\n",
      "------------------------------------------\n",
      "L1 loss: 0.07215624302625656\n",
      "Training batch 95 with loss 0.24910\n",
      "------------------------------------------\n",
      "L1 loss: 0.059391479939222336\n",
      "Training batch 96 with loss 0.26131\n",
      "------------------------------------------\n",
      "L1 loss: 0.08605943620204926\n",
      "Training batch 97 with loss 0.34525\n",
      "------------------------------------------\n",
      "L1 loss: 0.057176414877176285\n",
      "Training batch 98 with loss 0.25269\n",
      "------------------------------------------\n",
      "L1 loss: 0.06171707436442375\n",
      "Training batch 99 with loss 0.25234\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2928347919881344\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4811\n",
      "------------------------------------------\n",
      "L1 loss: 0.06367409229278564\n",
      "Training batch 0 with loss 0.25378\n",
      "------------------------------------------\n",
      "L1 loss: 0.08223940432071686\n",
      "Training batch 1 with loss 0.27791\n",
      "------------------------------------------\n",
      "L1 loss: 0.09333068132400513\n",
      "Training batch 2 with loss 0.31520\n",
      "------------------------------------------\n",
      "L1 loss: 0.07932835817337036\n",
      "Training batch 3 with loss 0.27528\n",
      "------------------------------------------\n",
      "L1 loss: 0.08247124403715134\n",
      "Training batch 4 with loss 0.34433\n",
      "------------------------------------------\n",
      "L1 loss: 0.06590519100427628\n",
      "Training batch 5 with loss 0.32628\n",
      "------------------------------------------\n",
      "L1 loss: 0.0788092091679573\n",
      "Training batch 6 with loss 0.26539\n",
      "------------------------------------------\n",
      "L1 loss: 0.08238591998815536\n",
      "Training batch 7 with loss 0.34346\n",
      "------------------------------------------\n",
      "L1 loss: 0.0783991888165474\n",
      "Training batch 8 with loss 0.32987\n",
      "------------------------------------------\n",
      "L1 loss: 0.05816888064146042\n",
      "Training batch 9 with loss 0.27287\n",
      "------------------------------------------\n",
      "L1 loss: 0.08443676680326462\n",
      "Training batch 10 with loss 0.30289\n",
      "------------------------------------------\n",
      "L1 loss: 0.062051109969615936\n",
      "Training batch 11 with loss 0.27853\n",
      "------------------------------------------\n",
      "L1 loss: 0.07479336857795715\n",
      "Training batch 12 with loss 0.31633\n",
      "------------------------------------------\n",
      "L1 loss: 0.07006336003541946\n",
      "Training batch 13 with loss 0.26593\n",
      "------------------------------------------\n",
      "L1 loss: 0.051709793508052826\n",
      "Training batch 14 with loss 0.25484\n",
      "------------------------------------------\n",
      "L1 loss: 0.08263637125492096\n",
      "Training batch 15 with loss 0.27282\n",
      "------------------------------------------\n",
      "L1 loss: 0.061678577214479446\n",
      "Training batch 16 with loss 0.26569\n",
      "------------------------------------------\n",
      "L1 loss: 0.08514589816331863\n",
      "Training batch 17 with loss 0.32605\n",
      "------------------------------------------\n",
      "L1 loss: 0.08116397261619568\n",
      "Training batch 18 with loss 0.30450\n",
      "------------------------------------------\n",
      "L1 loss: 0.06308353692293167\n",
      "Training batch 19 with loss 0.22765\n",
      "------------------------------------------\n",
      "L1 loss: 0.07430198788642883\n",
      "Training batch 20 with loss 0.32864\n",
      "------------------------------------------\n",
      "L1 loss: 0.07253443449735641\n",
      "Training batch 21 with loss 0.27645\n",
      "------------------------------------------\n",
      "L1 loss: 0.07007806748151779\n",
      "Training batch 22 with loss 0.28863\n",
      "------------------------------------------\n",
      "L1 loss: 0.0914735272526741\n",
      "Training batch 23 with loss 0.36513\n",
      "------------------------------------------\n",
      "L1 loss: 0.06958574801683426\n",
      "Training batch 24 with loss 0.30266\n",
      "------------------------------------------\n",
      "L1 loss: 0.0776233822107315\n",
      "Training batch 25 with loss 0.30192\n",
      "------------------------------------------\n",
      "L1 loss: 0.07677296549081802\n",
      "Training batch 26 with loss 0.27653\n",
      "------------------------------------------\n",
      "L1 loss: 0.05663255602121353\n",
      "Training batch 27 with loss 0.29818\n",
      "------------------------------------------\n",
      "L1 loss: 0.05750589445233345\n",
      "Training batch 28 with loss 0.30525\n",
      "------------------------------------------\n",
      "L1 loss: 0.06214449927210808\n",
      "Training batch 29 with loss 0.27359\n",
      "------------------------------------------\n",
      "L1 loss: 0.06029289960861206\n",
      "Training batch 30 with loss 0.27016\n",
      "------------------------------------------\n",
      "L1 loss: 0.06461896002292633\n",
      "Training batch 31 with loss 0.30988\n",
      "------------------------------------------\n",
      "L1 loss: 0.07067415863275528\n",
      "Training batch 32 with loss 0.28750\n",
      "------------------------------------------\n",
      "L1 loss: 0.06522972881793976\n",
      "Training batch 33 with loss 0.27117\n",
      "------------------------------------------\n",
      "L1 loss: 0.07566562294960022\n",
      "Training batch 34 with loss 0.32556\n",
      "------------------------------------------\n",
      "L1 loss: 0.09563225507736206\n",
      "Training batch 35 with loss 0.34864\n",
      "------------------------------------------\n",
      "L1 loss: 0.08554381877183914\n",
      "Training batch 36 with loss 0.28128\n",
      "------------------------------------------\n",
      "L1 loss: 0.06734689325094223\n",
      "Training batch 37 with loss 0.26200\n",
      "------------------------------------------\n",
      "L1 loss: 0.06012482941150665\n",
      "Training batch 38 with loss 0.21492\n",
      "------------------------------------------\n",
      "L1 loss: 0.06888098269701004\n",
      "Training batch 39 with loss 0.30900\n",
      "------------------------------------------\n",
      "L1 loss: 0.07172989100217819\n",
      "Training batch 40 with loss 0.25486\n",
      "------------------------------------------\n",
      "L1 loss: 0.06008516624569893\n",
      "Training batch 41 with loss 0.29808\n",
      "------------------------------------------\n",
      "L1 loss: 0.07873600721359253\n",
      "Training batch 42 with loss 0.27567\n",
      "------------------------------------------\n",
      "L1 loss: 0.06903082132339478\n",
      "Training batch 43 with loss 0.29639\n",
      "------------------------------------------\n",
      "L1 loss: 0.055117085576057434\n",
      "Training batch 44 with loss 0.26870\n",
      "------------------------------------------\n",
      "L1 loss: 0.0925750806927681\n",
      "Training batch 45 with loss 0.35503\n",
      "------------------------------------------\n",
      "L1 loss: 0.0698990672826767\n",
      "Training batch 46 with loss 0.27309\n",
      "------------------------------------------\n",
      "L1 loss: 0.11192412674427032\n",
      "Training batch 47 with loss 0.38419\n",
      "------------------------------------------\n",
      "L1 loss: 0.0496395044028759\n",
      "Training batch 48 with loss 0.24661\n",
      "------------------------------------------\n",
      "L1 loss: 0.07777088135480881\n",
      "Training batch 49 with loss 0.32699\n",
      "------------------------------------------\n",
      "L1 loss: 0.07347382605075836\n",
      "Training batch 50 with loss 0.28643\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08305657655000687\n",
      "Training batch 51 with loss 0.33508\n",
      "------------------------------------------\n",
      "L1 loss: 0.08886594325304031\n",
      "Training batch 52 with loss 0.28926\n",
      "------------------------------------------\n",
      "L1 loss: 0.07978454232215881\n",
      "Training batch 53 with loss 0.28856\n",
      "------------------------------------------\n",
      "L1 loss: 0.0707370713353157\n",
      "Training batch 54 with loss 0.27012\n",
      "------------------------------------------\n",
      "L1 loss: 0.06190772354602814\n",
      "Training batch 55 with loss 0.32267\n",
      "------------------------------------------\n",
      "L1 loss: 0.06098919361829758\n",
      "Training batch 56 with loss 0.28353\n",
      "------------------------------------------\n",
      "L1 loss: 0.06892868131399155\n",
      "Training batch 57 with loss 0.27464\n",
      "------------------------------------------\n",
      "L1 loss: 0.06520257145166397\n",
      "Training batch 58 with loss 0.26346\n",
      "------------------------------------------\n",
      "L1 loss: 0.08170691132545471\n",
      "Training batch 59 with loss 0.28966\n",
      "------------------------------------------\n",
      "L1 loss: 0.06637319922447205\n",
      "Training batch 60 with loss 0.27438\n",
      "------------------------------------------\n",
      "L1 loss: 0.09895819425582886\n",
      "Training batch 61 with loss 0.35545\n",
      "------------------------------------------\n",
      "L1 loss: 0.0579877570271492\n",
      "Training batch 62 with loss 0.25590\n",
      "------------------------------------------\n",
      "L1 loss: 0.06424330174922943\n",
      "Training batch 63 with loss 0.27166\n",
      "------------------------------------------\n",
      "L1 loss: 0.06730714440345764\n",
      "Training batch 64 with loss 0.28807\n",
      "------------------------------------------\n",
      "L1 loss: 0.058480504900217056\n",
      "Training batch 65 with loss 0.22288\n",
      "------------------------------------------\n",
      "L1 loss: 0.06337913125753403\n",
      "Training batch 66 with loss 0.27738\n",
      "------------------------------------------\n",
      "L1 loss: 0.06605062633752823\n",
      "Training batch 67 with loss 0.26139\n",
      "------------------------------------------\n",
      "L1 loss: 0.07284103333950043\n",
      "Training batch 68 with loss 0.31099\n",
      "------------------------------------------\n",
      "L1 loss: 0.05129319801926613\n",
      "Training batch 69 with loss 0.26019\n",
      "------------------------------------------\n",
      "L1 loss: 0.08117569237947464\n",
      "Training batch 70 with loss 0.32529\n",
      "------------------------------------------\n",
      "L1 loss: 0.07113153487443924\n",
      "Training batch 71 with loss 0.29714\n",
      "------------------------------------------\n",
      "L1 loss: 0.08454590290784836\n",
      "Training batch 72 with loss 0.30494\n",
      "------------------------------------------\n",
      "L1 loss: 0.07415490597486496\n",
      "Training batch 73 with loss 0.30078\n",
      "------------------------------------------\n",
      "L1 loss: 0.0943349152803421\n",
      "Training batch 74 with loss 0.30469\n",
      "------------------------------------------\n",
      "L1 loss: 0.07368561625480652\n",
      "Training batch 75 with loss 0.28913\n",
      "------------------------------------------\n",
      "L1 loss: 0.08629387617111206\n",
      "Training batch 76 with loss 0.30775\n",
      "------------------------------------------\n",
      "L1 loss: 0.0783715769648552\n",
      "Training batch 77 with loss 0.28156\n",
      "------------------------------------------\n",
      "L1 loss: 0.07846205681562424\n",
      "Training batch 78 with loss 0.29424\n",
      "------------------------------------------\n",
      "L1 loss: 0.10453212261199951\n",
      "Training batch 79 with loss 0.36863\n",
      "------------------------------------------\n",
      "L1 loss: 0.07631688565015793\n",
      "Training batch 80 with loss 0.27970\n",
      "------------------------------------------\n",
      "L1 loss: 0.07546189427375793\n",
      "Training batch 81 with loss 0.29023\n",
      "------------------------------------------\n",
      "L1 loss: 0.08394976705312729\n",
      "Training batch 82 with loss 0.36284\n",
      "------------------------------------------\n",
      "L1 loss: 0.07672082632780075\n",
      "Training batch 83 with loss 0.28660\n",
      "------------------------------------------\n",
      "L1 loss: 0.07272984087467194\n",
      "Training batch 84 with loss 0.30428\n",
      "------------------------------------------\n",
      "L1 loss: 0.0614028200507164\n",
      "Training batch 85 with loss 0.26590\n",
      "------------------------------------------\n",
      "L1 loss: 0.08331715315580368\n",
      "Training batch 86 with loss 0.29005\n",
      "------------------------------------------\n",
      "L1 loss: 0.1015557274222374\n",
      "Training batch 87 with loss 0.39142\n",
      "------------------------------------------\n",
      "L1 loss: 0.07672276347875595\n",
      "Training batch 88 with loss 0.32065\n",
      "------------------------------------------\n",
      "L1 loss: 0.06790677458047867\n",
      "Training batch 89 with loss 0.26497\n",
      "------------------------------------------\n",
      "L1 loss: 0.06666982918977737\n",
      "Training batch 90 with loss 0.26931\n",
      "------------------------------------------\n",
      "L1 loss: 0.06565290689468384\n",
      "Training batch 91 with loss 0.26410\n",
      "------------------------------------------\n",
      "L1 loss: 0.09474164247512817\n",
      "Training batch 92 with loss 0.29455\n",
      "------------------------------------------\n",
      "L1 loss: 0.06078348308801651\n",
      "Training batch 93 with loss 0.23704\n",
      "------------------------------------------\n",
      "L1 loss: 0.08371239900588989\n",
      "Training batch 94 with loss 0.29826\n",
      "------------------------------------------\n",
      "L1 loss: 0.07126404345035553\n",
      "Training batch 95 with loss 0.25873\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683957189321518\n",
      "Training batch 96 with loss 0.29042\n",
      "------------------------------------------\n",
      "L1 loss: 0.08525415509939194\n",
      "Training batch 97 with loss 0.34540\n",
      "------------------------------------------\n",
      "L1 loss: 0.057183388620615005\n",
      "Training batch 98 with loss 0.24624\n",
      "------------------------------------------\n",
      "L1 loss: 0.06613418459892273\n",
      "Training batch 99 with loss 0.28053\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2929337388277054\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4812\n",
      "------------------------------------------\n",
      "L1 loss: 0.061640504747629166\n",
      "Training batch 0 with loss 0.24897\n",
      "------------------------------------------\n",
      "L1 loss: 0.08283472061157227\n",
      "Training batch 1 with loss 0.28575\n",
      "------------------------------------------\n",
      "L1 loss: 0.09120560437440872\n",
      "Training batch 2 with loss 0.32477\n",
      "------------------------------------------\n",
      "L1 loss: 0.07249423116445541\n",
      "Training batch 3 with loss 0.26047\n",
      "------------------------------------------\n",
      "L1 loss: 0.08270005881786346\n",
      "Training batch 4 with loss 0.32626\n",
      "------------------------------------------\n",
      "L1 loss: 0.06569810956716537\n",
      "Training batch 5 with loss 0.33408\n",
      "------------------------------------------\n",
      "L1 loss: 0.08003097772598267\n",
      "Training batch 6 with loss 0.26272\n",
      "------------------------------------------\n",
      "L1 loss: 0.08320914208889008\n",
      "Training batch 7 with loss 0.33804\n",
      "------------------------------------------\n",
      "L1 loss: 0.08363097906112671\n",
      "Training batch 8 with loss 0.36439\n",
      "------------------------------------------\n",
      "L1 loss: 0.06012129783630371\n",
      "Training batch 9 with loss 0.25308\n",
      "------------------------------------------\n",
      "L1 loss: 0.0806654840707779\n",
      "Training batch 10 with loss 0.28956\n",
      "------------------------------------------\n",
      "L1 loss: 0.05742258578538895\n",
      "Training batch 11 with loss 0.28979\n",
      "------------------------------------------\n",
      "L1 loss: 0.0729433074593544\n",
      "Training batch 12 with loss 0.31942\n",
      "------------------------------------------\n",
      "L1 loss: 0.05432260408997536\n",
      "Training batch 13 with loss 0.29166\n",
      "------------------------------------------\n",
      "L1 loss: 0.05279969796538353\n",
      "Training batch 14 with loss 0.25355\n",
      "------------------------------------------\n",
      "L1 loss: 0.08192020654678345\n",
      "Training batch 15 with loss 0.27396\n",
      "------------------------------------------\n",
      "L1 loss: 0.06247555837035179\n",
      "Training batch 16 with loss 0.25505\n",
      "------------------------------------------\n",
      "L1 loss: 0.0873105451464653\n",
      "Training batch 17 with loss 0.34470\n",
      "------------------------------------------\n",
      "L1 loss: 0.08081322908401489\n",
      "Training batch 18 with loss 0.30207\n",
      "------------------------------------------\n",
      "L1 loss: 0.06279557198286057\n",
      "Training batch 19 with loss 0.23426\n",
      "------------------------------------------\n",
      "L1 loss: 0.07238483428955078\n",
      "Training batch 20 with loss 0.32453\n",
      "------------------------------------------\n",
      "L1 loss: 0.07003872841596603\n",
      "Training batch 21 with loss 0.27610\n",
      "------------------------------------------\n",
      "L1 loss: 0.06894779950380325\n",
      "Training batch 22 with loss 0.29661\n",
      "------------------------------------------\n",
      "L1 loss: 0.08977482467889786\n",
      "Training batch 23 with loss 0.36015\n",
      "------------------------------------------\n",
      "L1 loss: 0.06883000582456589\n",
      "Training batch 24 with loss 0.30225\n",
      "------------------------------------------\n",
      "L1 loss: 0.07225614786148071\n",
      "Training batch 25 with loss 0.29941\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07691303640604019\n",
      "Training batch 26 with loss 0.27414\n",
      "------------------------------------------\n",
      "L1 loss: 0.056230608373880386\n",
      "Training batch 27 with loss 0.31566\n",
      "------------------------------------------\n",
      "L1 loss: 0.05737747997045517\n",
      "Training batch 28 with loss 0.27651\n",
      "------------------------------------------\n",
      "L1 loss: 0.06089605763554573\n",
      "Training batch 29 with loss 0.25272\n",
      "------------------------------------------\n",
      "L1 loss: 0.060937777161598206\n",
      "Training batch 30 with loss 0.26098\n",
      "------------------------------------------\n",
      "L1 loss: 0.063889279961586\n",
      "Training batch 31 with loss 0.30000\n",
      "------------------------------------------\n",
      "L1 loss: 0.07107477635145187\n",
      "Training batch 32 with loss 0.29114\n",
      "------------------------------------------\n",
      "L1 loss: 0.06139569357037544\n",
      "Training batch 33 with loss 0.25815\n",
      "------------------------------------------\n",
      "L1 loss: 0.07771962136030197\n",
      "Training batch 34 with loss 0.32761\n",
      "------------------------------------------\n",
      "L1 loss: 0.09459486603736877\n",
      "Training batch 35 with loss 0.33716\n",
      "------------------------------------------\n",
      "L1 loss: 0.08539479225873947\n",
      "Training batch 36 with loss 0.29391\n",
      "------------------------------------------\n",
      "L1 loss: 0.06631478667259216\n",
      "Training batch 37 with loss 0.25642\n",
      "------------------------------------------\n",
      "L1 loss: 0.06434614956378937\n",
      "Training batch 38 with loss 0.22345\n",
      "------------------------------------------\n",
      "L1 loss: 0.06432089954614639\n",
      "Training batch 39 with loss 0.28576\n",
      "------------------------------------------\n",
      "L1 loss: 0.06781657785177231\n",
      "Training batch 40 with loss 0.24742\n",
      "------------------------------------------\n",
      "L1 loss: 0.05954878032207489\n",
      "Training batch 41 with loss 0.28842\n",
      "------------------------------------------\n",
      "L1 loss: 0.08050999790430069\n",
      "Training batch 42 with loss 0.28488\n",
      "------------------------------------------\n",
      "L1 loss: 0.06824027001857758\n",
      "Training batch 43 with loss 0.26892\n",
      "------------------------------------------\n",
      "L1 loss: 0.05769716575741768\n",
      "Training batch 44 with loss 0.27421\n",
      "------------------------------------------\n",
      "L1 loss: 0.08496198058128357\n",
      "Training batch 45 with loss 0.49998\n",
      "------------------------------------------\n",
      "L1 loss: 0.07226040959358215\n",
      "Training batch 46 with loss 0.27144\n",
      "------------------------------------------\n",
      "L1 loss: 0.11144477128982544\n",
      "Training batch 47 with loss 0.37091\n",
      "------------------------------------------\n",
      "L1 loss: 0.04646875336766243\n",
      "Training batch 48 with loss 0.23524\n",
      "------------------------------------------\n",
      "L1 loss: 0.07723437249660492\n",
      "Training batch 49 with loss 0.35621\n",
      "------------------------------------------\n",
      "L1 loss: 0.0732308179140091\n",
      "Training batch 50 with loss 0.29324\n",
      "------------------------------------------\n",
      "L1 loss: 0.08203112334012985\n",
      "Training batch 51 with loss 0.34209\n",
      "------------------------------------------\n",
      "L1 loss: 0.08922003209590912\n",
      "Training batch 52 with loss 0.29845\n",
      "------------------------------------------\n",
      "L1 loss: 0.08029182255268097\n",
      "Training batch 53 with loss 0.28521\n",
      "------------------------------------------\n",
      "L1 loss: 0.07391728460788727\n",
      "Training batch 54 with loss 0.26229\n",
      "------------------------------------------\n",
      "L1 loss: 0.05868331342935562\n",
      "Training batch 55 with loss 0.30537\n",
      "------------------------------------------\n",
      "L1 loss: 0.05765794590115547\n",
      "Training batch 56 with loss 0.25662\n",
      "------------------------------------------\n",
      "L1 loss: 0.0698760375380516\n",
      "Training batch 57 with loss 0.29021\n",
      "------------------------------------------\n",
      "L1 loss: 0.06457013636827469\n",
      "Training batch 58 with loss 0.25988\n",
      "------------------------------------------\n",
      "L1 loss: 0.08060606569051743\n",
      "Training batch 59 with loss 0.28638\n",
      "------------------------------------------\n",
      "L1 loss: 0.06316429376602173\n",
      "Training batch 60 with loss 0.28168\n",
      "------------------------------------------\n",
      "L1 loss: 0.09827647358179092\n",
      "Training batch 61 with loss 0.35824\n",
      "------------------------------------------\n",
      "L1 loss: 0.05598103255033493\n",
      "Training batch 62 with loss 0.25956\n",
      "------------------------------------------\n",
      "L1 loss: 0.061818335205316544\n",
      "Training batch 63 with loss 0.25666\n",
      "------------------------------------------\n",
      "L1 loss: 0.06922636181116104\n",
      "Training batch 64 with loss 0.28039\n",
      "------------------------------------------\n",
      "L1 loss: 0.061748310923576355\n",
      "Training batch 65 with loss 0.24091\n",
      "------------------------------------------\n",
      "L1 loss: 0.06758873909711838\n",
      "Training batch 66 with loss 0.28256\n",
      "------------------------------------------\n",
      "L1 loss: 0.06401661783456802\n",
      "Training batch 67 with loss 0.26448\n",
      "------------------------------------------\n",
      "L1 loss: 0.07348235696554184\n",
      "Training batch 68 with loss 0.31243\n",
      "------------------------------------------\n",
      "L1 loss: 0.04960210248827934\n",
      "Training batch 69 with loss 0.26824\n",
      "------------------------------------------\n",
      "L1 loss: 0.08132804185152054\n",
      "Training batch 70 with loss 0.30308\n",
      "------------------------------------------\n",
      "L1 loss: 0.07307543605566025\n",
      "Training batch 71 with loss 0.30213\n",
      "------------------------------------------\n",
      "L1 loss: 0.08164172619581223\n",
      "Training batch 72 with loss 0.30105\n",
      "------------------------------------------\n",
      "L1 loss: 0.07070410251617432\n",
      "Training batch 73 with loss 0.30189\n",
      "------------------------------------------\n",
      "L1 loss: 0.09606586396694183\n",
      "Training batch 74 with loss 0.30547\n",
      "------------------------------------------\n",
      "L1 loss: 0.07538000494241714\n",
      "Training batch 75 with loss 0.28433\n",
      "------------------------------------------\n",
      "L1 loss: 0.08606080710887909\n",
      "Training batch 76 with loss 0.30432\n",
      "------------------------------------------\n",
      "L1 loss: 0.07825397700071335\n",
      "Training batch 77 with loss 0.26759\n",
      "------------------------------------------\n",
      "L1 loss: 0.08509066700935364\n",
      "Training batch 78 with loss 0.32247\n",
      "------------------------------------------\n",
      "L1 loss: 0.10548018664121628\n",
      "Training batch 79 with loss 0.37245\n",
      "------------------------------------------\n",
      "L1 loss: 0.07671201974153519\n",
      "Training batch 80 with loss 0.27705\n",
      "------------------------------------------\n",
      "L1 loss: 0.07191487401723862\n",
      "Training batch 81 with loss 0.26658\n",
      "------------------------------------------\n",
      "L1 loss: 0.08720618486404419\n",
      "Training batch 82 with loss 0.35856\n",
      "------------------------------------------\n",
      "L1 loss: 0.07964099198579788\n",
      "Training batch 83 with loss 0.28041\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702047199010849\n",
      "Training batch 84 with loss 0.30417\n",
      "------------------------------------------\n",
      "L1 loss: 0.058014124631881714\n",
      "Training batch 85 with loss 0.26940\n",
      "------------------------------------------\n",
      "L1 loss: 0.08047565072774887\n",
      "Training batch 86 with loss 0.30115\n",
      "------------------------------------------\n",
      "L1 loss: 0.10093321651220322\n",
      "Training batch 87 with loss 0.39584\n",
      "------------------------------------------\n",
      "L1 loss: 0.07910697907209396\n",
      "Training batch 88 with loss 0.32382\n",
      "------------------------------------------\n",
      "L1 loss: 0.07031746953725815\n",
      "Training batch 89 with loss 0.26270\n",
      "------------------------------------------\n",
      "L1 loss: 0.07361499220132828\n",
      "Training batch 90 with loss 0.28565\n",
      "------------------------------------------\n",
      "L1 loss: 0.06474021822214127\n",
      "Training batch 91 with loss 0.26528\n",
      "------------------------------------------\n",
      "L1 loss: 0.0904879942536354\n",
      "Training batch 92 with loss 0.29726\n",
      "------------------------------------------\n",
      "L1 loss: 0.06478863209486008\n",
      "Training batch 93 with loss 0.26253\n",
      "------------------------------------------\n",
      "L1 loss: 0.08558430522680283\n",
      "Training batch 94 with loss 0.31953\n",
      "------------------------------------------\n",
      "L1 loss: 0.07243082672357559\n",
      "Training batch 95 with loss 0.27659\n",
      "------------------------------------------\n",
      "L1 loss: 0.056366436183452606\n",
      "Training batch 96 with loss 0.27551\n",
      "------------------------------------------\n",
      "L1 loss: 0.08655613660812378\n",
      "Training batch 97 with loss 0.35547\n",
      "------------------------------------------\n",
      "L1 loss: 0.05666382983326912\n",
      "Training batch 98 with loss 0.24324\n",
      "------------------------------------------\n",
      "L1 loss: 0.06580758839845657\n",
      "Training batch 99 with loss 0.28302\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29416254252195356\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4813\n",
      "------------------------------------------\n",
      "L1 loss: 0.06093871220946312\n",
      "Training batch 0 with loss 0.25060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.0870346873998642\n",
      "Training batch 1 with loss 0.29995\n",
      "------------------------------------------\n",
      "L1 loss: 0.09542440623044968\n",
      "Training batch 2 with loss 0.33170\n",
      "------------------------------------------\n",
      "L1 loss: 0.07303274422883987\n",
      "Training batch 3 with loss 0.26674\n",
      "------------------------------------------\n",
      "L1 loss: 0.0851999893784523\n",
      "Training batch 4 with loss 0.33100\n",
      "------------------------------------------\n",
      "L1 loss: 0.06728427112102509\n",
      "Training batch 5 with loss 0.32434\n",
      "------------------------------------------\n",
      "L1 loss: 0.07800430804491043\n",
      "Training batch 6 with loss 0.26923\n",
      "------------------------------------------\n",
      "L1 loss: 0.08349262923002243\n",
      "Training batch 7 with loss 0.32933\n",
      "------------------------------------------\n",
      "L1 loss: 0.07792410999536514\n",
      "Training batch 8 with loss 0.31586\n",
      "------------------------------------------\n",
      "L1 loss: 0.05951223894953728\n",
      "Training batch 9 with loss 0.26096\n",
      "------------------------------------------\n",
      "L1 loss: 0.08393751829862595\n",
      "Training batch 10 with loss 0.29240\n",
      "------------------------------------------\n",
      "L1 loss: 0.05962667241692543\n",
      "Training batch 11 with loss 0.28158\n",
      "------------------------------------------\n",
      "L1 loss: 0.0757276862859726\n",
      "Training batch 12 with loss 0.29962\n",
      "------------------------------------------\n",
      "L1 loss: 0.06910292059183121\n",
      "Training batch 13 with loss 0.24498\n",
      "------------------------------------------\n",
      "L1 loss: 0.052328504621982574\n",
      "Training batch 14 with loss 0.23704\n",
      "------------------------------------------\n",
      "L1 loss: 0.08472046256065369\n",
      "Training batch 15 with loss 0.27371\n",
      "------------------------------------------\n",
      "L1 loss: 0.06313841789960861\n",
      "Training batch 16 with loss 0.26343\n",
      "------------------------------------------\n",
      "L1 loss: 0.08748503029346466\n",
      "Training batch 17 with loss 0.33715\n",
      "------------------------------------------\n",
      "L1 loss: 0.08075641840696335\n",
      "Training batch 18 with loss 0.29779\n",
      "------------------------------------------\n",
      "L1 loss: 0.0624840222299099\n",
      "Training batch 19 with loss 0.22748\n",
      "------------------------------------------\n",
      "L1 loss: 0.07178420573472977\n",
      "Training batch 20 with loss 0.32286\n",
      "------------------------------------------\n",
      "L1 loss: 0.0723922848701477\n",
      "Training batch 21 with loss 0.28011\n",
      "------------------------------------------\n",
      "L1 loss: 0.06773145496845245\n",
      "Training batch 22 with loss 0.30485\n",
      "------------------------------------------\n",
      "L1 loss: 0.08598152548074722\n",
      "Training batch 23 with loss 0.37037\n",
      "------------------------------------------\n",
      "L1 loss: 0.06793896853923798\n",
      "Training batch 24 with loss 0.29166\n",
      "------------------------------------------\n",
      "L1 loss: 0.0740196630358696\n",
      "Training batch 25 with loss 0.29617\n",
      "------------------------------------------\n",
      "L1 loss: 0.07785779237747192\n",
      "Training batch 26 with loss 0.27651\n",
      "------------------------------------------\n",
      "L1 loss: 0.055721182376146317\n",
      "Training batch 27 with loss 0.29190\n",
      "------------------------------------------\n",
      "L1 loss: 0.05756118521094322\n",
      "Training batch 28 with loss 0.29060\n",
      "------------------------------------------\n",
      "L1 loss: 0.06308805197477341\n",
      "Training batch 29 with loss 0.26750\n",
      "------------------------------------------\n",
      "L1 loss: 0.04885856807231903\n",
      "Training batch 30 with loss 0.38290\n",
      "------------------------------------------\n",
      "L1 loss: 0.05824975669384003\n",
      "Training batch 31 with loss 0.49008\n",
      "------------------------------------------\n",
      "L1 loss: 0.068909652531147\n",
      "Training batch 32 with loss 0.26812\n",
      "------------------------------------------\n",
      "L1 loss: 0.06215055286884308\n",
      "Training batch 33 with loss 0.26526\n",
      "------------------------------------------\n",
      "L1 loss: 0.07629068195819855\n",
      "Training batch 34 with loss 0.32818\n",
      "------------------------------------------\n",
      "L1 loss: 0.09381059557199478\n",
      "Training batch 35 with loss 0.34416\n",
      "------------------------------------------\n",
      "L1 loss: 0.08673286437988281\n",
      "Training batch 36 with loss 0.28563\n",
      "------------------------------------------\n",
      "L1 loss: 0.06684308499097824\n",
      "Training batch 37 with loss 0.24883\n",
      "------------------------------------------\n",
      "L1 loss: 0.06164521723985672\n",
      "Training batch 38 with loss 0.22010\n",
      "------------------------------------------\n",
      "L1 loss: 0.06676780432462692\n",
      "Training batch 39 with loss 0.29549\n",
      "------------------------------------------\n",
      "L1 loss: 0.06673972308635712\n",
      "Training batch 40 with loss 0.25141\n",
      "------------------------------------------\n",
      "L1 loss: 0.05929454788565636\n",
      "Training batch 41 with loss 0.29741\n",
      "------------------------------------------\n",
      "L1 loss: 0.07758684456348419\n",
      "Training batch 42 with loss 0.27308\n",
      "------------------------------------------\n",
      "L1 loss: 0.06901335716247559\n",
      "Training batch 43 with loss 0.26857\n",
      "------------------------------------------\n",
      "L1 loss: 0.05628598481416702\n",
      "Training batch 44 with loss 0.27879\n",
      "------------------------------------------\n",
      "L1 loss: 0.09316575527191162\n",
      "Training batch 45 with loss 0.36910\n",
      "------------------------------------------\n",
      "L1 loss: 0.07543815672397614\n",
      "Training batch 46 with loss 0.26492\n",
      "------------------------------------------\n",
      "L1 loss: 0.10843654721975327\n",
      "Training batch 47 with loss 0.37541\n",
      "------------------------------------------\n",
      "L1 loss: 0.04220277816057205\n",
      "Training batch 48 with loss 0.23170\n",
      "------------------------------------------\n",
      "L1 loss: 0.08390115946531296\n",
      "Training batch 49 with loss 0.36852\n",
      "------------------------------------------\n",
      "L1 loss: 0.07438545674085617\n",
      "Training batch 50 with loss 0.29239\n",
      "------------------------------------------\n",
      "L1 loss: 0.08320575207471848\n",
      "Training batch 51 with loss 0.33436\n",
      "------------------------------------------\n",
      "L1 loss: 0.08951079100370407\n",
      "Training batch 52 with loss 0.29887\n",
      "------------------------------------------\n",
      "L1 loss: 0.0801168829202652\n",
      "Training batch 53 with loss 0.29025\n",
      "------------------------------------------\n",
      "L1 loss: 0.0716644898056984\n",
      "Training batch 54 with loss 0.25902\n",
      "------------------------------------------\n",
      "L1 loss: 0.0588848851621151\n",
      "Training batch 55 with loss 0.30298\n",
      "------------------------------------------\n",
      "L1 loss: 0.04988257959485054\n",
      "Training batch 56 with loss 0.23878\n",
      "------------------------------------------\n",
      "L1 loss: 0.05883166193962097\n",
      "Training batch 57 with loss 0.35880\n",
      "------------------------------------------\n",
      "L1 loss: 0.0687042698264122\n",
      "Training batch 58 with loss 0.26223\n",
      "------------------------------------------\n",
      "L1 loss: 0.08263953775167465\n",
      "Training batch 59 with loss 0.29038\n",
      "------------------------------------------\n",
      "L1 loss: 0.06450031697750092\n",
      "Training batch 60 with loss 0.27590\n",
      "------------------------------------------\n",
      "L1 loss: 0.09244953840970993\n",
      "Training batch 61 with loss 0.33599\n",
      "------------------------------------------\n",
      "L1 loss: 0.05658489465713501\n",
      "Training batch 62 with loss 0.25636\n",
      "------------------------------------------\n",
      "L1 loss: 0.06128790229558945\n",
      "Training batch 63 with loss 0.26393\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702272579073906\n",
      "Training batch 64 with loss 0.27634\n",
      "------------------------------------------\n",
      "L1 loss: 0.05877439305186272\n",
      "Training batch 65 with loss 0.22665\n",
      "------------------------------------------\n",
      "L1 loss: 0.06512508541345596\n",
      "Training batch 66 with loss 0.27643\n",
      "------------------------------------------\n",
      "L1 loss: 0.06268749386072159\n",
      "Training batch 67 with loss 0.27471\n",
      "------------------------------------------\n",
      "L1 loss: 0.07289844006299973\n",
      "Training batch 68 with loss 0.30041\n",
      "------------------------------------------\n",
      "L1 loss: 0.05073186382651329\n",
      "Training batch 69 with loss 0.26756\n",
      "------------------------------------------\n",
      "L1 loss: 0.07876313477754593\n",
      "Training batch 70 with loss 0.29967\n",
      "------------------------------------------\n",
      "L1 loss: 0.07124441117048264\n",
      "Training batch 71 with loss 0.30313\n",
      "------------------------------------------\n",
      "L1 loss: 0.08468922972679138\n",
      "Training batch 72 with loss 0.29900\n",
      "------------------------------------------\n",
      "L1 loss: 0.07112252712249756\n",
      "Training batch 73 with loss 0.28709\n",
      "------------------------------------------\n",
      "L1 loss: 0.09484117478132248\n",
      "Training batch 74 with loss 0.31346\n",
      "------------------------------------------\n",
      "L1 loss: 0.07289788872003555\n",
      "Training batch 75 with loss 0.28072\n",
      "------------------------------------------\n",
      "L1 loss: 0.08637620508670807\n",
      "Training batch 76 with loss 0.30059\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07922929525375366\n",
      "Training batch 77 with loss 0.26421\n",
      "------------------------------------------\n",
      "L1 loss: 0.08196967095136642\n",
      "Training batch 78 with loss 0.30251\n",
      "------------------------------------------\n",
      "L1 loss: 0.10316480696201324\n",
      "Training batch 79 with loss 0.35436\n",
      "------------------------------------------\n",
      "L1 loss: 0.0758836567401886\n",
      "Training batch 80 with loss 0.26612\n",
      "------------------------------------------\n",
      "L1 loss: 0.08043431490659714\n",
      "Training batch 81 with loss 0.30842\n",
      "------------------------------------------\n",
      "L1 loss: 0.08519287407398224\n",
      "Training batch 82 with loss 0.36499\n",
      "------------------------------------------\n",
      "L1 loss: 0.07432734221220016\n",
      "Training batch 83 with loss 0.26957\n",
      "------------------------------------------\n",
      "L1 loss: 0.06755028665065765\n",
      "Training batch 84 with loss 0.57867\n",
      "------------------------------------------\n",
      "L1 loss: 0.06356175243854523\n",
      "Training batch 85 with loss 0.29469\n",
      "------------------------------------------\n",
      "L1 loss: 0.08043646067380905\n",
      "Training batch 86 with loss 0.28712\n",
      "------------------------------------------\n",
      "L1 loss: 0.10109412670135498\n",
      "Training batch 87 with loss 0.41011\n",
      "------------------------------------------\n",
      "L1 loss: 0.07788822799921036\n",
      "Training batch 88 with loss 0.32453\n",
      "------------------------------------------\n",
      "L1 loss: 0.06898809224367142\n",
      "Training batch 89 with loss 0.25664\n",
      "------------------------------------------\n",
      "L1 loss: 0.06084580346941948\n",
      "Training batch 90 with loss 0.58635\n",
      "------------------------------------------\n",
      "L1 loss: 0.06370719522237778\n",
      "Training batch 91 with loss 0.26110\n",
      "------------------------------------------\n",
      "L1 loss: 0.09540290385484695\n",
      "Training batch 92 with loss 0.31158\n",
      "------------------------------------------\n",
      "L1 loss: 0.06665483117103577\n",
      "Training batch 93 with loss 0.25713\n",
      "------------------------------------------\n",
      "L1 loss: 0.08567428588867188\n",
      "Training batch 94 with loss 0.30921\n",
      "------------------------------------------\n",
      "L1 loss: 0.07404103130102158\n",
      "Training batch 95 with loss 0.26647\n",
      "------------------------------------------\n",
      "L1 loss: 0.07172615826129913\n",
      "Training batch 96 with loss 0.31251\n",
      "------------------------------------------\n",
      "L1 loss: 0.08439178764820099\n",
      "Training batch 97 with loss 0.33421\n",
      "------------------------------------------\n",
      "L1 loss: 0.046256352216005325\n",
      "Training batch 98 with loss 0.31901\n",
      "------------------------------------------\n",
      "L1 loss: 0.06530516594648361\n",
      "Training batch 99 with loss 0.25441\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.3009499205648899\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4814\n",
      "------------------------------------------\n",
      "L1 loss: 0.06413184851408005\n",
      "Training batch 0 with loss 0.35906\n",
      "------------------------------------------\n",
      "L1 loss: 0.08375147730112076\n",
      "Training batch 1 with loss 0.29531\n",
      "------------------------------------------\n",
      "L1 loss: 0.09182234108448029\n",
      "Training batch 2 with loss 0.31485\n",
      "------------------------------------------\n",
      "L1 loss: 0.06929449737071991\n",
      "Training batch 3 with loss 0.24519\n",
      "------------------------------------------\n",
      "L1 loss: 0.08525092899799347\n",
      "Training batch 4 with loss 0.33228\n",
      "------------------------------------------\n",
      "L1 loss: 0.06494267284870148\n",
      "Training batch 5 with loss 0.31467\n",
      "------------------------------------------\n",
      "L1 loss: 0.08125746995210648\n",
      "Training batch 6 with loss 0.26764\n",
      "------------------------------------------\n",
      "L1 loss: 0.08471515774726868\n",
      "Training batch 7 with loss 0.31767\n",
      "------------------------------------------\n",
      "L1 loss: 0.08273494988679886\n",
      "Training batch 8 with loss 0.32893\n",
      "------------------------------------------\n",
      "L1 loss: 0.05603734403848648\n",
      "Training batch 9 with loss 0.25416\n",
      "------------------------------------------\n",
      "L1 loss: 0.08511248230934143\n",
      "Training batch 10 with loss 0.29835\n",
      "------------------------------------------\n",
      "L1 loss: 0.05573652312159538\n",
      "Training batch 11 with loss 0.29617\n",
      "------------------------------------------\n",
      "L1 loss: 0.07487665861845016\n",
      "Training batch 12 with loss 0.32463\n",
      "------------------------------------------\n",
      "L1 loss: 0.06956694275140762\n",
      "Training batch 13 with loss 0.25511\n",
      "------------------------------------------\n",
      "L1 loss: 0.053559787571430206\n",
      "Training batch 14 with loss 0.28984\n",
      "------------------------------------------\n",
      "L1 loss: 0.08370663225650787\n",
      "Training batch 15 with loss 0.28906\n",
      "------------------------------------------\n",
      "L1 loss: 0.06528076529502869\n",
      "Training batch 16 with loss 0.26458\n",
      "------------------------------------------\n",
      "L1 loss: 0.0859973207116127\n",
      "Training batch 17 with loss 0.33729\n",
      "------------------------------------------\n",
      "L1 loss: 0.07980991899967194\n",
      "Training batch 18 with loss 0.29771\n",
      "------------------------------------------\n",
      "L1 loss: 0.061252299696207047\n",
      "Training batch 19 with loss 0.24755\n",
      "------------------------------------------\n",
      "L1 loss: 0.07289960980415344\n",
      "Training batch 20 with loss 0.32087\n",
      "------------------------------------------\n",
      "L1 loss: 0.0723152607679367\n",
      "Training batch 21 with loss 0.27494\n",
      "------------------------------------------\n",
      "L1 loss: 0.06858031451702118\n",
      "Training batch 22 with loss 0.27602\n",
      "------------------------------------------\n",
      "L1 loss: 0.0827302411198616\n",
      "Training batch 23 with loss 0.36472\n",
      "------------------------------------------\n",
      "L1 loss: 0.06721460074186325\n",
      "Training batch 24 with loss 0.29989\n",
      "------------------------------------------\n",
      "L1 loss: 0.07380302995443344\n",
      "Training batch 25 with loss 0.30423\n",
      "------------------------------------------\n",
      "L1 loss: 0.07605767995119095\n",
      "Training batch 26 with loss 0.28098\n",
      "------------------------------------------\n",
      "L1 loss: 0.0549539253115654\n",
      "Training batch 27 with loss 0.30542\n",
      "------------------------------------------\n",
      "L1 loss: 0.05686023458838463\n",
      "Training batch 28 with loss 0.29144\n",
      "------------------------------------------\n",
      "L1 loss: 0.061781346797943115\n",
      "Training batch 29 with loss 0.25682\n",
      "------------------------------------------\n",
      "L1 loss: 0.062066733837127686\n",
      "Training batch 30 with loss 0.26557\n",
      "------------------------------------------\n",
      "L1 loss: 0.060884635895490646\n",
      "Training batch 31 with loss 0.29911\n",
      "------------------------------------------\n",
      "L1 loss: 0.07005468755960464\n",
      "Training batch 32 with loss 0.27647\n",
      "------------------------------------------\n",
      "L1 loss: 0.06362178921699524\n",
      "Training batch 33 with loss 0.25984\n",
      "------------------------------------------\n",
      "L1 loss: 0.07594031095504761\n",
      "Training batch 34 with loss 0.30887\n",
      "------------------------------------------\n",
      "L1 loss: 0.09151121228933334\n",
      "Training batch 35 with loss 0.33040\n",
      "------------------------------------------\n",
      "L1 loss: 0.0872560515999794\n",
      "Training batch 36 with loss 0.29108\n",
      "------------------------------------------\n",
      "L1 loss: 0.060863133519887924\n",
      "Training batch 37 with loss 0.50913\n",
      "------------------------------------------\n",
      "L1 loss: 0.062239594757556915\n",
      "Training batch 38 with loss 0.21315\n",
      "------------------------------------------\n",
      "L1 loss: 0.0671568363904953\n",
      "Training batch 39 with loss 0.30288\n",
      "------------------------------------------\n",
      "L1 loss: 0.06911598145961761\n",
      "Training batch 40 with loss 0.25417\n",
      "------------------------------------------\n",
      "L1 loss: 0.05987473204731941\n",
      "Training batch 41 with loss 0.29610\n",
      "------------------------------------------\n",
      "L1 loss: 0.0803886204957962\n",
      "Training batch 42 with loss 0.28154\n",
      "------------------------------------------\n",
      "L1 loss: 0.07314332574605942\n",
      "Training batch 43 with loss 0.30283\n",
      "------------------------------------------\n",
      "L1 loss: 0.05642300844192505\n",
      "Training batch 44 with loss 0.26852\n",
      "------------------------------------------\n",
      "L1 loss: 0.09165213257074356\n",
      "Training batch 45 with loss 0.36467\n",
      "------------------------------------------\n",
      "L1 loss: 0.07390079647302628\n",
      "Training batch 46 with loss 0.27304\n",
      "------------------------------------------\n",
      "L1 loss: 0.08166219294071198\n",
      "Training batch 47 with loss 0.46812\n",
      "------------------------------------------\n",
      "L1 loss: 0.047148387879133224\n",
      "Training batch 48 with loss 0.24466\n",
      "------------------------------------------\n",
      "L1 loss: 0.08043942600488663\n",
      "Training batch 49 with loss 0.35012\n",
      "------------------------------------------\n",
      "L1 loss: 0.07191447913646698\n",
      "Training batch 50 with loss 0.28421\n",
      "------------------------------------------\n",
      "L1 loss: 0.08044230192899704\n",
      "Training batch 51 with loss 0.32635\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.09007612615823746\n",
      "Training batch 52 with loss 0.27818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07663527876138687\n",
      "Training batch 53 with loss 0.29062\n",
      "------------------------------------------\n",
      "L1 loss: 0.07100696861743927\n",
      "Training batch 54 with loss 0.27818\n",
      "------------------------------------------\n",
      "L1 loss: 0.057551056146621704\n",
      "Training batch 55 with loss 0.30211\n",
      "------------------------------------------\n",
      "L1 loss: 0.051674388349056244\n",
      "Training batch 56 with loss 0.23749\n",
      "------------------------------------------\n",
      "L1 loss: 0.07020994275808334\n",
      "Training batch 57 with loss 0.28042\n",
      "------------------------------------------\n",
      "L1 loss: 0.0669165626168251\n",
      "Training batch 58 with loss 0.26545\n",
      "------------------------------------------\n",
      "L1 loss: 0.08060601353645325\n",
      "Training batch 59 with loss 0.29850\n",
      "------------------------------------------\n",
      "L1 loss: 0.063196562230587\n",
      "Training batch 60 with loss 0.27204\n",
      "------------------------------------------\n",
      "L1 loss: 0.10476701706647873\n",
      "Training batch 61 with loss 0.36418\n",
      "------------------------------------------\n",
      "L1 loss: 0.05534450337290764\n",
      "Training batch 62 with loss 0.24886\n",
      "------------------------------------------\n",
      "L1 loss: 0.060425642877817154\n",
      "Training batch 63 with loss 0.24070\n",
      "------------------------------------------\n",
      "L1 loss: 0.06946021318435669\n",
      "Training batch 64 with loss 0.27741\n",
      "------------------------------------------\n",
      "L1 loss: 0.059824828058481216\n",
      "Training batch 65 with loss 0.21657\n",
      "------------------------------------------\n",
      "L1 loss: 0.06930264830589294\n",
      "Training batch 66 with loss 0.28952\n",
      "------------------------------------------\n",
      "L1 loss: 0.06612174957990646\n",
      "Training batch 67 with loss 0.26862\n",
      "------------------------------------------\n",
      "L1 loss: 0.07290434837341309\n",
      "Training batch 68 with loss 0.30817\n",
      "------------------------------------------\n",
      "L1 loss: 0.05141419917345047\n",
      "Training batch 69 with loss 0.27585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07889533042907715\n",
      "Training batch 70 with loss 0.30421\n",
      "------------------------------------------\n",
      "L1 loss: 0.07141665369272232\n",
      "Training batch 71 with loss 0.32430\n",
      "------------------------------------------\n",
      "L1 loss: 0.08627363294363022\n",
      "Training batch 72 with loss 0.31329\n",
      "------------------------------------------\n",
      "L1 loss: 0.07311972975730896\n",
      "Training batch 73 with loss 0.31265\n",
      "------------------------------------------\n",
      "L1 loss: 0.09668249636888504\n",
      "Training batch 74 with loss 0.30773\n",
      "------------------------------------------\n",
      "L1 loss: 0.07607977837324142\n",
      "Training batch 75 with loss 0.27416\n",
      "------------------------------------------\n",
      "L1 loss: 0.08669637143611908\n",
      "Training batch 76 with loss 0.29219\n",
      "------------------------------------------\n",
      "L1 loss: 0.07862824201583862\n",
      "Training batch 77 with loss 0.26882\n",
      "------------------------------------------\n",
      "L1 loss: 0.0832478404045105\n",
      "Training batch 78 with loss 0.31287\n",
      "------------------------------------------\n",
      "L1 loss: 0.1046876460313797\n",
      "Training batch 79 with loss 0.37434\n",
      "------------------------------------------\n",
      "L1 loss: 0.07635673880577087\n",
      "Training batch 80 with loss 0.28671\n",
      "------------------------------------------\n",
      "L1 loss: 0.08074261248111725\n",
      "Training batch 81 with loss 0.30088\n",
      "------------------------------------------\n",
      "L1 loss: 0.08390899002552032\n",
      "Training batch 82 with loss 0.34594\n",
      "------------------------------------------\n",
      "L1 loss: 0.0751197412610054\n",
      "Training batch 83 with loss 0.27016\n",
      "------------------------------------------\n",
      "L1 loss: 0.06657353788614273\n",
      "Training batch 84 with loss 0.31218\n",
      "------------------------------------------\n",
      "L1 loss: 0.06630531698465347\n",
      "Training batch 85 with loss 0.29638\n",
      "------------------------------------------\n",
      "L1 loss: 0.08354345709085464\n",
      "Training batch 86 with loss 0.29451\n",
      "------------------------------------------\n",
      "L1 loss: 0.10046945512294769\n",
      "Training batch 87 with loss 0.40169\n",
      "------------------------------------------\n",
      "L1 loss: 0.07600076496601105\n",
      "Training batch 88 with loss 0.30437\n",
      "------------------------------------------\n",
      "L1 loss: 0.0660921260714531\n",
      "Training batch 89 with loss 0.27256\n",
      "------------------------------------------\n",
      "L1 loss: 0.07483657449483871\n",
      "Training batch 90 with loss 0.28077\n",
      "------------------------------------------\n",
      "L1 loss: 0.06135287880897522\n",
      "Training batch 91 with loss 0.24812\n",
      "------------------------------------------\n",
      "L1 loss: 0.09423407912254333\n",
      "Training batch 92 with loss 0.30340\n",
      "------------------------------------------\n",
      "L1 loss: 0.0639100894331932\n",
      "Training batch 93 with loss 0.24686\n",
      "------------------------------------------\n",
      "L1 loss: 0.08606019616127014\n",
      "Training batch 94 with loss 0.29194\n",
      "------------------------------------------\n",
      "L1 loss: 0.07315670698881149\n",
      "Training batch 95 with loss 0.27266\n",
      "------------------------------------------\n",
      "L1 loss: 0.060682300478219986\n",
      "Training batch 96 with loss 0.27939\n",
      "------------------------------------------\n",
      "L1 loss: 0.08756604045629501\n",
      "Training batch 97 with loss 0.36011\n",
      "------------------------------------------\n",
      "L1 loss: 0.05560465157032013\n",
      "Training batch 98 with loss 0.23006\n",
      "------------------------------------------\n",
      "L1 loss: 0.06290903687477112\n",
      "Training batch 99 with loss 0.27035\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29622670233249665\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4815\n",
      "------------------------------------------\n",
      "L1 loss: 0.061907678842544556\n",
      "Training batch 0 with loss 0.26323\n",
      "------------------------------------------\n",
      "L1 loss: 0.08394283056259155\n",
      "Training batch 1 with loss 0.28733\n",
      "------------------------------------------\n",
      "L1 loss: 0.09457023441791534\n",
      "Training batch 2 with loss 0.33946\n",
      "------------------------------------------\n",
      "L1 loss: 0.07405958324670792\n",
      "Training batch 3 with loss 0.25072\n",
      "------------------------------------------\n",
      "L1 loss: 0.08389077335596085\n",
      "Training batch 4 with loss 0.33475\n",
      "------------------------------------------\n",
      "L1 loss: 0.06756081432104111\n",
      "Training batch 5 with loss 0.32141\n",
      "------------------------------------------\n",
      "L1 loss: 0.08043263107538223\n",
      "Training batch 6 with loss 0.26806\n",
      "------------------------------------------\n",
      "L1 loss: 0.08342622220516205\n",
      "Training batch 7 with loss 0.32741\n",
      "------------------------------------------\n",
      "L1 loss: 0.0821433812379837\n",
      "Training batch 8 with loss 0.32437\n",
      "------------------------------------------\n",
      "L1 loss: 0.0586029514670372\n",
      "Training batch 9 with loss 0.24604\n",
      "------------------------------------------\n",
      "L1 loss: 0.08379095047712326\n",
      "Training batch 10 with loss 0.28839\n",
      "------------------------------------------\n",
      "L1 loss: 0.062387529760599136\n",
      "Training batch 11 with loss 0.30773\n",
      "------------------------------------------\n",
      "L1 loss: 0.0733068510890007\n",
      "Training batch 12 with loss 0.31328\n",
      "------------------------------------------\n",
      "L1 loss: 0.06968522816896439\n",
      "Training batch 13 with loss 0.23968\n",
      "------------------------------------------\n",
      "L1 loss: 0.05292120203375816\n",
      "Training batch 14 with loss 0.26759\n",
      "------------------------------------------\n",
      "L1 loss: 0.08272643387317657\n",
      "Training batch 15 with loss 0.27024\n",
      "------------------------------------------\n",
      "L1 loss: 0.059850383549928665\n",
      "Training batch 16 with loss 0.25054\n",
      "------------------------------------------\n",
      "L1 loss: 0.08711376786231995\n",
      "Training batch 17 with loss 0.34531\n",
      "------------------------------------------\n",
      "L1 loss: 0.07990674674510956\n",
      "Training batch 18 with loss 0.31503\n",
      "------------------------------------------\n",
      "L1 loss: 0.06258533149957657\n",
      "Training batch 19 with loss 0.23774\n",
      "------------------------------------------\n",
      "L1 loss: 0.07379256933927536\n",
      "Training batch 20 with loss 0.32902\n",
      "------------------------------------------\n",
      "L1 loss: 0.0513816736638546\n",
      "Training batch 21 with loss 0.52608\n",
      "------------------------------------------\n",
      "L1 loss: 0.06770776957273483\n",
      "Training batch 22 with loss 0.27843\n",
      "------------------------------------------\n",
      "L1 loss: 0.08873280137777328\n",
      "Training batch 23 with loss 0.35364\n",
      "------------------------------------------\n",
      "L1 loss: 0.06756123155355453\n",
      "Training batch 24 with loss 0.31003\n",
      "------------------------------------------\n",
      "L1 loss: 0.07127565890550613\n",
      "Training batch 25 with loss 0.28120\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937538832426071\n",
      "Training batch 26 with loss 0.27675\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.058001939207315445\n",
      "Training batch 27 with loss 0.30463\n",
      "------------------------------------------\n",
      "L1 loss: 0.057522013783454895\n",
      "Training batch 28 with loss 0.31186\n",
      "------------------------------------------\n",
      "L1 loss: 0.06155980005860329\n",
      "Training batch 29 with loss 0.25326\n",
      "------------------------------------------\n",
      "L1 loss: 0.06291444599628448\n",
      "Training batch 30 with loss 0.25966\n",
      "------------------------------------------\n",
      "L1 loss: 0.06336325407028198\n",
      "Training batch 31 with loss 0.30344\n",
      "------------------------------------------\n",
      "L1 loss: 0.0689975842833519\n",
      "Training batch 32 with loss 0.27129\n",
      "------------------------------------------\n",
      "L1 loss: 0.06344275921583176\n",
      "Training batch 33 with loss 0.25387\n",
      "------------------------------------------\n",
      "L1 loss: 0.07430385798215866\n",
      "Training batch 34 with loss 0.31754\n",
      "------------------------------------------\n",
      "L1 loss: 0.09601746499538422\n",
      "Training batch 35 with loss 0.32292\n",
      "------------------------------------------\n",
      "L1 loss: 0.08882632106542587\n",
      "Training batch 36 with loss 0.29279\n",
      "------------------------------------------\n",
      "L1 loss: 0.0671839714050293\n",
      "Training batch 37 with loss 0.25187\n",
      "------------------------------------------\n",
      "L1 loss: 0.06402896344661713\n",
      "Training batch 38 with loss 0.22081\n",
      "------------------------------------------\n",
      "L1 loss: 0.06844698637723923\n",
      "Training batch 39 with loss 0.28281\n",
      "------------------------------------------\n",
      "L1 loss: 0.06735723465681076\n",
      "Training batch 40 with loss 0.27295\n",
      "------------------------------------------\n",
      "L1 loss: 0.05775377154350281\n",
      "Training batch 41 with loss 0.28551\n",
      "------------------------------------------\n",
      "L1 loss: 0.08016616106033325\n",
      "Training batch 42 with loss 0.28355\n",
      "------------------------------------------\n",
      "L1 loss: 0.06892332434654236\n",
      "Training batch 43 with loss 0.28097\n",
      "------------------------------------------\n",
      "L1 loss: 0.05826271325349808\n",
      "Training batch 44 with loss 0.27718\n",
      "------------------------------------------\n",
      "L1 loss: 0.09244600683450699\n",
      "Training batch 45 with loss 0.37004\n",
      "------------------------------------------\n",
      "L1 loss: 0.07277289777994156\n",
      "Training batch 46 with loss 0.26842\n",
      "------------------------------------------\n",
      "L1 loss: 0.10916037112474442\n",
      "Training batch 47 with loss 0.36369\n",
      "------------------------------------------\n",
      "L1 loss: 0.04861975088715553\n",
      "Training batch 48 with loss 0.26769\n",
      "------------------------------------------\n",
      "L1 loss: 0.07872873544692993\n",
      "Training batch 49 with loss 0.35115\n",
      "------------------------------------------\n",
      "L1 loss: 0.07360972464084625\n",
      "Training batch 50 with loss 0.29685\n",
      "------------------------------------------\n",
      "L1 loss: 0.07713144272565842\n",
      "Training batch 51 with loss 0.31386\n",
      "------------------------------------------\n",
      "L1 loss: 0.0884694904088974\n",
      "Training batch 52 with loss 0.29125\n",
      "------------------------------------------\n",
      "L1 loss: 0.07962477952241898\n",
      "Training batch 53 with loss 0.28992\n",
      "------------------------------------------\n",
      "L1 loss: 0.07563218474388123\n",
      "Training batch 54 with loss 0.26279\n",
      "------------------------------------------\n",
      "L1 loss: 0.0626915767788887\n",
      "Training batch 55 with loss 0.30806\n",
      "------------------------------------------\n",
      "L1 loss: 0.05469731241464615\n",
      "Training batch 56 with loss 0.24900\n",
      "------------------------------------------\n",
      "L1 loss: 0.06810737401247025\n",
      "Training batch 57 with loss 0.28344\n",
      "------------------------------------------\n",
      "L1 loss: 0.06508997827768326\n",
      "Training batch 58 with loss 0.25685\n",
      "------------------------------------------\n",
      "L1 loss: 0.08087903261184692\n",
      "Training batch 59 with loss 0.29560\n",
      "------------------------------------------\n",
      "L1 loss: 0.06561432778835297\n",
      "Training batch 60 with loss 0.28881\n",
      "------------------------------------------\n",
      "L1 loss: 0.09862220287322998\n",
      "Training batch 61 with loss 0.34312\n",
      "------------------------------------------\n",
      "L1 loss: 0.05764809250831604\n",
      "Training batch 62 with loss 0.25309\n",
      "------------------------------------------\n",
      "L1 loss: 0.06183532625436783\n",
      "Training batch 63 with loss 0.24633\n",
      "------------------------------------------\n",
      "L1 loss: 0.0693029835820198\n",
      "Training batch 64 with loss 0.27569\n",
      "------------------------------------------\n",
      "L1 loss: 0.061909571290016174\n",
      "Training batch 65 with loss 0.23625\n",
      "------------------------------------------\n",
      "L1 loss: 0.06794897466897964\n",
      "Training batch 66 with loss 0.28462\n",
      "------------------------------------------\n",
      "L1 loss: 0.06604848057031631\n",
      "Training batch 67 with loss 0.26658\n",
      "------------------------------------------\n",
      "L1 loss: 0.07425983250141144\n",
      "Training batch 68 with loss 0.31483\n",
      "------------------------------------------\n",
      "L1 loss: 0.05136290192604065\n",
      "Training batch 69 with loss 0.24791\n",
      "------------------------------------------\n",
      "L1 loss: 0.07712576538324356\n",
      "Training batch 70 with loss 0.31948\n",
      "------------------------------------------\n",
      "L1 loss: 0.07115266472101212\n",
      "Training batch 71 with loss 0.29339\n",
      "------------------------------------------\n",
      "L1 loss: 0.08434484899044037\n",
      "Training batch 72 with loss 0.33488\n",
      "------------------------------------------\n",
      "L1 loss: 0.07531995326280594\n",
      "Training batch 73 with loss 0.30444\n",
      "------------------------------------------\n",
      "L1 loss: 0.09558472037315369\n",
      "Training batch 74 with loss 0.31684\n",
      "------------------------------------------\n",
      "L1 loss: 0.07497123628854752\n",
      "Training batch 75 with loss 0.28461\n",
      "------------------------------------------\n",
      "L1 loss: 0.08624044805765152\n",
      "Training batch 76 with loss 0.30195\n",
      "------------------------------------------\n",
      "L1 loss: 0.0790945515036583\n",
      "Training batch 77 with loss 0.27215\n",
      "------------------------------------------\n",
      "L1 loss: 0.08009295910596848\n",
      "Training batch 78 with loss 0.31821\n",
      "------------------------------------------\n",
      "L1 loss: 0.10893481224775314\n",
      "Training batch 79 with loss 0.37784\n",
      "------------------------------------------\n",
      "L1 loss: 0.07663775980472565\n",
      "Training batch 80 with loss 0.29701\n",
      "------------------------------------------\n",
      "L1 loss: 0.07761038839817047\n",
      "Training batch 81 with loss 0.29900\n",
      "------------------------------------------\n",
      "L1 loss: 0.08568081259727478\n",
      "Training batch 82 with loss 0.38322\n",
      "------------------------------------------\n",
      "L1 loss: 0.07596892863512039\n",
      "Training batch 83 with loss 0.27465\n",
      "------------------------------------------\n",
      "L1 loss: 0.06482154130935669\n",
      "Training batch 84 with loss 0.28007\n",
      "------------------------------------------\n",
      "L1 loss: 0.06383787095546722\n",
      "Training batch 85 with loss 0.27841\n",
      "------------------------------------------\n",
      "L1 loss: 0.07928729802370071\n",
      "Training batch 86 with loss 0.29018\n",
      "------------------------------------------\n",
      "L1 loss: 0.09910140186548233\n",
      "Training batch 87 with loss 0.40814\n",
      "------------------------------------------\n",
      "L1 loss: 0.07418697327375412\n",
      "Training batch 88 with loss 0.30976\n",
      "------------------------------------------\n",
      "L1 loss: 0.06123821437358856\n",
      "Training batch 89 with loss 0.24051\n",
      "------------------------------------------\n",
      "L1 loss: 0.07499849051237106\n",
      "Training batch 90 with loss 0.28855\n",
      "------------------------------------------\n",
      "L1 loss: 0.06325244158506393\n",
      "Training batch 91 with loss 0.26434\n",
      "------------------------------------------\n",
      "L1 loss: 0.08499113470315933\n",
      "Training batch 92 with loss 0.37601\n",
      "------------------------------------------\n",
      "L1 loss: 0.06788154691457748\n",
      "Training batch 93 with loss 0.27259\n",
      "------------------------------------------\n",
      "L1 loss: 0.07252726703882217\n",
      "Training batch 94 with loss 0.40602\n",
      "------------------------------------------\n",
      "L1 loss: 0.07239151746034622\n",
      "Training batch 95 with loss 0.26688\n",
      "------------------------------------------\n",
      "L1 loss: 0.05838195979595184\n",
      "Training batch 96 with loss 0.29021\n",
      "------------------------------------------\n",
      "L1 loss: 0.08145008981227875\n",
      "Training batch 97 with loss 0.37328\n",
      "------------------------------------------\n",
      "L1 loss: 0.05947446823120117\n",
      "Training batch 98 with loss 0.25277\n",
      "------------------------------------------\n",
      "L1 loss: 0.06754712015390396\n",
      "Training batch 99 with loss 0.28213\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2968378728628159\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4816\n",
      "------------------------------------------\n",
      "L1 loss: 0.062233008444309235\n",
      "Training batch 0 with loss 0.27781\n",
      "------------------------------------------\n",
      "L1 loss: 0.08368358016014099\n",
      "Training batch 1 with loss 0.28100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.09404118359088898\n",
      "Training batch 2 with loss 0.31997\n",
      "------------------------------------------\n",
      "L1 loss: 0.08055277168750763\n",
      "Training batch 3 with loss 0.28219\n",
      "------------------------------------------\n",
      "L1 loss: 0.08515232801437378\n",
      "Training batch 4 with loss 0.34582\n",
      "------------------------------------------\n",
      "L1 loss: 0.06948942691087723\n",
      "Training batch 5 with loss 0.34033\n",
      "------------------------------------------\n",
      "L1 loss: 0.08090458810329437\n",
      "Training batch 6 with loss 0.26424\n",
      "------------------------------------------\n",
      "L1 loss: 0.05847283825278282\n",
      "Training batch 7 with loss 0.43543\n",
      "------------------------------------------\n",
      "L1 loss: 0.07808815687894821\n",
      "Training batch 8 with loss 0.31615\n",
      "------------------------------------------\n",
      "L1 loss: 0.0613999105989933\n",
      "Training batch 9 with loss 0.26494\n",
      "------------------------------------------\n",
      "L1 loss: 0.08181748539209366\n",
      "Training batch 10 with loss 0.29092\n",
      "------------------------------------------\n",
      "L1 loss: 0.058806173503398895\n",
      "Training batch 11 with loss 0.29281\n",
      "------------------------------------------\n",
      "L1 loss: 0.07242745161056519\n",
      "Training batch 12 with loss 0.31559\n",
      "------------------------------------------\n",
      "L1 loss: 0.0703953355550766\n",
      "Training batch 13 with loss 0.26335\n",
      "------------------------------------------\n",
      "L1 loss: 0.053327012807130814\n",
      "Training batch 14 with loss 0.26001\n",
      "------------------------------------------\n",
      "L1 loss: 0.08200234919786453\n",
      "Training batch 15 with loss 0.26786\n",
      "------------------------------------------\n",
      "L1 loss: 0.06295444071292877\n",
      "Training batch 16 with loss 0.26985\n",
      "------------------------------------------\n",
      "L1 loss: 0.090131975710392\n",
      "Training batch 17 with loss 0.36185\n",
      "------------------------------------------\n",
      "L1 loss: 0.08154276013374329\n",
      "Training batch 18 with loss 0.31192\n",
      "------------------------------------------\n",
      "L1 loss: 0.06231599301099777\n",
      "Training batch 19 with loss 0.24628\n",
      "------------------------------------------\n",
      "L1 loss: 0.07321451604366302\n",
      "Training batch 20 with loss 0.32291\n",
      "------------------------------------------\n",
      "L1 loss: 0.07272354513406754\n",
      "Training batch 21 with loss 0.27269\n",
      "------------------------------------------\n",
      "L1 loss: 0.068628691136837\n",
      "Training batch 22 with loss 0.27212\n",
      "------------------------------------------\n",
      "L1 loss: 0.08622127026319504\n",
      "Training batch 23 with loss 0.35688\n",
      "------------------------------------------\n",
      "L1 loss: 0.06844346970319748\n",
      "Training batch 24 with loss 0.30254\n",
      "------------------------------------------\n",
      "L1 loss: 0.07494726777076721\n",
      "Training batch 25 with loss 0.30354\n",
      "------------------------------------------\n",
      "L1 loss: 0.07350489497184753\n",
      "Training batch 26 with loss 0.27552\n",
      "------------------------------------------\n",
      "L1 loss: 0.05603092908859253\n",
      "Training batch 27 with loss 0.29290\n",
      "------------------------------------------\n",
      "L1 loss: 0.05775038152933121\n",
      "Training batch 28 with loss 0.29818\n",
      "------------------------------------------\n",
      "L1 loss: 0.05860823765397072\n",
      "Training batch 29 with loss 0.25024\n",
      "------------------------------------------\n",
      "L1 loss: 0.06314047425985336\n",
      "Training batch 30 with loss 0.28042\n",
      "------------------------------------------\n",
      "L1 loss: 0.0637931376695633\n",
      "Training batch 31 with loss 0.30199\n",
      "------------------------------------------\n",
      "L1 loss: 0.0689697265625\n",
      "Training batch 32 with loss 0.26977\n",
      "------------------------------------------\n",
      "L1 loss: 0.06588740646839142\n",
      "Training batch 33 with loss 0.27016\n",
      "------------------------------------------\n",
      "L1 loss: 0.07463602721691132\n",
      "Training batch 34 with loss 0.33643\n",
      "------------------------------------------\n",
      "L1 loss: 0.0953061655163765\n",
      "Training batch 35 with loss 0.33653\n",
      "------------------------------------------\n",
      "L1 loss: 0.08569370955228806\n",
      "Training batch 36 with loss 0.27421\n",
      "------------------------------------------\n",
      "L1 loss: 0.06716936826705933\n",
      "Training batch 37 with loss 0.26054\n",
      "------------------------------------------\n",
      "L1 loss: 0.06330084055662155\n",
      "Training batch 38 with loss 0.22136\n",
      "------------------------------------------\n",
      "L1 loss: 0.06721749156713486\n",
      "Training batch 39 with loss 0.31073\n",
      "------------------------------------------\n",
      "L1 loss: 0.06747342646121979\n",
      "Training batch 40 with loss 0.24936\n",
      "------------------------------------------\n",
      "L1 loss: 0.04475844278931618\n",
      "Training batch 41 with loss 0.34736\n",
      "------------------------------------------\n",
      "L1 loss: 0.07748585939407349\n",
      "Training batch 42 with loss 0.29692\n",
      "------------------------------------------\n",
      "L1 loss: 0.07106894999742508\n",
      "Training batch 43 with loss 0.28253\n",
      "------------------------------------------\n",
      "L1 loss: 0.05372201278805733\n",
      "Training batch 44 with loss 0.27007\n",
      "------------------------------------------\n",
      "L1 loss: 0.0931389108300209\n",
      "Training batch 45 with loss 0.36999\n",
      "------------------------------------------\n",
      "L1 loss: 0.07260709255933762\n",
      "Training batch 46 with loss 0.27960\n",
      "------------------------------------------\n",
      "L1 loss: 0.11046808958053589\n",
      "Training batch 47 with loss 0.38701\n",
      "------------------------------------------\n",
      "L1 loss: 0.047654230147600174\n",
      "Training batch 48 with loss 0.25219\n",
      "------------------------------------------\n",
      "L1 loss: 0.07847323268651962\n",
      "Training batch 49 with loss 0.36337\n",
      "------------------------------------------\n",
      "L1 loss: 0.07250124961137772\n",
      "Training batch 50 with loss 0.30552\n",
      "------------------------------------------\n",
      "L1 loss: 0.07792554050683975\n",
      "Training batch 51 with loss 0.32378\n",
      "------------------------------------------\n",
      "L1 loss: 0.08943196386098862\n",
      "Training batch 52 with loss 0.30478\n",
      "------------------------------------------\n",
      "L1 loss: 0.08008845150470734\n",
      "Training batch 53 with loss 0.30442\n",
      "------------------------------------------\n",
      "L1 loss: 0.07421553879976273\n",
      "Training batch 54 with loss 0.26412\n",
      "------------------------------------------\n",
      "L1 loss: 0.05946550518274307\n",
      "Training batch 55 with loss 0.30223\n",
      "------------------------------------------\n",
      "L1 loss: 0.05947015807032585\n",
      "Training batch 56 with loss 0.26292\n",
      "------------------------------------------\n",
      "L1 loss: 0.06742428988218307\n",
      "Training batch 57 with loss 0.28143\n",
      "------------------------------------------\n",
      "L1 loss: 0.06900697201490402\n",
      "Training batch 58 with loss 0.26985\n",
      "------------------------------------------\n",
      "L1 loss: 0.07913356274366379\n",
      "Training batch 59 with loss 0.64490\n",
      "------------------------------------------\n",
      "L1 loss: 0.06695108115673065\n",
      "Training batch 60 with loss 0.28230\n",
      "------------------------------------------\n",
      "L1 loss: 0.09684177488088608\n",
      "Training batch 61 with loss 0.33430\n",
      "------------------------------------------\n",
      "L1 loss: 0.05510956421494484\n",
      "Training batch 62 with loss 0.25407\n",
      "------------------------------------------\n",
      "L1 loss: 0.05007219314575195\n",
      "Training batch 63 with loss 0.20835\n",
      "------------------------------------------\n",
      "L1 loss: 0.06137480586767197\n",
      "Training batch 64 with loss 0.39529\n",
      "------------------------------------------\n",
      "L1 loss: 0.06041032448410988\n",
      "Training batch 65 with loss 0.36477\n",
      "------------------------------------------\n",
      "L1 loss: 0.06727754324674606\n",
      "Training batch 66 with loss 0.28644\n",
      "------------------------------------------\n",
      "L1 loss: 0.061114467680454254\n",
      "Training batch 67 with loss 0.26883\n",
      "------------------------------------------\n",
      "L1 loss: 0.07537706941366196\n",
      "Training batch 68 with loss 0.30962\n",
      "------------------------------------------\n",
      "L1 loss: 0.050698086619377136\n",
      "Training batch 69 with loss 0.25819\n",
      "------------------------------------------\n",
      "L1 loss: 0.08158516883850098\n",
      "Training batch 70 with loss 0.30271\n",
      "------------------------------------------\n",
      "L1 loss: 0.07158532738685608\n",
      "Training batch 71 with loss 0.28081\n",
      "------------------------------------------\n",
      "L1 loss: 0.08081410080194473\n",
      "Training batch 72 with loss 0.29869\n",
      "------------------------------------------\n",
      "L1 loss: 0.06823278218507767\n",
      "Training batch 73 with loss 0.27842\n",
      "------------------------------------------\n",
      "L1 loss: 0.09584387391805649\n",
      "Training batch 74 with loss 0.34342\n",
      "------------------------------------------\n",
      "L1 loss: 0.07326523214578629\n",
      "Training batch 75 with loss 0.27616\n",
      "------------------------------------------\n",
      "L1 loss: 0.08634145557880402\n",
      "Training batch 76 with loss 0.31424\n",
      "------------------------------------------\n",
      "L1 loss: 0.07739811390638351\n",
      "Training batch 77 with loss 0.26315\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08207201957702637\n",
      "Training batch 78 with loss 0.30439\n",
      "------------------------------------------\n",
      "L1 loss: 0.1036602333188057\n",
      "Training batch 79 with loss 0.36296\n",
      "------------------------------------------\n",
      "L1 loss: 0.07531484216451645\n",
      "Training batch 80 with loss 0.28767\n",
      "------------------------------------------\n",
      "L1 loss: 0.0730087086558342\n",
      "Training batch 81 with loss 0.28144\n",
      "------------------------------------------\n",
      "L1 loss: 0.08507229387760162\n",
      "Training batch 82 with loss 0.37210\n",
      "------------------------------------------\n",
      "L1 loss: 0.07363162934780121\n",
      "Training batch 83 with loss 0.28153\n",
      "------------------------------------------\n",
      "L1 loss: 0.0704856738448143\n",
      "Training batch 84 with loss 0.30103\n",
      "------------------------------------------\n",
      "L1 loss: 0.06580886989831924\n",
      "Training batch 85 with loss 0.29996\n",
      "------------------------------------------\n",
      "L1 loss: 0.0848488137125969\n",
      "Training batch 86 with loss 0.29966\n",
      "------------------------------------------\n",
      "L1 loss: 0.09992111474275589\n",
      "Training batch 87 with loss 0.39878\n",
      "------------------------------------------\n",
      "L1 loss: 0.07581691443920135\n",
      "Training batch 88 with loss 0.32369\n",
      "------------------------------------------\n",
      "L1 loss: 0.07084875553846359\n",
      "Training batch 89 with loss 0.27223\n",
      "------------------------------------------\n",
      "L1 loss: 0.07103456556797028\n",
      "Training batch 90 with loss 0.27111\n",
      "------------------------------------------\n",
      "L1 loss: 0.06143666058778763\n",
      "Training batch 91 with loss 0.26679\n",
      "------------------------------------------\n",
      "L1 loss: 0.09231763333082199\n",
      "Training batch 92 with loss 0.30915\n",
      "------------------------------------------\n",
      "L1 loss: 0.05955956503748894\n",
      "Training batch 93 with loss 0.23671\n",
      "------------------------------------------\n",
      "L1 loss: 0.08632110059261322\n",
      "Training batch 94 with loss 0.30754\n",
      "------------------------------------------\n",
      "L1 loss: 0.07232114672660828\n",
      "Training batch 95 with loss 0.25119\n",
      "------------------------------------------\n",
      "L1 loss: 0.061879873275756836\n",
      "Training batch 96 with loss 0.29346\n",
      "------------------------------------------\n",
      "L1 loss: 0.08748573809862137\n",
      "Training batch 97 with loss 0.37003\n",
      "------------------------------------------\n",
      "L1 loss: 0.05579277500510216\n",
      "Training batch 98 with loss 0.23981\n",
      "------------------------------------------\n",
      "L1 loss: 0.06287054717540741\n",
      "Training batch 99 with loss 0.25466\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.3008600935339928\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4817\n",
      "------------------------------------------\n",
      "L1 loss: 0.06217820569872856\n",
      "Training batch 0 with loss 0.24620\n",
      "------------------------------------------\n",
      "L1 loss: 0.08197401463985443\n",
      "Training batch 1 with loss 0.29378\n",
      "------------------------------------------\n",
      "L1 loss: 0.09427312761545181\n",
      "Training batch 2 with loss 0.33957\n",
      "------------------------------------------\n",
      "L1 loss: 0.07949609309434891\n",
      "Training batch 3 with loss 0.27238\n",
      "------------------------------------------\n",
      "L1 loss: 0.08535203337669373\n",
      "Training batch 4 with loss 0.32194\n",
      "------------------------------------------\n",
      "L1 loss: 0.06808148324489594\n",
      "Training batch 5 with loss 0.33832\n",
      "------------------------------------------\n",
      "L1 loss: 0.05358409881591797\n",
      "Training batch 6 with loss 0.28680\n",
      "------------------------------------------\n",
      "L1 loss: 0.08493834733963013\n",
      "Training batch 7 with loss 0.32769\n",
      "------------------------------------------\n",
      "L1 loss: 0.0786755234003067\n",
      "Training batch 8 with loss 0.32294\n",
      "------------------------------------------\n",
      "L1 loss: 0.059272706508636475\n",
      "Training batch 9 with loss 0.24882\n",
      "------------------------------------------\n",
      "L1 loss: 0.08107240498065948\n",
      "Training batch 10 with loss 0.28957\n",
      "------------------------------------------\n",
      "L1 loss: 0.061079636216163635\n",
      "Training batch 11 with loss 0.29252\n",
      "------------------------------------------\n",
      "L1 loss: 0.07642213255167007\n",
      "Training batch 12 with loss 0.31501\n",
      "------------------------------------------\n",
      "L1 loss: 0.06932782381772995\n",
      "Training batch 13 with loss 0.25853\n",
      "------------------------------------------\n",
      "L1 loss: 0.05116734653711319\n",
      "Training batch 14 with loss 0.24266\n",
      "------------------------------------------\n",
      "L1 loss: 0.08506406098604202\n",
      "Training batch 15 with loss 0.26865\n",
      "------------------------------------------\n",
      "L1 loss: 0.0626668706536293\n",
      "Training batch 16 with loss 0.26672\n",
      "------------------------------------------\n",
      "L1 loss: 0.08368896692991257\n",
      "Training batch 17 with loss 0.46592\n",
      "------------------------------------------\n",
      "L1 loss: 0.08012790977954865\n",
      "Training batch 18 with loss 0.29795\n",
      "------------------------------------------\n",
      "L1 loss: 0.06343472003936768\n",
      "Training batch 19 with loss 0.23546\n",
      "------------------------------------------\n",
      "L1 loss: 0.07245583832263947\n",
      "Training batch 20 with loss 0.33917\n",
      "------------------------------------------\n",
      "L1 loss: 0.07205228507518768\n",
      "Training batch 21 with loss 0.28444\n",
      "------------------------------------------\n",
      "L1 loss: 0.06962715834379196\n",
      "Training batch 22 with loss 0.28912\n",
      "------------------------------------------\n",
      "L1 loss: 0.08308882266283035\n",
      "Training batch 23 with loss 0.37016\n",
      "------------------------------------------\n",
      "L1 loss: 0.06698506325483322\n",
      "Training batch 24 with loss 0.28223\n",
      "------------------------------------------\n",
      "L1 loss: 0.07527869194746017\n",
      "Training batch 25 with loss 0.29781\n",
      "------------------------------------------\n",
      "L1 loss: 0.07935575395822525\n",
      "Training batch 26 with loss 0.29564\n",
      "------------------------------------------\n",
      "L1 loss: 0.05749361589550972\n",
      "Training batch 27 with loss 0.30355\n",
      "------------------------------------------\n",
      "L1 loss: 0.05780426412820816\n",
      "Training batch 28 with loss 0.28358\n",
      "------------------------------------------\n",
      "L1 loss: 0.053055085241794586\n",
      "Training batch 29 with loss 0.23161\n",
      "------------------------------------------\n",
      "L1 loss: 0.05109042674303055\n",
      "Training batch 30 with loss 0.46361\n",
      "------------------------------------------\n",
      "L1 loss: 0.0629664957523346\n",
      "Training batch 31 with loss 0.28581\n",
      "------------------------------------------\n",
      "L1 loss: 0.06872036308050156\n",
      "Training batch 32 with loss 0.27082\n",
      "------------------------------------------\n",
      "L1 loss: 0.06242679804563522\n",
      "Training batch 33 with loss 0.26156\n",
      "------------------------------------------\n",
      "L1 loss: 0.07490944862365723\n",
      "Training batch 34 with loss 0.31617\n",
      "------------------------------------------\n",
      "L1 loss: 0.09516940265893936\n",
      "Training batch 35 with loss 0.33400\n",
      "------------------------------------------\n",
      "L1 loss: 0.08383921533823013\n",
      "Training batch 36 with loss 0.28594\n",
      "------------------------------------------\n",
      "L1 loss: 0.0678652748465538\n",
      "Training batch 37 with loss 0.26566\n",
      "------------------------------------------\n",
      "L1 loss: 0.0645047128200531\n",
      "Training batch 38 with loss 0.22563\n",
      "------------------------------------------\n",
      "L1 loss: 0.0686180368065834\n",
      "Training batch 39 with loss 0.29925\n",
      "------------------------------------------\n",
      "L1 loss: 0.06734725087881088\n",
      "Training batch 40 with loss 0.25553\n",
      "------------------------------------------\n",
      "L1 loss: 0.05728275328874588\n",
      "Training batch 41 with loss 0.28423\n",
      "------------------------------------------\n",
      "L1 loss: 0.0747079998254776\n",
      "Training batch 42 with loss 0.30071\n",
      "------------------------------------------\n",
      "L1 loss: 0.06606735289096832\n",
      "Training batch 43 with loss 0.27379\n",
      "------------------------------------------\n",
      "L1 loss: 0.056155264377593994\n",
      "Training batch 44 with loss 0.29331\n",
      "------------------------------------------\n",
      "L1 loss: 0.0912252888083458\n",
      "Training batch 45 with loss 0.36448\n",
      "------------------------------------------\n",
      "L1 loss: 0.07593567669391632\n",
      "Training batch 46 with loss 0.27149\n",
      "------------------------------------------\n",
      "L1 loss: 0.10881095379590988\n",
      "Training batch 47 with loss 0.37219\n",
      "------------------------------------------\n",
      "L1 loss: 0.04609295725822449\n",
      "Training batch 48 with loss 0.23046\n",
      "------------------------------------------\n",
      "L1 loss: 0.08074356615543365\n",
      "Training batch 49 with loss 0.36593\n",
      "------------------------------------------\n",
      "L1 loss: 0.07103711366653442\n",
      "Training batch 50 with loss 0.28339\n",
      "------------------------------------------\n",
      "L1 loss: 0.08249881863594055\n",
      "Training batch 51 with loss 0.33842\n",
      "------------------------------------------\n",
      "L1 loss: 0.0893949568271637\n",
      "Training batch 52 with loss 0.30316\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07922827452421188\n",
      "Training batch 53 with loss 0.30152\n",
      "------------------------------------------\n",
      "L1 loss: 0.07655151188373566\n",
      "Training batch 54 with loss 0.28182\n",
      "------------------------------------------\n",
      "L1 loss: 0.06060108542442322\n",
      "Training batch 55 with loss 0.32593\n",
      "------------------------------------------\n",
      "L1 loss: 0.06135770305991173\n",
      "Training batch 56 with loss 0.27295\n",
      "------------------------------------------\n",
      "L1 loss: 0.06811068952083588\n",
      "Training batch 57 with loss 0.28457\n",
      "------------------------------------------\n",
      "L1 loss: 0.06522264331579208\n",
      "Training batch 58 with loss 0.25947\n",
      "------------------------------------------\n",
      "L1 loss: 0.07791320234537125\n",
      "Training batch 59 with loss 0.41026\n",
      "------------------------------------------\n",
      "L1 loss: 0.06483534723520279\n",
      "Training batch 60 with loss 0.28925\n",
      "------------------------------------------\n",
      "L1 loss: 0.10165757685899734\n",
      "Training batch 61 with loss 0.35762\n",
      "------------------------------------------\n",
      "L1 loss: 0.038724396377801895\n",
      "Training batch 62 with loss 0.43381\n",
      "------------------------------------------\n",
      "L1 loss: 0.06247026100754738\n",
      "Training batch 63 with loss 0.24807\n",
      "------------------------------------------\n",
      "L1 loss: 0.06889400631189346\n",
      "Training batch 64 with loss 0.28520\n",
      "------------------------------------------\n",
      "L1 loss: 0.06011948734521866\n",
      "Training batch 65 with loss 0.22385\n",
      "------------------------------------------\n",
      "L1 loss: 0.06537237763404846\n",
      "Training batch 66 with loss 0.26522\n",
      "------------------------------------------\n",
      "L1 loss: 0.06323805451393127\n",
      "Training batch 67 with loss 0.26867\n",
      "------------------------------------------\n",
      "L1 loss: 0.07648128271102905\n",
      "Training batch 68 with loss 0.31335\n",
      "------------------------------------------\n",
      "L1 loss: 0.048738494515419006\n",
      "Training batch 69 with loss 0.25142\n",
      "------------------------------------------\n",
      "L1 loss: 0.07662678509950638\n",
      "Training batch 70 with loss 0.30408\n",
      "------------------------------------------\n",
      "L1 loss: 0.07287759333848953\n",
      "Training batch 71 with loss 0.29696\n",
      "------------------------------------------\n",
      "L1 loss: 0.08353915810585022\n",
      "Training batch 72 with loss 0.31974\n",
      "------------------------------------------\n",
      "L1 loss: 0.06952045857906342\n",
      "Training batch 73 with loss 0.28955\n",
      "------------------------------------------\n",
      "L1 loss: 0.09545387327671051\n",
      "Training batch 74 with loss 0.31432\n",
      "------------------------------------------\n",
      "L1 loss: 0.0761251226067543\n",
      "Training batch 75 with loss 0.28279\n",
      "------------------------------------------\n",
      "L1 loss: 0.08719933778047562\n",
      "Training batch 76 with loss 0.28815\n",
      "------------------------------------------\n",
      "L1 loss: 0.07937867939472198\n",
      "Training batch 77 with loss 0.27521\n",
      "------------------------------------------\n",
      "L1 loss: 0.08411002904176712\n",
      "Training batch 78 with loss 0.31031\n",
      "------------------------------------------\n",
      "L1 loss: 0.10611474514007568\n",
      "Training batch 79 with loss 0.38018\n",
      "------------------------------------------\n",
      "L1 loss: 0.07620853930711746\n",
      "Training batch 80 with loss 0.28573\n",
      "------------------------------------------\n",
      "L1 loss: 0.0766163021326065\n",
      "Training batch 81 with loss 0.31924\n",
      "------------------------------------------\n",
      "L1 loss: 0.08452963829040527\n",
      "Training batch 82 with loss 0.34608\n",
      "------------------------------------------\n",
      "L1 loss: 0.07552162557840347\n",
      "Training batch 83 with loss 0.27529\n",
      "------------------------------------------\n",
      "L1 loss: 0.06691236793994904\n",
      "Training batch 84 with loss 0.30452\n",
      "------------------------------------------\n",
      "L1 loss: 0.06710246950387955\n",
      "Training batch 85 with loss 0.28478\n",
      "------------------------------------------\n",
      "L1 loss: 0.08313889056444168\n",
      "Training batch 86 with loss 0.29759\n",
      "------------------------------------------\n",
      "L1 loss: 0.10134176164865494\n",
      "Training batch 87 with loss 0.39740\n",
      "------------------------------------------\n",
      "L1 loss: 0.0770445242524147\n",
      "Training batch 88 with loss 0.30955\n",
      "------------------------------------------\n",
      "L1 loss: 0.06910634785890579\n",
      "Training batch 89 with loss 0.28113\n",
      "------------------------------------------\n",
      "L1 loss: 0.07461431622505188\n",
      "Training batch 90 with loss 0.27406\n",
      "------------------------------------------\n",
      "L1 loss: 0.06509796530008316\n",
      "Training batch 91 with loss 0.26987\n",
      "------------------------------------------\n",
      "L1 loss: 0.09071539342403412\n",
      "Training batch 92 with loss 0.30601\n",
      "------------------------------------------\n",
      "L1 loss: 0.07091007381677628\n",
      "Training batch 93 with loss 0.27751\n",
      "------------------------------------------\n",
      "L1 loss: 0.08561702817678452\n",
      "Training batch 94 with loss 0.30286\n",
      "------------------------------------------\n",
      "L1 loss: 0.070726677775383\n",
      "Training batch 95 with loss 0.25405\n",
      "------------------------------------------\n",
      "L1 loss: 0.05826643109321594\n",
      "Training batch 96 with loss 0.27559\n",
      "------------------------------------------\n",
      "L1 loss: 0.0881657749414444\n",
      "Training batch 97 with loss 0.37746\n",
      "------------------------------------------\n",
      "L1 loss: 0.05839676409959793\n",
      "Training batch 98 with loss 0.24300\n",
      "------------------------------------------\n",
      "L1 loss: 0.06359376013278961\n",
      "Training batch 99 with loss 0.28065\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29950917363166807\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4818\n",
      "------------------------------------------\n",
      "L1 loss: 0.06322059780359268\n",
      "Training batch 0 with loss 0.26093\n",
      "------------------------------------------\n",
      "L1 loss: 0.06793733686208725\n",
      "Training batch 1 with loss 0.28035\n",
      "------------------------------------------\n",
      "L1 loss: 0.08886397629976273\n",
      "Training batch 2 with loss 0.32424\n",
      "------------------------------------------\n",
      "L1 loss: 0.06834098696708679\n",
      "Training batch 3 with loss 0.26522\n",
      "------------------------------------------\n",
      "L1 loss: 0.08470812439918518\n",
      "Training batch 4 with loss 0.32755\n",
      "------------------------------------------\n",
      "L1 loss: 0.07000961899757385\n",
      "Training batch 5 with loss 0.34811\n",
      "------------------------------------------\n",
      "L1 loss: 0.05209549143910408\n",
      "Training batch 6 with loss 0.32197\n",
      "------------------------------------------\n",
      "L1 loss: 0.08458147943019867\n",
      "Training batch 7 with loss 0.33160\n",
      "------------------------------------------\n",
      "L1 loss: 0.08289909362792969\n",
      "Training batch 8 with loss 0.35042\n",
      "------------------------------------------\n",
      "L1 loss: 0.06250764429569244\n",
      "Training batch 9 with loss 0.25351\n",
      "------------------------------------------\n",
      "L1 loss: 0.0844106674194336\n",
      "Training batch 10 with loss 0.29058\n",
      "------------------------------------------\n",
      "L1 loss: 0.05325015261769295\n",
      "Training batch 11 with loss 0.60155\n",
      "------------------------------------------\n",
      "L1 loss: 0.07773243635892868\n",
      "Training batch 12 with loss 0.33425\n",
      "------------------------------------------\n",
      "L1 loss: 0.06802519410848618\n",
      "Training batch 13 with loss 0.24149\n",
      "------------------------------------------\n",
      "L1 loss: 0.05176623538136482\n",
      "Training batch 14 with loss 0.25135\n",
      "------------------------------------------\n",
      "L1 loss: 0.08295434713363647\n",
      "Training batch 15 with loss 0.26593\n",
      "------------------------------------------\n",
      "L1 loss: 0.055095426738262177\n",
      "Training batch 16 with loss 0.35994\n",
      "------------------------------------------\n",
      "L1 loss: 0.08991380780935287\n",
      "Training batch 17 with loss 0.38529\n",
      "------------------------------------------\n",
      "L1 loss: 0.07898657768964767\n",
      "Training batch 18 with loss 0.28707\n",
      "------------------------------------------\n",
      "L1 loss: 0.06084121763706207\n",
      "Training batch 19 with loss 0.24315\n",
      "------------------------------------------\n",
      "L1 loss: 0.05645492300391197\n",
      "Training batch 20 with loss 0.55218\n",
      "------------------------------------------\n",
      "L1 loss: 0.07127303630113602\n",
      "Training batch 21 with loss 0.27579\n",
      "------------------------------------------\n",
      "L1 loss: 0.06753035634756088\n",
      "Training batch 22 with loss 0.29547\n",
      "------------------------------------------\n",
      "L1 loss: 0.08791708201169968\n",
      "Training batch 23 with loss 0.34917\n",
      "------------------------------------------\n",
      "L1 loss: 0.06982918083667755\n",
      "Training batch 24 with loss 0.31157\n",
      "------------------------------------------\n",
      "L1 loss: 0.07464702427387238\n",
      "Training batch 25 with loss 0.33029\n",
      "------------------------------------------\n",
      "L1 loss: 0.07516153156757355\n",
      "Training batch 26 with loss 0.26702\n",
      "------------------------------------------\n",
      "L1 loss: 0.055472031235694885\n",
      "Training batch 27 with loss 0.27457\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.057446081191301346\n",
      "Training batch 28 with loss 0.29185\n",
      "------------------------------------------\n",
      "L1 loss: 0.05706782639026642\n",
      "Training batch 29 with loss 0.25430\n",
      "------------------------------------------\n",
      "L1 loss: 0.061797648668289185\n",
      "Training batch 30 with loss 0.27956\n",
      "------------------------------------------\n",
      "L1 loss: 0.06554973125457764\n",
      "Training batch 31 with loss 0.31316\n",
      "------------------------------------------\n",
      "L1 loss: 0.07107742875814438\n",
      "Training batch 32 with loss 0.29734\n",
      "------------------------------------------\n",
      "L1 loss: 0.06320535391569138\n",
      "Training batch 33 with loss 0.25380\n",
      "------------------------------------------\n",
      "L1 loss: 0.07554041594266891\n",
      "Training batch 34 with loss 0.31458\n",
      "------------------------------------------\n",
      "L1 loss: 0.09578968584537506\n",
      "Training batch 35 with loss 0.33207\n",
      "------------------------------------------\n",
      "L1 loss: 0.08429049700498581\n",
      "Training batch 36 with loss 0.27531\n",
      "------------------------------------------\n",
      "L1 loss: 0.0682966560125351\n",
      "Training batch 37 with loss 0.25156\n",
      "------------------------------------------\n",
      "L1 loss: 0.06181761249899864\n",
      "Training batch 38 with loss 0.21956\n",
      "------------------------------------------\n",
      "L1 loss: 0.06792058795690536\n",
      "Training batch 39 with loss 0.32431\n",
      "------------------------------------------\n",
      "L1 loss: 0.0712459608912468\n",
      "Training batch 40 with loss 0.26711\n",
      "------------------------------------------\n",
      "L1 loss: 0.05791521444916725\n",
      "Training batch 41 with loss 0.30207\n",
      "------------------------------------------\n",
      "L1 loss: 0.07874664664268494\n",
      "Training batch 42 with loss 0.28630\n",
      "------------------------------------------\n",
      "L1 loss: 0.0719718337059021\n",
      "Training batch 43 with loss 0.29750\n",
      "------------------------------------------\n",
      "L1 loss: 0.054054394364356995\n",
      "Training batch 44 with loss 0.26984\n",
      "------------------------------------------\n",
      "L1 loss: 0.09211863577365875\n",
      "Training batch 45 with loss 0.37648\n",
      "------------------------------------------\n",
      "L1 loss: 0.07270864397287369\n",
      "Training batch 46 with loss 0.26698\n",
      "------------------------------------------\n",
      "L1 loss: 0.11251183599233627\n",
      "Training batch 47 with loss 0.38707\n",
      "------------------------------------------\n",
      "L1 loss: 0.04953660070896149\n",
      "Training batch 48 with loss 0.23939\n",
      "------------------------------------------\n",
      "L1 loss: 0.08290275186300278\n",
      "Training batch 49 with loss 0.34870\n",
      "------------------------------------------\n",
      "L1 loss: 0.0731211006641388\n",
      "Training batch 50 with loss 0.30114\n",
      "------------------------------------------\n",
      "L1 loss: 0.08371491730213165\n",
      "Training batch 51 with loss 0.35947\n",
      "------------------------------------------\n",
      "L1 loss: 0.08992291986942291\n",
      "Training batch 52 with loss 0.31317\n",
      "------------------------------------------\n",
      "L1 loss: 0.0787043422460556\n",
      "Training batch 53 with loss 0.28871\n",
      "------------------------------------------\n",
      "L1 loss: 0.07545866072177887\n",
      "Training batch 54 with loss 0.27163\n",
      "------------------------------------------\n",
      "L1 loss: 0.06312058120965958\n",
      "Training batch 55 with loss 0.30925\n",
      "------------------------------------------\n",
      "L1 loss: 0.057732418179512024\n",
      "Training batch 56 with loss 0.25683\n",
      "------------------------------------------\n",
      "L1 loss: 0.06497949361801147\n",
      "Training batch 57 with loss 0.27217\n",
      "------------------------------------------\n",
      "L1 loss: 0.06374325603246689\n",
      "Training batch 58 with loss 0.25663\n",
      "------------------------------------------\n",
      "L1 loss: 0.08104309439659119\n",
      "Training batch 59 with loss 0.28266\n",
      "------------------------------------------\n",
      "L1 loss: 0.06385120004415512\n",
      "Training batch 60 with loss 0.27194\n",
      "------------------------------------------\n",
      "L1 loss: 0.09857997298240662\n",
      "Training batch 61 with loss 0.37577\n",
      "------------------------------------------\n",
      "L1 loss: 0.05653483793139458\n",
      "Training batch 62 with loss 0.25956\n",
      "------------------------------------------\n",
      "L1 loss: 0.06438819319009781\n",
      "Training batch 63 with loss 0.25668\n",
      "------------------------------------------\n",
      "L1 loss: 0.0714305192232132\n",
      "Training batch 64 with loss 0.28809\n",
      "------------------------------------------\n",
      "L1 loss: 0.06135115027427673\n",
      "Training batch 65 with loss 0.21450\n",
      "------------------------------------------\n",
      "L1 loss: 0.0665961354970932\n",
      "Training batch 66 with loss 0.28510\n",
      "------------------------------------------\n",
      "L1 loss: 0.06938491761684418\n",
      "Training batch 67 with loss 0.43110\n",
      "------------------------------------------\n",
      "L1 loss: 0.07627467811107635\n",
      "Training batch 68 with loss 0.31618\n",
      "------------------------------------------\n",
      "L1 loss: 0.049799900501966476\n",
      "Training batch 69 with loss 0.25203\n",
      "------------------------------------------\n",
      "L1 loss: 0.0781099870800972\n",
      "Training batch 70 with loss 0.29672\n",
      "------------------------------------------\n",
      "L1 loss: 0.07282224297523499\n",
      "Training batch 71 with loss 0.32619\n",
      "------------------------------------------\n",
      "L1 loss: 0.08613213896751404\n",
      "Training batch 72 with loss 0.31394\n",
      "------------------------------------------\n",
      "L1 loss: 0.07320031523704529\n",
      "Training batch 73 with loss 0.29815\n",
      "------------------------------------------\n",
      "L1 loss: 0.09811848402023315\n",
      "Training batch 74 with loss 0.31958\n",
      "------------------------------------------\n",
      "L1 loss: 0.0739644467830658\n",
      "Training batch 75 with loss 0.28243\n",
      "------------------------------------------\n",
      "L1 loss: 0.08654169738292694\n",
      "Training batch 76 with loss 0.29616\n",
      "------------------------------------------\n",
      "L1 loss: 0.07679670304059982\n",
      "Training batch 77 with loss 0.26693\n",
      "------------------------------------------\n",
      "L1 loss: 0.07757166028022766\n",
      "Training batch 78 with loss 0.29588\n",
      "------------------------------------------\n",
      "L1 loss: 0.10350215435028076\n",
      "Training batch 79 with loss 0.36853\n",
      "------------------------------------------\n",
      "L1 loss: 0.07649195939302444\n",
      "Training batch 80 with loss 0.28547\n",
      "------------------------------------------\n",
      "L1 loss: 0.07046197354793549\n",
      "Training batch 81 with loss 0.27506\n",
      "------------------------------------------\n",
      "L1 loss: 0.08424849808216095\n",
      "Training batch 82 with loss 0.38362\n",
      "------------------------------------------\n",
      "L1 loss: 0.07601826637983322\n",
      "Training batch 83 with loss 0.27983\n",
      "------------------------------------------\n",
      "L1 loss: 0.06572332233190536\n",
      "Training batch 84 with loss 0.35415\n",
      "------------------------------------------\n",
      "L1 loss: 0.06333480775356293\n",
      "Training batch 85 with loss 0.27156\n",
      "------------------------------------------\n",
      "L1 loss: 0.08251452445983887\n",
      "Training batch 86 with loss 0.30259\n",
      "------------------------------------------\n",
      "L1 loss: 0.09925675392150879\n",
      "Training batch 87 with loss 0.41433\n",
      "------------------------------------------\n",
      "L1 loss: 0.08052866905927658\n",
      "Training batch 88 with loss 0.32742\n",
      "------------------------------------------\n",
      "L1 loss: 0.06949432939291\n",
      "Training batch 89 with loss 0.26620\n",
      "------------------------------------------\n",
      "L1 loss: 0.07167575508356094\n",
      "Training batch 90 with loss 0.27887\n",
      "------------------------------------------\n",
      "L1 loss: 0.06515411287546158\n",
      "Training batch 91 with loss 0.27258\n",
      "------------------------------------------\n",
      "L1 loss: 0.08938781172037125\n",
      "Training batch 92 with loss 0.28285\n",
      "------------------------------------------\n",
      "L1 loss: 0.0699271559715271\n",
      "Training batch 93 with loss 0.27346\n",
      "------------------------------------------\n",
      "L1 loss: 0.08639835566282272\n",
      "Training batch 94 with loss 0.30683\n",
      "------------------------------------------\n",
      "L1 loss: 0.07410954684019089\n",
      "Training batch 95 with loss 0.27010\n",
      "------------------------------------------\n",
      "L1 loss: 0.06329268217086792\n",
      "Training batch 96 with loss 0.26571\n",
      "------------------------------------------\n",
      "L1 loss: 0.08779983222484589\n",
      "Training batch 97 with loss 0.36473\n",
      "------------------------------------------\n",
      "L1 loss: 0.05545606464147568\n",
      "Training batch 98 with loss 0.22944\n",
      "------------------------------------------\n",
      "L1 loss: 0.06477875262498856\n",
      "Training batch 99 with loss 0.26771\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.30329025894403455\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4819\n",
      "------------------------------------------\n",
      "L1 loss: 0.06138433516025543\n",
      "Training batch 0 with loss 0.23943\n",
      "------------------------------------------\n",
      "L1 loss: 0.08504632115364075\n",
      "Training batch 1 with loss 0.28896\n",
      "------------------------------------------\n",
      "L1 loss: 0.09036289900541306\n",
      "Training batch 2 with loss 0.32931\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07806490361690521\n",
      "Training batch 3 with loss 0.26365\n",
      "------------------------------------------\n",
      "L1 loss: 0.08164917677640915\n",
      "Training batch 4 with loss 0.32652\n",
      "------------------------------------------\n",
      "L1 loss: 0.0673389881849289\n",
      "Training batch 5 with loss 0.32089\n",
      "------------------------------------------\n",
      "L1 loss: 0.07912477850914001\n",
      "Training batch 6 with loss 0.25708\n",
      "------------------------------------------\n",
      "L1 loss: 0.08455424010753632\n",
      "Training batch 7 with loss 0.32916\n",
      "------------------------------------------\n",
      "L1 loss: 0.08484359830617905\n",
      "Training batch 8 with loss 0.34737\n",
      "------------------------------------------\n",
      "L1 loss: 0.06417147070169449\n",
      "Training batch 9 with loss 0.25273\n",
      "------------------------------------------\n",
      "L1 loss: 0.08286228030920029\n",
      "Training batch 10 with loss 0.29089\n",
      "------------------------------------------\n",
      "L1 loss: 0.06154162809252739\n",
      "Training batch 11 with loss 0.30191\n",
      "------------------------------------------\n",
      "L1 loss: 0.07374253123998642\n",
      "Training batch 12 with loss 0.31433\n",
      "------------------------------------------\n",
      "L1 loss: 0.06859955191612244\n",
      "Training batch 13 with loss 0.24143\n",
      "------------------------------------------\n",
      "L1 loss: 0.052472151815891266\n",
      "Training batch 14 with loss 0.24530\n",
      "------------------------------------------\n",
      "L1 loss: 0.08465251326560974\n",
      "Training batch 15 with loss 0.27030\n",
      "------------------------------------------\n",
      "L1 loss: 0.06192198023200035\n",
      "Training batch 16 with loss 0.25694\n",
      "------------------------------------------\n",
      "L1 loss: 0.08665226399898529\n",
      "Training batch 17 with loss 0.33808\n",
      "------------------------------------------\n",
      "L1 loss: 0.07955273240804672\n",
      "Training batch 18 with loss 0.28769\n",
      "------------------------------------------\n",
      "L1 loss: 0.06369374692440033\n",
      "Training batch 19 with loss 0.33973\n",
      "------------------------------------------\n",
      "L1 loss: 0.07147678732872009\n",
      "Training batch 20 with loss 0.32033\n",
      "------------------------------------------\n",
      "L1 loss: 0.07006566226482391\n",
      "Training batch 21 with loss 0.27586\n",
      "------------------------------------------\n",
      "L1 loss: 0.06898531317710876\n",
      "Training batch 22 with loss 0.28972\n",
      "------------------------------------------\n",
      "L1 loss: 0.08735422044992447\n",
      "Training batch 23 with loss 0.37467\n",
      "------------------------------------------\n",
      "L1 loss: 0.06678544729948044\n",
      "Training batch 24 with loss 0.30176\n",
      "------------------------------------------\n",
      "L1 loss: 0.07366017252206802\n",
      "Training batch 25 with loss 0.28862\n",
      "------------------------------------------\n",
      "L1 loss: 0.07213898748159409\n",
      "Training batch 26 with loss 0.36705\n",
      "------------------------------------------\n",
      "L1 loss: 0.05550757795572281\n",
      "Training batch 27 with loss 0.29717\n",
      "------------------------------------------\n",
      "L1 loss: 0.05652531981468201\n",
      "Training batch 28 with loss 0.29537\n",
      "------------------------------------------\n",
      "L1 loss: 0.06106424331665039\n",
      "Training batch 29 with loss 0.24845\n",
      "------------------------------------------\n",
      "L1 loss: 0.06023511290550232\n",
      "Training batch 30 with loss 0.25964\n",
      "------------------------------------------\n",
      "L1 loss: 0.06398919969797134\n",
      "Training batch 31 with loss 0.30171\n",
      "------------------------------------------\n",
      "L1 loss: 0.06935689598321915\n",
      "Training batch 32 with loss 0.29092\n",
      "------------------------------------------\n",
      "L1 loss: 0.060703933238983154\n",
      "Training batch 33 with loss 0.25871\n",
      "------------------------------------------\n",
      "L1 loss: 0.07467179745435715\n",
      "Training batch 34 with loss 0.31039\n",
      "------------------------------------------\n",
      "L1 loss: 0.09183377027511597\n",
      "Training batch 35 with loss 0.32441\n",
      "------------------------------------------\n",
      "L1 loss: 0.08683518320322037\n",
      "Training batch 36 with loss 0.29485\n",
      "------------------------------------------\n",
      "L1 loss: 0.06612864136695862\n",
      "Training batch 37 with loss 0.24844\n",
      "------------------------------------------\n",
      "L1 loss: 0.06202884018421173\n",
      "Training batch 38 with loss 0.22743\n",
      "------------------------------------------\n",
      "L1 loss: 0.0640944242477417\n",
      "Training batch 39 with loss 0.29503\n",
      "------------------------------------------\n",
      "L1 loss: 0.0725550726056099\n",
      "Training batch 40 with loss 0.26512\n",
      "------------------------------------------\n",
      "L1 loss: 0.05657090246677399\n",
      "Training batch 41 with loss 0.28331\n",
      "------------------------------------------\n",
      "L1 loss: 0.07872273027896881\n",
      "Training batch 42 with loss 0.27639\n",
      "------------------------------------------\n",
      "L1 loss: 0.06819973886013031\n",
      "Training batch 43 with loss 0.27437\n",
      "------------------------------------------\n",
      "L1 loss: 0.05760221928358078\n",
      "Training batch 44 with loss 0.26863\n",
      "------------------------------------------\n",
      "L1 loss: 0.09193190187215805\n",
      "Training batch 45 with loss 0.36869\n",
      "------------------------------------------\n",
      "L1 loss: 0.07124102860689163\n",
      "Training batch 46 with loss 0.27317\n",
      "------------------------------------------\n",
      "L1 loss: 0.11046970635652542\n",
      "Training batch 47 with loss 0.35197\n",
      "------------------------------------------\n",
      "L1 loss: 0.04498325660824776\n",
      "Training batch 48 with loss 0.23773\n",
      "------------------------------------------\n",
      "L1 loss: 0.08504096418619156\n",
      "Training batch 49 with loss 0.39036\n",
      "------------------------------------------\n",
      "L1 loss: 0.07229209691286087\n",
      "Training batch 50 with loss 0.27148\n",
      "------------------------------------------\n",
      "L1 loss: 0.08022311329841614\n",
      "Training batch 51 with loss 0.35067\n",
      "------------------------------------------\n",
      "L1 loss: 0.089728444814682\n",
      "Training batch 52 with loss 0.28938\n",
      "------------------------------------------\n",
      "L1 loss: 0.07756517827510834\n",
      "Training batch 53 with loss 0.28941\n",
      "------------------------------------------\n",
      "L1 loss: 0.07770786434412003\n",
      "Training batch 54 with loss 0.27317\n",
      "------------------------------------------\n",
      "L1 loss: 0.06201319396495819\n",
      "Training batch 55 with loss 0.30063\n",
      "------------------------------------------\n",
      "L1 loss: 0.05700868368148804\n",
      "Training batch 56 with loss 0.25396\n",
      "------------------------------------------\n",
      "L1 loss: 0.06469474732875824\n",
      "Training batch 57 with loss 0.26543\n",
      "------------------------------------------\n",
      "L1 loss: 0.06500460207462311\n",
      "Training batch 58 with loss 0.26295\n",
      "------------------------------------------\n",
      "L1 loss: 0.0806855782866478\n",
      "Training batch 59 with loss 0.28815\n",
      "------------------------------------------\n",
      "L1 loss: 0.06704023480415344\n",
      "Training batch 60 with loss 0.28673\n",
      "------------------------------------------\n",
      "L1 loss: 0.09928684681653976\n",
      "Training batch 61 with loss 0.37575\n",
      "------------------------------------------\n",
      "L1 loss: 0.05509636178612709\n",
      "Training batch 62 with loss 0.25853\n",
      "------------------------------------------\n",
      "L1 loss: 0.061323292553424835\n",
      "Training batch 63 with loss 0.26818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07344545423984528\n",
      "Training batch 64 with loss 0.27837\n",
      "------------------------------------------\n",
      "L1 loss: 0.06139819696545601\n",
      "Training batch 65 with loss 0.22785\n",
      "------------------------------------------\n",
      "L1 loss: 0.06082767993211746\n",
      "Training batch 66 with loss 0.26518\n",
      "------------------------------------------\n",
      "L1 loss: 0.06615623831748962\n",
      "Training batch 67 with loss 0.26197\n",
      "------------------------------------------\n",
      "L1 loss: 0.07170987874269485\n",
      "Training batch 68 with loss 0.31036\n",
      "------------------------------------------\n",
      "L1 loss: 0.050708919763565063\n",
      "Training batch 69 with loss 0.25781\n",
      "------------------------------------------\n",
      "L1 loss: 0.07733242958784103\n",
      "Training batch 70 with loss 0.30269\n",
      "------------------------------------------\n",
      "L1 loss: 0.07156331837177277\n",
      "Training batch 71 with loss 0.29526\n",
      "------------------------------------------\n",
      "L1 loss: 0.08300106972455978\n",
      "Training batch 72 with loss 0.32101\n",
      "------------------------------------------\n",
      "L1 loss: 0.06963059306144714\n",
      "Training batch 73 with loss 0.29570\n",
      "------------------------------------------\n",
      "L1 loss: 0.0928042009472847\n",
      "Training batch 74 with loss 0.32402\n",
      "------------------------------------------\n",
      "L1 loss: 0.07552531361579895\n",
      "Training batch 75 with loss 0.28629\n",
      "------------------------------------------\n",
      "L1 loss: 0.08666009455919266\n",
      "Training batch 76 with loss 0.29920\n",
      "------------------------------------------\n",
      "L1 loss: 0.08039329946041107\n",
      "Training batch 77 with loss 0.26509\n",
      "------------------------------------------\n",
      "L1 loss: 0.07885207235813141\n",
      "Training batch 78 with loss 0.29538\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.10440230369567871\n",
      "Training batch 79 with loss 0.37403\n",
      "------------------------------------------\n",
      "L1 loss: 0.07561124861240387\n",
      "Training batch 80 with loss 0.27348\n",
      "------------------------------------------\n",
      "L1 loss: 0.0812106505036354\n",
      "Training batch 81 with loss 0.29666\n",
      "------------------------------------------\n",
      "L1 loss: 0.08306916803121567\n",
      "Training batch 82 with loss 0.36737\n",
      "------------------------------------------\n",
      "L1 loss: 0.07856937497854233\n",
      "Training batch 83 with loss 0.27467\n",
      "------------------------------------------\n",
      "L1 loss: 0.07127867639064789\n",
      "Training batch 84 with loss 0.30060\n",
      "------------------------------------------\n",
      "L1 loss: 0.06246919557452202\n",
      "Training batch 85 with loss 0.26385\n",
      "------------------------------------------\n",
      "L1 loss: 0.07501962780952454\n",
      "Training batch 86 with loss 0.28973\n",
      "------------------------------------------\n",
      "L1 loss: 0.10074947029352188\n",
      "Training batch 87 with loss 0.39787\n",
      "------------------------------------------\n",
      "L1 loss: 0.0729643777012825\n",
      "Training batch 88 with loss 0.30752\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976936757564545\n",
      "Training batch 89 with loss 0.28179\n",
      "------------------------------------------\n",
      "L1 loss: 0.0729718953371048\n",
      "Training batch 90 with loss 0.28040\n",
      "------------------------------------------\n",
      "L1 loss: 0.06495552510023117\n",
      "Training batch 91 with loss 0.26937\n",
      "------------------------------------------\n",
      "L1 loss: 0.09252889454364777\n",
      "Training batch 92 with loss 0.30995\n",
      "------------------------------------------\n",
      "L1 loss: 0.06217066943645477\n",
      "Training batch 93 with loss 0.23432\n",
      "------------------------------------------\n",
      "L1 loss: 0.08583708852529526\n",
      "Training batch 94 with loss 0.30351\n",
      "------------------------------------------\n",
      "L1 loss: 0.07144688069820404\n",
      "Training batch 95 with loss 0.26080\n",
      "------------------------------------------\n",
      "L1 loss: 0.06364672631025314\n",
      "Training batch 96 with loss 0.29061\n",
      "------------------------------------------\n",
      "L1 loss: 0.0827166885137558\n",
      "Training batch 97 with loss 0.36122\n",
      "------------------------------------------\n",
      "L1 loss: 0.05825172737240791\n",
      "Training batch 98 with loss 0.23170\n",
      "------------------------------------------\n",
      "L1 loss: 0.06459084153175354\n",
      "Training batch 99 with loss 0.27380\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29265827402472494\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4820\n",
      "------------------------------------------\n",
      "L1 loss: 0.06289975345134735\n",
      "Training batch 0 with loss 0.24656\n",
      "------------------------------------------\n",
      "L1 loss: 0.08286333829164505\n",
      "Training batch 1 with loss 0.29243\n",
      "------------------------------------------\n",
      "L1 loss: 0.09457393735647202\n",
      "Training batch 2 with loss 0.33063\n",
      "------------------------------------------\n",
      "L1 loss: 0.07840557396411896\n",
      "Training batch 3 with loss 0.26703\n",
      "------------------------------------------\n",
      "L1 loss: 0.0839444026350975\n",
      "Training batch 4 with loss 0.33268\n",
      "------------------------------------------\n",
      "L1 loss: 0.0656198263168335\n",
      "Training batch 5 with loss 0.32730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07801805436611176\n",
      "Training batch 6 with loss 0.26356\n",
      "------------------------------------------\n",
      "L1 loss: 0.08473452925682068\n",
      "Training batch 7 with loss 0.33109\n",
      "------------------------------------------\n",
      "L1 loss: 0.08162710815668106\n",
      "Training batch 8 with loss 0.34644\n",
      "------------------------------------------\n",
      "L1 loss: 0.06405515968799591\n",
      "Training batch 9 with loss 0.24436\n",
      "------------------------------------------\n",
      "L1 loss: 0.08120246231555939\n",
      "Training batch 10 with loss 0.30882\n",
      "------------------------------------------\n",
      "L1 loss: 0.0613083653151989\n",
      "Training batch 11 with loss 0.29002\n",
      "------------------------------------------\n",
      "L1 loss: 0.07419867068529129\n",
      "Training batch 12 with loss 0.31101\n",
      "------------------------------------------\n",
      "L1 loss: 0.06921134889125824\n",
      "Training batch 13 with loss 0.25609\n",
      "------------------------------------------\n",
      "L1 loss: 0.0543026328086853\n",
      "Training batch 14 with loss 0.26864\n",
      "------------------------------------------\n",
      "L1 loss: 0.08319699019193649\n",
      "Training batch 15 with loss 0.26460\n",
      "------------------------------------------\n",
      "L1 loss: 0.06304595619440079\n",
      "Training batch 16 with loss 0.27745\n",
      "------------------------------------------\n",
      "L1 loss: 0.08390403538942337\n",
      "Training batch 17 with loss 0.33356\n",
      "------------------------------------------\n",
      "L1 loss: 0.08082347363233566\n",
      "Training batch 18 with loss 0.28857\n",
      "------------------------------------------\n",
      "L1 loss: 0.06247049942612648\n",
      "Training batch 19 with loss 0.24611\n",
      "------------------------------------------\n",
      "L1 loss: 0.07266031205654144\n",
      "Training batch 20 with loss 0.31908\n",
      "------------------------------------------\n",
      "L1 loss: 0.07185740023851395\n",
      "Training batch 21 with loss 0.28018\n",
      "------------------------------------------\n",
      "L1 loss: 0.06988447904586792\n",
      "Training batch 22 with loss 0.28386\n",
      "------------------------------------------\n",
      "L1 loss: 0.06825601309537888\n",
      "Training batch 23 with loss 0.52678\n",
      "------------------------------------------\n",
      "L1 loss: 0.06684017181396484\n",
      "Training batch 24 with loss 0.29697\n",
      "------------------------------------------\n",
      "L1 loss: 0.0738835334777832\n",
      "Training batch 25 with loss 0.29194\n",
      "------------------------------------------\n",
      "L1 loss: 0.0735747441649437\n",
      "Training batch 26 with loss 0.26272\n",
      "------------------------------------------\n",
      "L1 loss: 0.05535239726305008\n",
      "Training batch 27 with loss 0.29230\n",
      "------------------------------------------\n",
      "L1 loss: 0.057554662227630615\n",
      "Training batch 28 with loss 0.28606\n",
      "------------------------------------------\n",
      "L1 loss: 0.06148887053132057\n",
      "Training batch 29 with loss 0.25222\n",
      "------------------------------------------\n",
      "L1 loss: 0.05734147131443024\n",
      "Training batch 30 with loss 0.26201\n",
      "------------------------------------------\n",
      "L1 loss: 0.06409679353237152\n",
      "Training batch 31 with loss 0.31032\n",
      "------------------------------------------\n",
      "L1 loss: 0.07046148926019669\n",
      "Training batch 32 with loss 0.27859\n",
      "------------------------------------------\n",
      "L1 loss: 0.06224014237523079\n",
      "Training batch 33 with loss 0.25498\n",
      "------------------------------------------\n",
      "L1 loss: 0.07462974637746811\n",
      "Training batch 34 with loss 0.30421\n",
      "------------------------------------------\n",
      "L1 loss: 0.09489941596984863\n",
      "Training batch 35 with loss 0.32611\n",
      "------------------------------------------\n",
      "L1 loss: 0.08546848595142365\n",
      "Training batch 36 with loss 0.28393\n",
      "------------------------------------------\n",
      "L1 loss: 0.06559165567159653\n",
      "Training batch 37 with loss 0.26057\n",
      "------------------------------------------\n",
      "L1 loss: 0.06363124400377274\n",
      "Training batch 38 with loss 0.22473\n",
      "------------------------------------------\n",
      "L1 loss: 0.06870031356811523\n",
      "Training batch 39 with loss 0.31146\n",
      "------------------------------------------\n",
      "L1 loss: 0.06553875654935837\n",
      "Training batch 40 with loss 0.25128\n",
      "------------------------------------------\n",
      "L1 loss: 0.06162547692656517\n",
      "Training batch 41 with loss 0.28950\n",
      "------------------------------------------\n",
      "L1 loss: 0.07586506754159927\n",
      "Training batch 42 with loss 0.28079\n",
      "------------------------------------------\n",
      "L1 loss: 0.0695696547627449\n",
      "Training batch 43 with loss 0.28306\n",
      "------------------------------------------\n",
      "L1 loss: 0.05389685928821564\n",
      "Training batch 44 with loss 0.27191\n",
      "------------------------------------------\n",
      "L1 loss: 0.09137237817049026\n",
      "Training batch 45 with loss 0.37867\n",
      "------------------------------------------\n",
      "L1 loss: 0.06756086647510529\n",
      "Training batch 46 with loss 0.27204\n",
      "------------------------------------------\n",
      "L1 loss: 0.10810458660125732\n",
      "Training batch 47 with loss 0.35337\n",
      "------------------------------------------\n",
      "L1 loss: 0.044307269155979156\n",
      "Training batch 48 with loss 0.23769\n",
      "------------------------------------------\n",
      "L1 loss: 0.07920651882886887\n",
      "Training batch 49 with loss 0.35340\n",
      "------------------------------------------\n",
      "L1 loss: 0.07294382154941559\n",
      "Training batch 50 with loss 0.28046\n",
      "------------------------------------------\n",
      "L1 loss: 0.07631530612707138\n",
      "Training batch 51 with loss 0.33727\n",
      "------------------------------------------\n",
      "L1 loss: 0.09004638344049454\n",
      "Training batch 52 with loss 0.28807\n",
      "------------------------------------------\n",
      "L1 loss: 0.08010931313037872\n",
      "Training batch 53 with loss 0.28752\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0708407461643219\n",
      "Training batch 54 with loss 0.27674\n",
      "------------------------------------------\n",
      "L1 loss: 0.06407546997070312\n",
      "Training batch 55 with loss 0.31645\n",
      "------------------------------------------\n",
      "L1 loss: 0.05495648458600044\n",
      "Training batch 56 with loss 0.26118\n",
      "------------------------------------------\n",
      "L1 loss: 0.06905675679445267\n",
      "Training batch 57 with loss 0.27257\n",
      "------------------------------------------\n",
      "L1 loss: 0.06715188920497894\n",
      "Training batch 58 with loss 0.26044\n",
      "------------------------------------------\n",
      "L1 loss: 0.08118011057376862\n",
      "Training batch 59 with loss 0.29097\n",
      "------------------------------------------\n",
      "L1 loss: 0.06718061119318008\n",
      "Training batch 60 with loss 0.28121\n",
      "------------------------------------------\n",
      "L1 loss: 0.09657644480466843\n",
      "Training batch 61 with loss 0.34581\n",
      "------------------------------------------\n",
      "L1 loss: 0.05548212304711342\n",
      "Training batch 62 with loss 0.26540\n",
      "------------------------------------------\n",
      "L1 loss: 0.06308017671108246\n",
      "Training batch 63 with loss 0.25903\n",
      "------------------------------------------\n",
      "L1 loss: 0.07190819829702377\n",
      "Training batch 64 with loss 0.28299\n",
      "------------------------------------------\n",
      "L1 loss: 0.0633154883980751\n",
      "Training batch 65 with loss 0.22580\n",
      "------------------------------------------\n",
      "L1 loss: 0.06588289886713028\n",
      "Training batch 66 with loss 0.27393\n",
      "------------------------------------------\n",
      "L1 loss: 0.06470648944377899\n",
      "Training batch 67 with loss 0.27111\n",
      "------------------------------------------\n",
      "L1 loss: 0.07612480968236923\n",
      "Training batch 68 with loss 0.31366\n",
      "------------------------------------------\n",
      "L1 loss: 0.0503215491771698\n",
      "Training batch 69 with loss 0.26335\n",
      "------------------------------------------\n",
      "L1 loss: 0.0799313634634018\n",
      "Training batch 70 with loss 0.31640\n",
      "------------------------------------------\n",
      "L1 loss: 0.07079280912876129\n",
      "Training batch 71 with loss 0.29984\n",
      "------------------------------------------\n",
      "L1 loss: 0.08547375351190567\n",
      "Training batch 72 with loss 0.30533\n",
      "------------------------------------------\n",
      "L1 loss: 0.07237215340137482\n",
      "Training batch 73 with loss 0.29753\n",
      "------------------------------------------\n",
      "L1 loss: 0.09455570578575134\n",
      "Training batch 74 with loss 0.32469\n",
      "------------------------------------------\n",
      "L1 loss: 0.07337349653244019\n",
      "Training batch 75 with loss 0.28317\n",
      "------------------------------------------\n",
      "L1 loss: 0.085423544049263\n",
      "Training batch 76 with loss 0.29555\n",
      "------------------------------------------\n",
      "L1 loss: 0.07929324358701706\n",
      "Training batch 77 with loss 0.27050\n",
      "------------------------------------------\n",
      "L1 loss: 0.08405784517526627\n",
      "Training batch 78 with loss 0.31933\n",
      "------------------------------------------\n",
      "L1 loss: 0.10444902628660202\n",
      "Training batch 79 with loss 0.35291\n",
      "------------------------------------------\n",
      "L1 loss: 0.0744049996137619\n",
      "Training batch 80 with loss 0.27668\n",
      "------------------------------------------\n",
      "L1 loss: 0.0717906579375267\n",
      "Training batch 81 with loss 0.27455\n",
      "------------------------------------------\n",
      "L1 loss: 0.08678033947944641\n",
      "Training batch 82 with loss 0.37311\n",
      "------------------------------------------\n",
      "L1 loss: 0.07402347773313522\n",
      "Training batch 83 with loss 0.28944\n",
      "------------------------------------------\n",
      "L1 loss: 0.070918507874012\n",
      "Training batch 84 with loss 0.31875\n",
      "------------------------------------------\n",
      "L1 loss: 0.06304135918617249\n",
      "Training batch 85 with loss 0.27811\n",
      "------------------------------------------\n",
      "L1 loss: 0.07727960497140884\n",
      "Training batch 86 with loss 0.28728\n",
      "------------------------------------------\n",
      "L1 loss: 0.10132169723510742\n",
      "Training batch 87 with loss 0.40348\n",
      "------------------------------------------\n",
      "L1 loss: 0.07686751335859299\n",
      "Training batch 88 with loss 0.30604\n",
      "------------------------------------------\n",
      "L1 loss: 0.06798051297664642\n",
      "Training batch 89 with loss 0.26155\n",
      "------------------------------------------\n",
      "L1 loss: 0.07380653917789459\n",
      "Training batch 90 with loss 0.28488\n",
      "------------------------------------------\n",
      "L1 loss: 0.06062246114015579\n",
      "Training batch 91 with loss 0.24996\n",
      "------------------------------------------\n",
      "L1 loss: 0.09487602859735489\n",
      "Training batch 92 with loss 0.33352\n",
      "------------------------------------------\n",
      "L1 loss: 0.06170838326215744\n",
      "Training batch 93 with loss 0.24889\n",
      "------------------------------------------\n",
      "L1 loss: 0.08650894463062286\n",
      "Training batch 94 with loss 0.31101\n",
      "------------------------------------------\n",
      "L1 loss: 0.0704820305109024\n",
      "Training batch 95 with loss 0.26621\n",
      "------------------------------------------\n",
      "L1 loss: 0.0588308610022068\n",
      "Training batch 96 with loss 0.26093\n",
      "------------------------------------------\n",
      "L1 loss: 0.08927550166845322\n",
      "Training batch 97 with loss 0.36610\n",
      "------------------------------------------\n",
      "L1 loss: 0.055979568511247635\n",
      "Training batch 98 with loss 0.23495\n",
      "------------------------------------------\n",
      "L1 loss: 0.055254314094781876\n",
      "Training batch 99 with loss 0.70587\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29758191883563995\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4821\n",
      "------------------------------------------\n",
      "L1 loss: 0.06297336518764496\n",
      "Training batch 0 with loss 0.25605\n",
      "------------------------------------------\n",
      "L1 loss: 0.08294359594583511\n",
      "Training batch 1 with loss 0.29384\n",
      "------------------------------------------\n",
      "L1 loss: 0.09419450163841248\n",
      "Training batch 2 with loss 0.32286\n",
      "------------------------------------------\n",
      "L1 loss: 0.0771506130695343\n",
      "Training batch 3 with loss 0.27344\n",
      "------------------------------------------\n",
      "L1 loss: 0.08283767104148865\n",
      "Training batch 4 with loss 0.31953\n",
      "------------------------------------------\n",
      "L1 loss: 0.06562969088554382\n",
      "Training batch 5 with loss 0.32472\n",
      "------------------------------------------\n",
      "L1 loss: 0.08126221597194672\n",
      "Training batch 6 with loss 0.26848\n",
      "------------------------------------------\n",
      "L1 loss: 0.08284731954336166\n",
      "Training batch 7 with loss 0.30696\n",
      "------------------------------------------\n",
      "L1 loss: 0.07827777415513992\n",
      "Training batch 8 with loss 0.31753\n",
      "------------------------------------------\n",
      "L1 loss: 0.05800899863243103\n",
      "Training batch 9 with loss 0.25376\n",
      "------------------------------------------\n",
      "L1 loss: 0.08112135529518127\n",
      "Training batch 10 with loss 0.29957\n",
      "------------------------------------------\n",
      "L1 loss: 0.06314181536436081\n",
      "Training batch 11 with loss 0.31153\n",
      "------------------------------------------\n",
      "L1 loss: 0.07500113546848297\n",
      "Training batch 12 with loss 0.30717\n",
      "------------------------------------------\n",
      "L1 loss: 0.06916428357362747\n",
      "Training batch 13 with loss 0.26650\n",
      "------------------------------------------\n",
      "L1 loss: 0.05328958481550217\n",
      "Training batch 14 with loss 0.25115\n",
      "------------------------------------------\n",
      "L1 loss: 0.08217541128396988\n",
      "Training batch 15 with loss 0.27930\n",
      "------------------------------------------\n",
      "L1 loss: 0.06362388283014297\n",
      "Training batch 16 with loss 0.27647\n",
      "------------------------------------------\n",
      "L1 loss: 0.08466373383998871\n",
      "Training batch 17 with loss 0.33887\n",
      "------------------------------------------\n",
      "L1 loss: 0.08065207302570343\n",
      "Training batch 18 with loss 0.29747\n",
      "------------------------------------------\n",
      "L1 loss: 0.06268338114023209\n",
      "Training batch 19 with loss 0.24764\n",
      "------------------------------------------\n",
      "L1 loss: 0.0735093206167221\n",
      "Training batch 20 with loss 0.30504\n",
      "------------------------------------------\n",
      "L1 loss: 0.07073114812374115\n",
      "Training batch 21 with loss 0.29020\n",
      "------------------------------------------\n",
      "L1 loss: 0.06849859654903412\n",
      "Training batch 22 with loss 0.27841\n",
      "------------------------------------------\n",
      "L1 loss: 0.08963078260421753\n",
      "Training batch 23 with loss 0.36313\n",
      "------------------------------------------\n",
      "L1 loss: 0.06746293604373932\n",
      "Training batch 24 with loss 0.30520\n",
      "------------------------------------------\n",
      "L1 loss: 0.06154307350516319\n",
      "Training batch 25 with loss 0.45634\n",
      "------------------------------------------\n",
      "L1 loss: 0.07966022193431854\n",
      "Training batch 26 with loss 0.29068\n",
      "------------------------------------------\n",
      "L1 loss: 0.05543546378612518\n",
      "Training batch 27 with loss 0.29601\n",
      "------------------------------------------\n",
      "L1 loss: 0.05790422484278679\n",
      "Training batch 28 with loss 0.28040\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.05955665186047554\n",
      "Training batch 29 with loss 0.26285\n",
      "------------------------------------------\n",
      "L1 loss: 0.061318475753068924\n",
      "Training batch 30 with loss 0.27876\n",
      "------------------------------------------\n",
      "L1 loss: 0.06570948660373688\n",
      "Training batch 31 with loss 0.29715\n",
      "------------------------------------------\n",
      "L1 loss: 0.06954928487539291\n",
      "Training batch 32 with loss 0.26670\n",
      "------------------------------------------\n",
      "L1 loss: 0.06358393281698227\n",
      "Training batch 33 with loss 0.26246\n",
      "------------------------------------------\n",
      "L1 loss: 0.07599735260009766\n",
      "Training batch 34 with loss 0.31631\n",
      "------------------------------------------\n",
      "L1 loss: 0.09339684993028641\n",
      "Training batch 35 with loss 0.32456\n",
      "------------------------------------------\n",
      "L1 loss: 0.08163576573133469\n",
      "Training batch 36 with loss 0.28527\n",
      "------------------------------------------\n",
      "L1 loss: 0.06753804534673691\n",
      "Training batch 37 with loss 0.26246\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394954770803452\n",
      "Training batch 38 with loss 0.22627\n",
      "------------------------------------------\n",
      "L1 loss: 0.06798507273197174\n",
      "Training batch 39 with loss 0.30607\n",
      "------------------------------------------\n",
      "L1 loss: 0.06970811635255814\n",
      "Training batch 40 with loss 0.25606\n",
      "------------------------------------------\n",
      "L1 loss: 0.058418866246938705\n",
      "Training batch 41 with loss 0.30190\n",
      "------------------------------------------\n",
      "L1 loss: 0.07721732556819916\n",
      "Training batch 42 with loss 0.28188\n",
      "------------------------------------------\n",
      "L1 loss: 0.0706314966082573\n",
      "Training batch 43 with loss 0.28277\n",
      "------------------------------------------\n",
      "L1 loss: 0.05969621613621712\n",
      "Training batch 44 with loss 0.26564\n",
      "------------------------------------------\n",
      "L1 loss: 0.08422926068305969\n",
      "Training batch 45 with loss 0.49263\n",
      "------------------------------------------\n",
      "L1 loss: 0.07306049764156342\n",
      "Training batch 46 with loss 0.26987\n",
      "------------------------------------------\n",
      "L1 loss: 0.10850066691637039\n",
      "Training batch 47 with loss 0.36656\n",
      "------------------------------------------\n",
      "L1 loss: 0.04060946777462959\n",
      "Training batch 48 with loss 0.33322\n",
      "------------------------------------------\n",
      "L1 loss: 0.0796923115849495\n",
      "Training batch 49 with loss 0.35721\n",
      "------------------------------------------\n",
      "L1 loss: 0.07503292709589005\n",
      "Training batch 50 with loss 0.30548\n",
      "------------------------------------------\n",
      "L1 loss: 0.07705257087945938\n",
      "Training batch 51 with loss 0.34251\n",
      "------------------------------------------\n",
      "L1 loss: 0.09079979360103607\n",
      "Training batch 52 with loss 0.28732\n",
      "------------------------------------------\n",
      "L1 loss: 0.07735220342874527\n",
      "Training batch 53 with loss 0.27361\n",
      "------------------------------------------\n",
      "L1 loss: 0.07735446095466614\n",
      "Training batch 54 with loss 0.27665\n",
      "------------------------------------------\n",
      "L1 loss: 0.06258498877286911\n",
      "Training batch 55 with loss 0.31683\n",
      "------------------------------------------\n",
      "L1 loss: 0.056840814650058746\n",
      "Training batch 56 with loss 0.25603\n",
      "------------------------------------------\n",
      "L1 loss: 0.06658310443162918\n",
      "Training batch 57 with loss 0.27150\n",
      "------------------------------------------\n",
      "L1 loss: 0.06440840661525726\n",
      "Training batch 58 with loss 0.25942\n",
      "------------------------------------------\n",
      "L1 loss: 0.07971496880054474\n",
      "Training batch 59 with loss 0.28166\n",
      "------------------------------------------\n",
      "L1 loss: 0.06557203084230423\n",
      "Training batch 60 with loss 0.27508\n",
      "------------------------------------------\n",
      "L1 loss: 0.10221269726753235\n",
      "Training batch 61 with loss 0.36421\n",
      "------------------------------------------\n",
      "L1 loss: 0.056937169283628464\n",
      "Training batch 62 with loss 0.25494\n",
      "------------------------------------------\n",
      "L1 loss: 0.06380835175514221\n",
      "Training batch 63 with loss 0.25142\n",
      "------------------------------------------\n",
      "L1 loss: 0.07190874963998795\n",
      "Training batch 64 with loss 0.27031\n",
      "------------------------------------------\n",
      "L1 loss: 0.06172369047999382\n",
      "Training batch 65 with loss 0.22438\n",
      "------------------------------------------\n",
      "L1 loss: 0.06757822632789612\n",
      "Training batch 66 with loss 0.28617\n",
      "------------------------------------------\n",
      "L1 loss: 0.06475982815027237\n",
      "Training batch 67 with loss 0.26855\n",
      "------------------------------------------\n",
      "L1 loss: 0.07285142689943314\n",
      "Training batch 68 with loss 0.28990\n",
      "------------------------------------------\n",
      "L1 loss: 0.04898872226476669\n",
      "Training batch 69 with loss 0.26099\n",
      "------------------------------------------\n",
      "L1 loss: 0.0794762596487999\n",
      "Training batch 70 with loss 0.31583\n",
      "------------------------------------------\n",
      "L1 loss: 0.07158102095127106\n",
      "Training batch 71 with loss 0.33066\n",
      "------------------------------------------\n",
      "L1 loss: 0.08678872883319855\n",
      "Training batch 72 with loss 0.32876\n",
      "------------------------------------------\n",
      "L1 loss: 0.07229864597320557\n",
      "Training batch 73 with loss 0.30366\n",
      "------------------------------------------\n",
      "L1 loss: 0.09584933519363403\n",
      "Training batch 74 with loss 0.31747\n",
      "------------------------------------------\n",
      "L1 loss: 0.07192998379468918\n",
      "Training batch 75 with loss 0.28108\n",
      "------------------------------------------\n",
      "L1 loss: 0.08643684536218643\n",
      "Training batch 76 with loss 0.28921\n",
      "------------------------------------------\n",
      "L1 loss: 0.07896555960178375\n",
      "Training batch 77 with loss 0.26335\n",
      "------------------------------------------\n",
      "L1 loss: 0.08294027298688889\n",
      "Training batch 78 with loss 0.29390\n",
      "------------------------------------------\n",
      "L1 loss: 0.10537135601043701\n",
      "Training batch 79 with loss 0.35121\n",
      "------------------------------------------\n",
      "L1 loss: 0.07703179121017456\n",
      "Training batch 80 with loss 0.27884\n",
      "------------------------------------------\n",
      "L1 loss: 0.07677000761032104\n",
      "Training batch 81 with loss 0.30357\n",
      "------------------------------------------\n",
      "L1 loss: 0.087924063205719\n",
      "Training batch 82 with loss 0.34734\n",
      "------------------------------------------\n",
      "L1 loss: 0.07428912818431854\n",
      "Training batch 83 with loss 0.27971\n",
      "------------------------------------------\n",
      "L1 loss: 0.06773059815168381\n",
      "Training batch 84 with loss 0.30138\n",
      "------------------------------------------\n",
      "L1 loss: 0.06789427995681763\n",
      "Training batch 85 with loss 0.29183\n",
      "------------------------------------------\n",
      "L1 loss: 0.08293575793504715\n",
      "Training batch 86 with loss 0.29429\n",
      "------------------------------------------\n",
      "L1 loss: 0.09940608590841293\n",
      "Training batch 87 with loss 0.40571\n",
      "------------------------------------------\n",
      "L1 loss: 0.0777820497751236\n",
      "Training batch 88 with loss 0.31844\n",
      "------------------------------------------\n",
      "L1 loss: 0.06964350491762161\n",
      "Training batch 89 with loss 0.26679\n",
      "------------------------------------------\n",
      "L1 loss: 0.07742272317409515\n",
      "Training batch 90 with loss 0.29516\n",
      "------------------------------------------\n",
      "L1 loss: 0.06253483891487122\n",
      "Training batch 91 with loss 0.26871\n",
      "------------------------------------------\n",
      "L1 loss: 0.08852910995483398\n",
      "Training batch 92 with loss 0.30656\n",
      "------------------------------------------\n",
      "L1 loss: 0.055441733449697495\n",
      "Training batch 93 with loss 0.24999\n",
      "------------------------------------------\n",
      "L1 loss: 0.08419141173362732\n",
      "Training batch 94 with loss 0.30010\n",
      "------------------------------------------\n",
      "L1 loss: 0.07273322343826294\n",
      "Training batch 95 with loss 0.26769\n",
      "------------------------------------------\n",
      "L1 loss: 0.06392021477222443\n",
      "Training batch 96 with loss 0.27808\n",
      "------------------------------------------\n",
      "L1 loss: 0.08872591704130173\n",
      "Training batch 97 with loss 0.37367\n",
      "------------------------------------------\n",
      "L1 loss: 0.055128492414951324\n",
      "Training batch 98 with loss 0.23451\n",
      "------------------------------------------\n",
      "L1 loss: 0.06504225730895996\n",
      "Training batch 99 with loss 0.27366\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29602976962924005\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4822\n",
      "------------------------------------------\n",
      "L1 loss: 0.061744801700115204\n",
      "Training batch 0 with loss 0.25363\n",
      "------------------------------------------\n",
      "L1 loss: 0.08186859637498856\n",
      "Training batch 1 with loss 0.28388\n",
      "------------------------------------------\n",
      "L1 loss: 0.09325313568115234\n",
      "Training batch 2 with loss 0.32969\n",
      "------------------------------------------\n",
      "L1 loss: 0.05513876676559448\n",
      "Training batch 3 with loss 0.35906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.08359745889902115\n",
      "Training batch 4 with loss 0.33983\n",
      "------------------------------------------\n",
      "L1 loss: 0.06611204892396927\n",
      "Training batch 5 with loss 0.31794\n",
      "------------------------------------------\n",
      "L1 loss: 0.07995878905057907\n",
      "Training batch 6 with loss 0.27209\n",
      "------------------------------------------\n",
      "L1 loss: 0.06787962466478348\n",
      "Training batch 7 with loss 0.30255\n",
      "------------------------------------------\n",
      "L1 loss: 0.0792945921421051\n",
      "Training batch 8 with loss 0.33050\n",
      "------------------------------------------\n",
      "L1 loss: 0.06154915317893028\n",
      "Training batch 9 with loss 0.24078\n",
      "------------------------------------------\n",
      "L1 loss: 0.08219242095947266\n",
      "Training batch 10 with loss 0.29410\n",
      "------------------------------------------\n",
      "L1 loss: 0.06096452847123146\n",
      "Training batch 11 with loss 0.28138\n",
      "------------------------------------------\n",
      "L1 loss: 0.07666774094104767\n",
      "Training batch 12 with loss 0.31491\n",
      "------------------------------------------\n",
      "L1 loss: 0.07007163017988205\n",
      "Training batch 13 with loss 0.24250\n",
      "------------------------------------------\n",
      "L1 loss: 0.05330057069659233\n",
      "Training batch 14 with loss 0.25519\n",
      "------------------------------------------\n",
      "L1 loss: 0.0835113376379013\n",
      "Training batch 15 with loss 0.27329\n",
      "------------------------------------------\n",
      "L1 loss: 0.06440822035074234\n",
      "Training batch 16 with loss 0.27476\n",
      "------------------------------------------\n",
      "L1 loss: 0.08426258713006973\n",
      "Training batch 17 with loss 0.33682\n",
      "------------------------------------------\n",
      "L1 loss: 0.08056077361106873\n",
      "Training batch 18 with loss 0.30161\n",
      "------------------------------------------\n",
      "L1 loss: 0.06284927576780319\n",
      "Training batch 19 with loss 0.23537\n",
      "------------------------------------------\n",
      "L1 loss: 0.0748128667473793\n",
      "Training batch 20 with loss 0.31034\n",
      "------------------------------------------\n",
      "L1 loss: 0.07292474061250687\n",
      "Training batch 21 with loss 0.28243\n",
      "------------------------------------------\n",
      "L1 loss: 0.06746894121170044\n",
      "Training batch 22 with loss 0.31121\n",
      "------------------------------------------\n",
      "L1 loss: 0.09216311573982239\n",
      "Training batch 23 with loss 0.37527\n",
      "------------------------------------------\n",
      "L1 loss: 0.06814907491207123\n",
      "Training batch 24 with loss 0.30002\n",
      "------------------------------------------\n",
      "L1 loss: 0.07529472559690475\n",
      "Training batch 25 with loss 0.31055\n",
      "------------------------------------------\n",
      "L1 loss: 0.07700704783201218\n",
      "Training batch 26 with loss 0.28325\n",
      "------------------------------------------\n",
      "L1 loss: 0.05481396242976189\n",
      "Training batch 27 with loss 0.29090\n",
      "------------------------------------------\n",
      "L1 loss: 0.05722441524267197\n",
      "Training batch 28 with loss 0.28568\n",
      "------------------------------------------\n",
      "L1 loss: 0.055278386920690536\n",
      "Training batch 29 with loss 0.24474\n",
      "------------------------------------------\n",
      "L1 loss: 0.05918728560209274\n",
      "Training batch 30 with loss 0.27106\n",
      "------------------------------------------\n",
      "L1 loss: 0.06418829411268234\n",
      "Training batch 31 with loss 0.31487\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976645439863205\n",
      "Training batch 32 with loss 0.29963\n",
      "------------------------------------------\n",
      "L1 loss: 0.061866067349910736\n",
      "Training batch 33 with loss 0.26790\n",
      "------------------------------------------\n",
      "L1 loss: 0.07676120847463608\n",
      "Training batch 34 with loss 0.31920\n",
      "------------------------------------------\n",
      "L1 loss: 0.09691083431243896\n",
      "Training batch 35 with loss 0.34402\n",
      "------------------------------------------\n",
      "L1 loss: 0.08194954693317413\n",
      "Training batch 36 with loss 0.28796\n",
      "------------------------------------------\n",
      "L1 loss: 0.0659266859292984\n",
      "Training batch 37 with loss 0.24552\n",
      "------------------------------------------\n",
      "L1 loss: 0.06316531449556351\n",
      "Training batch 38 with loss 0.21741\n",
      "------------------------------------------\n",
      "L1 loss: 0.0673760399222374\n",
      "Training batch 39 with loss 0.28909\n",
      "------------------------------------------\n",
      "L1 loss: 0.06863747537136078\n",
      "Training batch 40 with loss 0.24827\n",
      "------------------------------------------\n",
      "L1 loss: 0.061341527849435806\n",
      "Training batch 41 with loss 0.29265\n",
      "------------------------------------------\n",
      "L1 loss: 0.0742531344294548\n",
      "Training batch 42 with loss 0.30101\n",
      "------------------------------------------\n",
      "L1 loss: 0.06737960875034332\n",
      "Training batch 43 with loss 0.27147\n",
      "------------------------------------------\n",
      "L1 loss: 0.056018978357315063\n",
      "Training batch 44 with loss 0.27472\n",
      "------------------------------------------\n",
      "L1 loss: 0.09265311062335968\n",
      "Training batch 45 with loss 0.35787\n",
      "------------------------------------------\n",
      "L1 loss: 0.07062304019927979\n",
      "Training batch 46 with loss 0.26168\n",
      "------------------------------------------\n",
      "L1 loss: 0.11097269505262375\n",
      "Training batch 47 with loss 0.36973\n",
      "------------------------------------------\n",
      "L1 loss: 0.04959489405155182\n",
      "Training batch 48 with loss 0.26602\n",
      "------------------------------------------\n",
      "L1 loss: 0.07073965668678284\n",
      "Training batch 49 with loss 0.67997\n",
      "------------------------------------------\n",
      "L1 loss: 0.07248343527317047\n",
      "Training batch 50 with loss 0.30007\n",
      "------------------------------------------\n",
      "L1 loss: 0.07857871055603027\n",
      "Training batch 51 with loss 0.33546\n",
      "------------------------------------------\n",
      "L1 loss: 0.08875604718923569\n",
      "Training batch 52 with loss 0.29896\n",
      "------------------------------------------\n",
      "L1 loss: 0.07833971828222275\n",
      "Training batch 53 with loss 0.30045\n",
      "------------------------------------------\n",
      "L1 loss: 0.0734405443072319\n",
      "Training batch 54 with loss 0.26806\n",
      "------------------------------------------\n",
      "L1 loss: 0.062199752777814865\n",
      "Training batch 55 with loss 0.31854\n",
      "------------------------------------------\n",
      "L1 loss: 0.050571586936712265\n",
      "Training batch 56 with loss 0.23937\n",
      "------------------------------------------\n",
      "L1 loss: 0.06682097911834717\n",
      "Training batch 57 with loss 0.27156\n",
      "------------------------------------------\n",
      "L1 loss: 0.06552127748727798\n",
      "Training batch 58 with loss 0.25019\n",
      "------------------------------------------\n",
      "L1 loss: 0.07972849905490875\n",
      "Training batch 59 with loss 0.29643\n",
      "------------------------------------------\n",
      "L1 loss: 0.06496105343103409\n",
      "Training batch 60 with loss 0.27123\n",
      "------------------------------------------\n",
      "L1 loss: 0.10157611966133118\n",
      "Training batch 61 with loss 0.35565\n",
      "------------------------------------------\n",
      "L1 loss: 0.057342950254678726\n",
      "Training batch 62 with loss 0.26328\n",
      "------------------------------------------\n",
      "L1 loss: 0.06042967364192009\n",
      "Training batch 63 with loss 0.24985\n",
      "------------------------------------------\n",
      "L1 loss: 0.07084299623966217\n",
      "Training batch 64 with loss 0.30501\n",
      "------------------------------------------\n",
      "L1 loss: 0.05812230333685875\n",
      "Training batch 65 with loss 0.22194\n",
      "------------------------------------------\n",
      "L1 loss: 0.06696688383817673\n",
      "Training batch 66 with loss 0.27640\n",
      "------------------------------------------\n",
      "L1 loss: 0.06614984571933746\n",
      "Training batch 67 with loss 0.27222\n",
      "------------------------------------------\n",
      "L1 loss: 0.07163925468921661\n",
      "Training batch 68 with loss 0.30219\n",
      "------------------------------------------\n",
      "L1 loss: 0.04759397357702255\n",
      "Training batch 69 with loss 0.26316\n",
      "------------------------------------------\n",
      "L1 loss: 0.08284001797437668\n",
      "Training batch 70 with loss 0.32073\n",
      "------------------------------------------\n",
      "L1 loss: 0.07152711600065231\n",
      "Training batch 71 with loss 0.30477\n",
      "------------------------------------------\n",
      "L1 loss: 0.08260426670312881\n",
      "Training batch 72 with loss 0.31682\n",
      "------------------------------------------\n",
      "L1 loss: 0.07224880903959274\n",
      "Training batch 73 with loss 0.29793\n",
      "------------------------------------------\n",
      "L1 loss: 0.0980500727891922\n",
      "Training batch 74 with loss 0.31841\n",
      "------------------------------------------\n",
      "L1 loss: 0.07388723641633987\n",
      "Training batch 75 with loss 0.28054\n",
      "------------------------------------------\n",
      "L1 loss: 0.0858406126499176\n",
      "Training batch 76 with loss 0.31962\n",
      "------------------------------------------\n",
      "L1 loss: 0.07820900529623032\n",
      "Training batch 77 with loss 0.25646\n",
      "------------------------------------------\n",
      "L1 loss: 0.08304110169410706\n",
      "Training batch 78 with loss 0.33054\n",
      "------------------------------------------\n",
      "L1 loss: 0.10876227915287018\n",
      "Training batch 79 with loss 0.38386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.07611924409866333\n",
      "Training batch 80 with loss 0.28466\n",
      "------------------------------------------\n",
      "L1 loss: 0.0791647881269455\n",
      "Training batch 81 with loss 0.28874\n",
      "------------------------------------------\n",
      "L1 loss: 0.08229085057973862\n",
      "Training batch 82 with loss 0.34938\n",
      "------------------------------------------\n",
      "L1 loss: 0.07960337400436401\n",
      "Training batch 83 with loss 0.28867\n",
      "------------------------------------------\n",
      "L1 loss: 0.07064040750265121\n",
      "Training batch 84 with loss 0.29116\n",
      "------------------------------------------\n",
      "L1 loss: 0.06208052113652229\n",
      "Training batch 85 with loss 0.25852\n",
      "------------------------------------------\n",
      "L1 loss: 0.07969383150339127\n",
      "Training batch 86 with loss 0.28546\n",
      "------------------------------------------\n",
      "L1 loss: 0.10184460878372192\n",
      "Training batch 87 with loss 0.40347\n",
      "------------------------------------------\n",
      "L1 loss: 0.07589572668075562\n",
      "Training batch 88 with loss 0.31288\n",
      "------------------------------------------\n",
      "L1 loss: 0.07039467245340347\n",
      "Training batch 89 with loss 0.26527\n",
      "------------------------------------------\n",
      "L1 loss: 0.07328709214925766\n",
      "Training batch 90 with loss 0.27188\n",
      "------------------------------------------\n",
      "L1 loss: 0.06280924379825592\n",
      "Training batch 91 with loss 0.25382\n",
      "------------------------------------------\n",
      "L1 loss: 0.09052614122629166\n",
      "Training batch 92 with loss 0.29941\n",
      "------------------------------------------\n",
      "L1 loss: 0.06411857157945633\n",
      "Training batch 93 with loss 0.25112\n",
      "------------------------------------------\n",
      "L1 loss: 0.08598947525024414\n",
      "Training batch 94 with loss 0.31255\n",
      "------------------------------------------\n",
      "L1 loss: 0.06907401978969574\n",
      "Training batch 95 with loss 0.25218\n",
      "------------------------------------------\n",
      "L1 loss: 0.05594613775610924\n",
      "Training batch 96 with loss 0.27340\n",
      "------------------------------------------\n",
      "L1 loss: 0.08900139480829239\n",
      "Training batch 97 with loss 0.36453\n",
      "------------------------------------------\n",
      "L1 loss: 0.05664922297000885\n",
      "Training batch 98 with loss 0.23803\n",
      "------------------------------------------\n",
      "L1 loss: 0.06387334316968918\n",
      "Training batch 99 with loss 0.26660\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2961574947834015\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4823\n",
      "------------------------------------------\n",
      "L1 loss: 0.06177571415901184\n",
      "Training batch 0 with loss 0.24802\n",
      "------------------------------------------\n",
      "L1 loss: 0.08508915454149246\n",
      "Training batch 1 with loss 0.29235\n",
      "------------------------------------------\n",
      "L1 loss: 0.09390727430582047\n",
      "Training batch 2 with loss 0.33906\n",
      "------------------------------------------\n",
      "L1 loss: 0.071096271276474\n",
      "Training batch 3 with loss 0.25690\n",
      "------------------------------------------\n",
      "L1 loss: 0.08464611321687698\n",
      "Training batch 4 with loss 0.33365\n",
      "------------------------------------------\n",
      "L1 loss: 0.07013209164142609\n",
      "Training batch 5 with loss 0.35734\n",
      "------------------------------------------\n",
      "L1 loss: 0.07960323244333267\n",
      "Training batch 6 with loss 0.28458\n",
      "------------------------------------------\n",
      "L1 loss: 0.08282846957445145\n",
      "Training batch 7 with loss 0.33042\n",
      "------------------------------------------\n",
      "L1 loss: 0.07848213613033295\n",
      "Training batch 8 with loss 0.32526\n",
      "------------------------------------------\n",
      "L1 loss: 0.059687189757823944\n",
      "Training batch 9 with loss 0.26713\n",
      "------------------------------------------\n",
      "L1 loss: 0.08180312812328339\n",
      "Training batch 10 with loss 0.30578\n",
      "------------------------------------------\n",
      "L1 loss: 0.06215773895382881\n",
      "Training batch 11 with loss 0.28232\n",
      "------------------------------------------\n",
      "L1 loss: 0.07731947302818298\n",
      "Training batch 12 with loss 0.31013\n",
      "------------------------------------------\n",
      "L1 loss: 0.06956621259450912\n",
      "Training batch 13 with loss 0.24987\n",
      "------------------------------------------\n",
      "L1 loss: 0.05210332199931145\n",
      "Training batch 14 with loss 0.25965\n",
      "------------------------------------------\n",
      "L1 loss: 0.08359520882368088\n",
      "Training batch 15 with loss 0.27295\n",
      "------------------------------------------\n",
      "L1 loss: 0.06198932230472565\n",
      "Training batch 16 with loss 0.25446\n",
      "------------------------------------------\n",
      "L1 loss: 0.06629537045955658\n",
      "Training batch 17 with loss 0.47842\n",
      "------------------------------------------\n",
      "L1 loss: 0.08033040165901184\n",
      "Training batch 18 with loss 0.29801\n",
      "------------------------------------------\n",
      "L1 loss: 0.06073495373129845\n",
      "Training batch 19 with loss 0.23569\n",
      "------------------------------------------\n",
      "L1 loss: 0.07476354390382767\n",
      "Training batch 20 with loss 0.31569\n",
      "------------------------------------------\n",
      "L1 loss: 0.06759103387594223\n",
      "Training batch 21 with loss 0.27793\n",
      "------------------------------------------\n",
      "L1 loss: 0.06805803626775742\n",
      "Training batch 22 with loss 0.27473\n",
      "------------------------------------------\n",
      "L1 loss: 0.08892017602920532\n",
      "Training batch 23 with loss 0.36460\n",
      "------------------------------------------\n",
      "L1 loss: 0.06693681329488754\n",
      "Training batch 24 with loss 0.30538\n",
      "------------------------------------------\n",
      "L1 loss: 0.07672577351331711\n",
      "Training batch 25 with loss 0.30373\n",
      "------------------------------------------\n",
      "L1 loss: 0.07595372945070267\n",
      "Training batch 26 with loss 0.28725\n",
      "------------------------------------------\n",
      "L1 loss: 0.05649451166391373\n",
      "Training batch 27 with loss 0.30183\n",
      "------------------------------------------\n",
      "L1 loss: 0.0572279691696167\n",
      "Training batch 28 with loss 0.29871\n",
      "------------------------------------------\n",
      "L1 loss: 0.062261100858449936\n",
      "Training batch 29 with loss 0.25410\n",
      "------------------------------------------\n",
      "L1 loss: 0.06045220419764519\n",
      "Training batch 30 with loss 0.27398\n",
      "------------------------------------------\n",
      "L1 loss: 0.06063307821750641\n",
      "Training batch 31 with loss 0.30049\n",
      "------------------------------------------\n",
      "L1 loss: 0.06965082883834839\n",
      "Training batch 32 with loss 0.30372\n",
      "------------------------------------------\n",
      "L1 loss: 0.06177153438329697\n",
      "Training batch 33 with loss 0.25556\n",
      "------------------------------------------\n",
      "L1 loss: 0.0733724981546402\n",
      "Training batch 34 with loss 0.29586\n",
      "------------------------------------------\n",
      "L1 loss: 0.09556703269481659\n",
      "Training batch 35 with loss 0.33787\n",
      "------------------------------------------\n",
      "L1 loss: 0.08756199479103088\n",
      "Training batch 36 with loss 0.28495\n",
      "------------------------------------------\n",
      "L1 loss: 0.06794338673353195\n",
      "Training batch 37 with loss 0.26749\n",
      "------------------------------------------\n",
      "L1 loss: 0.06250277161598206\n",
      "Training batch 38 with loss 0.22440\n",
      "------------------------------------------\n",
      "L1 loss: 0.06927698105573654\n",
      "Training batch 39 with loss 0.29959\n",
      "------------------------------------------\n",
      "L1 loss: 0.0689246654510498\n",
      "Training batch 40 with loss 0.26141\n",
      "------------------------------------------\n",
      "L1 loss: 0.05597275495529175\n",
      "Training batch 41 with loss 0.28080\n",
      "------------------------------------------\n",
      "L1 loss: 0.07726471871137619\n",
      "Training batch 42 with loss 0.29232\n",
      "------------------------------------------\n",
      "L1 loss: 0.07101988792419434\n",
      "Training batch 43 with loss 0.28194\n",
      "------------------------------------------\n",
      "L1 loss: 0.056951116770505905\n",
      "Training batch 44 with loss 0.26642\n",
      "------------------------------------------\n",
      "L1 loss: 0.09204091876745224\n",
      "Training batch 45 with loss 0.38205\n",
      "------------------------------------------\n",
      "L1 loss: 0.07080952078104019\n",
      "Training batch 46 with loss 0.25901\n",
      "------------------------------------------\n",
      "L1 loss: 0.1087978184223175\n",
      "Training batch 47 with loss 0.36051\n",
      "------------------------------------------\n",
      "L1 loss: 0.048392023891210556\n",
      "Training batch 48 with loss 0.24340\n",
      "------------------------------------------\n",
      "L1 loss: 0.07775264233350754\n",
      "Training batch 49 with loss 0.35655\n",
      "------------------------------------------\n",
      "L1 loss: 0.07176289707422256\n",
      "Training batch 50 with loss 0.30735\n",
      "------------------------------------------\n",
      "L1 loss: 0.07921614497900009\n",
      "Training batch 51 with loss 0.32586\n",
      "------------------------------------------\n",
      "L1 loss: 0.08864732831716537\n",
      "Training batch 52 with loss 0.28903\n",
      "------------------------------------------\n",
      "L1 loss: 0.07950545102357864\n",
      "Training batch 53 with loss 0.29305\n",
      "------------------------------------------\n",
      "L1 loss: 0.07305198907852173\n",
      "Training batch 54 with loss 0.26209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.06247803568840027\n",
      "Training batch 55 with loss 0.31802\n",
      "------------------------------------------\n",
      "L1 loss: 0.053879108279943466\n",
      "Training batch 56 with loss 0.25099\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776514649391174\n",
      "Training batch 57 with loss 0.27415\n",
      "------------------------------------------\n",
      "L1 loss: 0.06669396907091141\n",
      "Training batch 58 with loss 0.26174\n",
      "------------------------------------------\n",
      "L1 loss: 0.07936181873083115\n",
      "Training batch 59 with loss 0.28528\n",
      "------------------------------------------\n",
      "L1 loss: 0.06472381204366684\n",
      "Training batch 60 with loss 0.28752\n",
      "------------------------------------------\n",
      "L1 loss: 0.10026608407497406\n",
      "Training batch 61 with loss 0.34930\n",
      "------------------------------------------\n",
      "L1 loss: 0.05528118461370468\n",
      "Training batch 62 with loss 0.25186\n",
      "------------------------------------------\n",
      "L1 loss: 0.06129393354058266\n",
      "Training batch 63 with loss 0.24994\n",
      "------------------------------------------\n",
      "L1 loss: 0.07040262967348099\n",
      "Training batch 64 with loss 0.27205\n",
      "------------------------------------------\n",
      "L1 loss: 0.058398813009262085\n",
      "Training batch 65 with loss 0.21899\n",
      "------------------------------------------\n",
      "L1 loss: 0.06582479923963547\n",
      "Training batch 66 with loss 0.26937\n",
      "------------------------------------------\n",
      "L1 loss: 0.06652120500802994\n",
      "Training batch 67 with loss 0.27356\n",
      "------------------------------------------\n",
      "L1 loss: 0.07213100790977478\n",
      "Training batch 68 with loss 0.30485\n",
      "------------------------------------------\n",
      "L1 loss: 0.05175676941871643\n",
      "Training batch 69 with loss 0.25883\n",
      "------------------------------------------\n",
      "L1 loss: 0.08005963265895844\n",
      "Training batch 70 with loss 0.30711\n",
      "------------------------------------------\n",
      "L1 loss: 0.07135260850191116\n",
      "Training batch 71 with loss 0.29738\n",
      "------------------------------------------\n",
      "L1 loss: 0.08785084635019302\n",
      "Training batch 72 with loss 0.32892\n",
      "------------------------------------------\n",
      "L1 loss: 0.07088819891214371\n",
      "Training batch 73 with loss 0.28506\n",
      "------------------------------------------\n",
      "L1 loss: 0.09465990960597992\n",
      "Training batch 74 with loss 0.31482\n",
      "------------------------------------------\n",
      "L1 loss: 0.07098641991615295\n",
      "Training batch 75 with loss 0.46118\n",
      "------------------------------------------\n",
      "L1 loss: 0.0862351506948471\n",
      "Training batch 76 with loss 0.29938\n",
      "------------------------------------------\n",
      "L1 loss: 0.07830146700143814\n",
      "Training batch 77 with loss 0.27745\n",
      "------------------------------------------\n",
      "L1 loss: 0.07530692964792252\n",
      "Training batch 78 with loss 0.48104\n",
      "------------------------------------------\n",
      "L1 loss: 0.09520988911390305\n",
      "Training batch 79 with loss 0.32572\n",
      "------------------------------------------\n",
      "L1 loss: 0.07503590732812881\n",
      "Training batch 80 with loss 0.26867\n",
      "------------------------------------------\n",
      "L1 loss: 0.07530719041824341\n",
      "Training batch 81 with loss 0.28574\n",
      "------------------------------------------\n",
      "L1 loss: 0.0834842100739479\n",
      "Training batch 82 with loss 0.34881\n",
      "------------------------------------------\n",
      "L1 loss: 0.07769101113080978\n",
      "Training batch 83 with loss 0.28168\n",
      "------------------------------------------\n",
      "L1 loss: 0.07334358245134354\n",
      "Training batch 84 with loss 0.32815\n",
      "------------------------------------------\n",
      "L1 loss: 0.061086032539606094\n",
      "Training batch 85 with loss 0.26815\n",
      "------------------------------------------\n",
      "L1 loss: 0.08245062083005905\n",
      "Training batch 86 with loss 0.29732\n",
      "------------------------------------------\n",
      "L1 loss: 0.1021026223897934\n",
      "Training batch 87 with loss 0.38600\n",
      "------------------------------------------\n",
      "L1 loss: 0.0774218812584877\n",
      "Training batch 88 with loss 0.30970\n",
      "------------------------------------------\n",
      "L1 loss: 0.07052954286336899\n",
      "Training batch 89 with loss 0.25960\n",
      "------------------------------------------\n",
      "L1 loss: 0.07603476196527481\n",
      "Training batch 90 with loss 0.28300\n",
      "------------------------------------------\n",
      "L1 loss: 0.06196830794215202\n",
      "Training batch 91 with loss 0.25776\n",
      "------------------------------------------\n",
      "L1 loss: 0.09256569296121597\n",
      "Training batch 92 with loss 0.29538\n",
      "------------------------------------------\n",
      "L1 loss: 0.05331496149301529\n",
      "Training batch 93 with loss 0.23614\n",
      "------------------------------------------\n",
      "L1 loss: 0.08587145060300827\n",
      "Training batch 94 with loss 0.30214\n",
      "------------------------------------------\n",
      "L1 loss: 0.07000212371349335\n",
      "Training batch 95 with loss 0.24994\n",
      "------------------------------------------\n",
      "L1 loss: 0.06524202972650528\n",
      "Training batch 96 with loss 0.28637\n",
      "------------------------------------------\n",
      "L1 loss: 0.08287820219993591\n",
      "Training batch 97 with loss 0.35450\n",
      "------------------------------------------\n",
      "L1 loss: 0.056769318878650665\n",
      "Training batch 98 with loss 0.24363\n",
      "------------------------------------------\n",
      "L1 loss: 0.06468885391950607\n",
      "Training batch 99 with loss 0.26232\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.295811917334795\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4824\n",
      "------------------------------------------\n",
      "L1 loss: 0.06091863289475441\n",
      "Training batch 0 with loss 0.25513\n",
      "------------------------------------------\n",
      "L1 loss: 0.08466320484876633\n",
      "Training batch 1 with loss 0.28680\n",
      "------------------------------------------\n",
      "L1 loss: 0.0917433351278305\n",
      "Training batch 2 with loss 0.33182\n",
      "------------------------------------------\n",
      "L1 loss: 0.07500217109918594\n",
      "Training batch 3 with loss 0.26101\n",
      "------------------------------------------\n",
      "L1 loss: 0.07851342856884003\n",
      "Training batch 4 with loss 0.48520\n",
      "------------------------------------------\n",
      "L1 loss: 0.06496777385473251\n",
      "Training batch 5 with loss 0.33694\n",
      "------------------------------------------\n",
      "L1 loss: 0.07906245440244675\n",
      "Training batch 6 with loss 0.27435\n",
      "------------------------------------------\n",
      "L1 loss: 0.08463340252637863\n",
      "Training batch 7 with loss 0.32053\n",
      "------------------------------------------\n",
      "L1 loss: 0.08341535180807114\n",
      "Training batch 8 with loss 0.35511\n",
      "------------------------------------------\n",
      "L1 loss: 0.06188734248280525\n",
      "Training batch 9 with loss 0.25525\n",
      "------------------------------------------\n",
      "L1 loss: 0.08447866886854172\n",
      "Training batch 10 with loss 0.30718\n",
      "------------------------------------------\n",
      "L1 loss: 0.06175312399864197\n",
      "Training batch 11 with loss 0.29328\n",
      "------------------------------------------\n",
      "L1 loss: 0.06153804063796997\n",
      "Training batch 12 with loss 0.25706\n",
      "------------------------------------------\n",
      "L1 loss: 0.06897006928920746\n",
      "Training batch 13 with loss 0.24341\n",
      "------------------------------------------\n",
      "L1 loss: 0.05099346116185188\n",
      "Training batch 14 with loss 0.24227\n",
      "------------------------------------------\n",
      "L1 loss: 0.0824292004108429\n",
      "Training batch 15 with loss 0.26930\n",
      "------------------------------------------\n",
      "L1 loss: 0.06218358501791954\n",
      "Training batch 16 with loss 0.26962\n",
      "------------------------------------------\n",
      "L1 loss: 0.08788922429084778\n",
      "Training batch 17 with loss 0.34751\n",
      "------------------------------------------\n",
      "L1 loss: 0.0802411362528801\n",
      "Training batch 18 with loss 0.32230\n",
      "------------------------------------------\n",
      "L1 loss: 0.06028364226222038\n",
      "Training batch 19 with loss 0.22894\n",
      "------------------------------------------\n",
      "L1 loss: 0.07337379455566406\n",
      "Training batch 20 with loss 0.32293\n",
      "------------------------------------------\n",
      "L1 loss: 0.07090207189321518\n",
      "Training batch 21 with loss 0.29407\n",
      "------------------------------------------\n",
      "L1 loss: 0.0691504254937172\n",
      "Training batch 22 with loss 0.29083\n",
      "------------------------------------------\n",
      "L1 loss: 0.08600125461816788\n",
      "Training batch 23 with loss 0.38095\n",
      "------------------------------------------\n",
      "L1 loss: 0.06902658939361572\n",
      "Training batch 24 with loss 0.31350\n",
      "------------------------------------------\n",
      "L1 loss: 0.07698080688714981\n",
      "Training batch 25 with loss 0.30702\n",
      "------------------------------------------\n",
      "L1 loss: 0.07900490611791611\n",
      "Training batch 26 with loss 0.28834\n",
      "------------------------------------------\n",
      "L1 loss: 0.0556647963821888\n",
      "Training batch 27 with loss 0.28943\n",
      "------------------------------------------\n",
      "L1 loss: 0.05735750123858452\n",
      "Training batch 28 with loss 0.30754\n",
      "------------------------------------------\n",
      "L1 loss: 0.060431450605392456\n",
      "Training batch 29 with loss 0.26975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.06208522990345955\n",
      "Training batch 30 with loss 0.25979\n",
      "------------------------------------------\n",
      "L1 loss: 0.06453312933444977\n",
      "Training batch 31 with loss 0.29310\n",
      "------------------------------------------\n",
      "L1 loss: 0.0699203759431839\n",
      "Training batch 32 with loss 0.27227\n",
      "------------------------------------------\n",
      "L1 loss: 0.06345970928668976\n",
      "Training batch 33 with loss 0.26777\n",
      "------------------------------------------\n",
      "L1 loss: 0.07599399983882904\n",
      "Training batch 34 with loss 0.34934\n",
      "------------------------------------------\n",
      "L1 loss: 0.09090670943260193\n",
      "Training batch 35 with loss 0.33678\n",
      "------------------------------------------\n",
      "L1 loss: 0.08382029086351395\n",
      "Training batch 36 with loss 0.28636\n",
      "------------------------------------------\n",
      "L1 loss: 0.06664841622114182\n",
      "Training batch 37 with loss 0.26709\n",
      "------------------------------------------\n",
      "L1 loss: 0.06170584633946419\n",
      "Training batch 38 with loss 0.22184\n",
      "------------------------------------------\n",
      "L1 loss: 0.06413834542036057\n",
      "Training batch 39 with loss 0.29106\n",
      "------------------------------------------\n",
      "L1 loss: 0.06905605643987656\n",
      "Training batch 40 with loss 0.24842\n",
      "------------------------------------------\n",
      "L1 loss: 0.05691959708929062\n",
      "Training batch 41 with loss 0.27702\n",
      "------------------------------------------\n",
      "L1 loss: 0.07941626012325287\n",
      "Training batch 42 with loss 0.27783\n",
      "------------------------------------------\n",
      "L1 loss: 0.07128642499446869\n",
      "Training batch 43 with loss 0.27871\n",
      "------------------------------------------\n",
      "L1 loss: 0.058470435440540314\n",
      "Training batch 44 with loss 0.27002\n",
      "------------------------------------------\n",
      "L1 loss: 0.09310693293809891\n",
      "Training batch 45 with loss 0.36554\n",
      "------------------------------------------\n",
      "L1 loss: 0.07156655937433243\n",
      "Training batch 46 with loss 0.28305\n",
      "------------------------------------------\n",
      "L1 loss: 0.11040067672729492\n",
      "Training batch 47 with loss 0.37174\n",
      "------------------------------------------\n",
      "L1 loss: 0.048611342906951904\n",
      "Training batch 48 with loss 0.25692\n",
      "------------------------------------------\n",
      "L1 loss: 0.07910309731960297\n",
      "Training batch 49 with loss 0.34554\n",
      "------------------------------------------\n",
      "L1 loss: 0.07039808481931686\n",
      "Training batch 50 with loss 0.29423\n",
      "------------------------------------------\n",
      "L1 loss: 0.08161778748035431\n",
      "Training batch 51 with loss 0.35518\n",
      "------------------------------------------\n",
      "L1 loss: 0.08906597644090652\n",
      "Training batch 52 with loss 0.29310\n",
      "------------------------------------------\n",
      "L1 loss: 0.07689229398965836\n",
      "Training batch 53 with loss 0.29072\n",
      "------------------------------------------\n",
      "L1 loss: 0.07393935322761536\n",
      "Training batch 54 with loss 0.27032\n",
      "------------------------------------------\n",
      "L1 loss: 0.0611761249601841\n",
      "Training batch 55 with loss 0.31586\n",
      "------------------------------------------\n",
      "L1 loss: 0.0597650445997715\n",
      "Training batch 56 with loss 0.27520\n",
      "------------------------------------------\n",
      "L1 loss: 0.06588833034038544\n",
      "Training batch 57 with loss 0.27994\n",
      "------------------------------------------\n",
      "L1 loss: 0.0632079467177391\n",
      "Training batch 58 with loss 0.25337\n",
      "------------------------------------------\n",
      "L1 loss: 0.07665441185235977\n",
      "Training batch 59 with loss 0.30681\n",
      "------------------------------------------\n",
      "L1 loss: 0.061104580760002136\n",
      "Training batch 60 with loss 0.26915\n",
      "------------------------------------------\n",
      "L1 loss: 0.09974457323551178\n",
      "Training batch 61 with loss 0.35213\n",
      "------------------------------------------\n",
      "L1 loss: 0.05715383589267731\n",
      "Training batch 62 with loss 0.25968\n",
      "------------------------------------------\n",
      "L1 loss: 0.06100500002503395\n",
      "Training batch 63 with loss 0.25413\n",
      "------------------------------------------\n",
      "L1 loss: 0.06988422572612762\n",
      "Training batch 64 with loss 0.27817\n",
      "------------------------------------------\n",
      "L1 loss: 0.06100126728415489\n",
      "Training batch 65 with loss 0.24027\n",
      "------------------------------------------\n",
      "L1 loss: 0.05312275141477585\n",
      "Training batch 66 with loss 0.25010\n",
      "------------------------------------------\n",
      "L1 loss: 0.06511188298463821\n",
      "Training batch 67 with loss 0.26807\n",
      "------------------------------------------\n",
      "L1 loss: 0.07101386040449142\n",
      "Training batch 68 with loss 0.29076\n",
      "------------------------------------------\n",
      "L1 loss: 0.05240289494395256\n",
      "Training batch 69 with loss 0.26628\n",
      "------------------------------------------\n",
      "L1 loss: 0.08154367655515671\n",
      "Training batch 70 with loss 0.31632\n",
      "------------------------------------------\n",
      "L1 loss: 0.0719924122095108\n",
      "Training batch 71 with loss 0.31212\n",
      "------------------------------------------\n",
      "L1 loss: 0.08417318016290665\n",
      "Training batch 72 with loss 0.30413\n",
      "------------------------------------------\n",
      "L1 loss: 0.07017689943313599\n",
      "Training batch 73 with loss 0.28273\n",
      "------------------------------------------\n",
      "L1 loss: 0.09755069017410278\n",
      "Training batch 74 with loss 0.30836\n",
      "------------------------------------------\n",
      "L1 loss: 0.07560219615697861\n",
      "Training batch 75 with loss 0.29840\n",
      "------------------------------------------\n",
      "L1 loss: 0.087468720972538\n",
      "Training batch 76 with loss 0.29152\n",
      "------------------------------------------\n",
      "L1 loss: 0.08029299974441528\n",
      "Training batch 77 with loss 0.27137\n",
      "------------------------------------------\n",
      "L1 loss: 0.08206935226917267\n",
      "Training batch 78 with loss 0.31085\n",
      "------------------------------------------\n",
      "L1 loss: 0.1072368174791336\n",
      "Training batch 79 with loss 0.36320\n",
      "------------------------------------------\n",
      "L1 loss: 0.07712459564208984\n",
      "Training batch 80 with loss 0.26874\n",
      "------------------------------------------\n",
      "L1 loss: 0.06473690271377563\n",
      "Training batch 81 with loss 0.42328\n",
      "------------------------------------------\n",
      "L1 loss: 0.08320885896682739\n",
      "Training batch 82 with loss 0.34854\n",
      "------------------------------------------\n",
      "L1 loss: 0.0745646134018898\n",
      "Training batch 83 with loss 0.28305\n",
      "------------------------------------------\n",
      "L1 loss: 0.06545563042163849\n",
      "Training batch 84 with loss 0.29494\n",
      "------------------------------------------\n",
      "L1 loss: 0.06084424629807472\n",
      "Training batch 85 with loss 0.26913\n",
      "------------------------------------------\n",
      "L1 loss: 0.08329356461763382\n",
      "Training batch 86 with loss 0.31313\n",
      "------------------------------------------\n",
      "L1 loss: 0.10072377324104309\n",
      "Training batch 87 with loss 0.38343\n",
      "------------------------------------------\n",
      "L1 loss: 0.07544205337762833\n",
      "Training batch 88 with loss 0.29721\n",
      "------------------------------------------\n",
      "L1 loss: 0.06940523535013199\n",
      "Training batch 89 with loss 0.26982\n",
      "------------------------------------------\n",
      "L1 loss: 0.07499232888221741\n",
      "Training batch 90 with loss 0.29503\n",
      "------------------------------------------\n",
      "L1 loss: 0.06420090794563293\n",
      "Training batch 91 with loss 0.26951\n",
      "------------------------------------------\n",
      "L1 loss: 0.08305857330560684\n",
      "Training batch 92 with loss 0.52338\n",
      "------------------------------------------\n",
      "L1 loss: 0.06532428413629532\n",
      "Training batch 93 with loss 0.25686\n",
      "------------------------------------------\n",
      "L1 loss: 0.08528383821249008\n",
      "Training batch 94 with loss 0.29122\n",
      "------------------------------------------\n",
      "L1 loss: 0.07090851664543152\n",
      "Training batch 95 with loss 0.26335\n",
      "------------------------------------------\n",
      "L1 loss: 0.06679718941450119\n",
      "Training batch 96 with loss 0.29076\n",
      "------------------------------------------\n",
      "L1 loss: 0.08030126988887787\n",
      "Training batch 97 with loss 0.35105\n",
      "------------------------------------------\n",
      "L1 loss: 0.05801970139145851\n",
      "Training batch 98 with loss 0.25526\n",
      "------------------------------------------\n",
      "L1 loss: 0.059046514332294464\n",
      "Training batch 99 with loss 0.40251\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2990026009082794\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4825\n",
      "------------------------------------------\n",
      "L1 loss: 0.05948806181550026\n",
      "Training batch 0 with loss 0.25268\n",
      "------------------------------------------\n",
      "L1 loss: 0.08155056089162827\n",
      "Training batch 1 with loss 0.27701\n",
      "------------------------------------------\n",
      "L1 loss: 0.09410280734300613\n",
      "Training batch 2 with loss 0.32089\n",
      "------------------------------------------\n",
      "L1 loss: 0.0711740031838417\n",
      "Training batch 3 with loss 0.26107\n",
      "------------------------------------------\n",
      "L1 loss: 0.0833849236369133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 4 with loss 0.33385\n",
      "------------------------------------------\n",
      "L1 loss: 0.06420934945344925\n",
      "Training batch 5 with loss 0.31480\n",
      "------------------------------------------\n",
      "L1 loss: 0.07844981551170349\n",
      "Training batch 6 with loss 0.27124\n",
      "------------------------------------------\n",
      "L1 loss: 0.08330858498811722\n",
      "Training batch 7 with loss 0.34638\n",
      "------------------------------------------\n",
      "L1 loss: 0.07908309251070023\n",
      "Training batch 8 with loss 0.32245\n",
      "------------------------------------------\n",
      "L1 loss: 0.060311369597911835\n",
      "Training batch 9 with loss 0.24385\n",
      "------------------------------------------\n",
      "L1 loss: 0.08289673924446106\n",
      "Training batch 10 with loss 0.30015\n",
      "------------------------------------------\n",
      "L1 loss: 0.06058097630739212\n",
      "Training batch 11 with loss 0.29910\n",
      "------------------------------------------\n",
      "L1 loss: 0.07438013702630997\n",
      "Training batch 12 with loss 0.31008\n",
      "------------------------------------------\n",
      "L1 loss: 0.06883113086223602\n",
      "Training batch 13 with loss 0.23675\n",
      "------------------------------------------\n",
      "L1 loss: 0.05252800136804581\n",
      "Training batch 14 with loss 0.24400\n",
      "------------------------------------------\n",
      "L1 loss: 0.08295878022909164\n",
      "Training batch 15 with loss 0.27182\n",
      "------------------------------------------\n",
      "L1 loss: 0.06269510835409164\n",
      "Training batch 16 with loss 0.26644\n",
      "------------------------------------------\n",
      "L1 loss: 0.08352655917406082\n",
      "Training batch 17 with loss 0.33298\n",
      "------------------------------------------\n",
      "L1 loss: 0.07922845333814621\n",
      "Training batch 18 with loss 0.29976\n",
      "------------------------------------------\n",
      "L1 loss: 0.06279011815786362\n",
      "Training batch 19 with loss 0.22208\n",
      "------------------------------------------\n",
      "L1 loss: 0.0739913359284401\n",
      "Training batch 20 with loss 0.31342\n",
      "------------------------------------------\n",
      "L1 loss: 0.06939221173524857\n",
      "Training batch 21 with loss 0.27904\n",
      "------------------------------------------\n",
      "L1 loss: 0.0673014223575592\n",
      "Training batch 22 with loss 0.28441\n",
      "------------------------------------------\n",
      "L1 loss: 0.08793073892593384\n",
      "Training batch 23 with loss 0.36382\n",
      "------------------------------------------\n",
      "L1 loss: 0.06700236350297928\n",
      "Training batch 24 with loss 0.31423\n",
      "------------------------------------------\n",
      "L1 loss: 0.07208721339702606\n",
      "Training batch 25 with loss 0.29465\n",
      "------------------------------------------\n",
      "L1 loss: 0.07525043189525604\n",
      "Training batch 26 with loss 0.27225\n",
      "------------------------------------------\n",
      "L1 loss: 0.05477111414074898\n",
      "Training batch 27 with loss 0.28773\n",
      "------------------------------------------\n",
      "L1 loss: 0.05762581527233124\n",
      "Training batch 28 with loss 0.29980\n",
      "------------------------------------------\n",
      "L1 loss: 0.06083423271775246\n",
      "Training batch 29 with loss 0.25724\n",
      "------------------------------------------\n",
      "L1 loss: 0.0621163584291935\n",
      "Training batch 30 with loss 0.27797\n",
      "------------------------------------------\n",
      "L1 loss: 0.06698565930128098\n",
      "Training batch 31 with loss 0.30599\n",
      "------------------------------------------\n",
      "L1 loss: 0.07102332264184952\n",
      "Training batch 32 with loss 0.28054\n",
      "------------------------------------------\n",
      "L1 loss: 0.061951134353876114\n",
      "Training batch 33 with loss 0.25912\n",
      "------------------------------------------\n",
      "L1 loss: 0.07379493862390518\n",
      "Training batch 34 with loss 0.31190\n",
      "------------------------------------------\n",
      "L1 loss: 0.09462570399045944\n",
      "Training batch 35 with loss 0.33266\n",
      "------------------------------------------\n",
      "L1 loss: 0.08312665671110153\n",
      "Training batch 36 with loss 0.27333\n",
      "------------------------------------------\n",
      "L1 loss: 0.06824671477079391\n",
      "Training batch 37 with loss 0.26350\n",
      "------------------------------------------\n",
      "L1 loss: 0.06342244893312454\n",
      "Training batch 38 with loss 0.22496\n",
      "------------------------------------------\n",
      "L1 loss: 0.06384456902742386\n",
      "Training batch 39 with loss 0.29394\n",
      "------------------------------------------\n",
      "L1 loss: 0.06864859163761139\n",
      "Training batch 40 with loss 0.24682\n",
      "------------------------------------------\n",
      "L1 loss: 0.057306643575429916\n",
      "Training batch 41 with loss 0.28011\n",
      "------------------------------------------\n",
      "L1 loss: 0.07664857059717178\n",
      "Training batch 42 with loss 0.28023\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702308714389801\n",
      "Training batch 43 with loss 0.27247\n",
      "------------------------------------------\n",
      "L1 loss: 0.05644600838422775\n",
      "Training batch 44 with loss 0.26844\n",
      "------------------------------------------\n",
      "L1 loss: 0.09227481484413147\n",
      "Training batch 45 with loss 0.38006\n",
      "------------------------------------------\n",
      "L1 loss: 0.07470528036355972\n",
      "Training batch 46 with loss 0.27140\n",
      "------------------------------------------\n",
      "L1 loss: 0.10800571739673615\n",
      "Training batch 47 with loss 0.36155\n",
      "------------------------------------------\n",
      "L1 loss: 0.049739837646484375\n",
      "Training batch 48 with loss 0.25521\n",
      "------------------------------------------\n",
      "L1 loss: 0.0831533893942833\n",
      "Training batch 49 with loss 0.36105\n",
      "------------------------------------------\n",
      "L1 loss: 0.07483018934726715\n",
      "Training batch 50 with loss 0.29958\n",
      "------------------------------------------\n",
      "L1 loss: 0.08464404195547104\n",
      "Training batch 51 with loss 0.34399\n",
      "------------------------------------------\n",
      "L1 loss: 0.08918987959623337\n",
      "Training batch 52 with loss 0.29659\n",
      "------------------------------------------\n",
      "L1 loss: 0.07921385765075684\n",
      "Training batch 53 with loss 0.29209\n",
      "------------------------------------------\n",
      "L1 loss: 0.07779061049222946\n",
      "Training batch 54 with loss 0.28991\n",
      "------------------------------------------\n",
      "L1 loss: 0.06285808235406876\n",
      "Training batch 55 with loss 0.31537\n",
      "------------------------------------------\n",
      "L1 loss: 0.05897412449121475\n",
      "Training batch 56 with loss 0.25941\n",
      "------------------------------------------\n",
      "L1 loss: 0.06634394079446793\n",
      "Training batch 57 with loss 0.27045\n",
      "------------------------------------------\n",
      "L1 loss: 0.061142709106206894\n",
      "Training batch 58 with loss 0.43307\n",
      "------------------------------------------\n",
      "L1 loss: 0.08021494001150131\n",
      "Training batch 59 with loss 0.28641\n",
      "------------------------------------------\n",
      "L1 loss: 0.06389613449573517\n",
      "Training batch 60 with loss 0.28017\n",
      "------------------------------------------\n",
      "L1 loss: 0.09949655085802078\n",
      "Training batch 61 with loss 0.35588\n",
      "------------------------------------------\n",
      "L1 loss: 0.05698530748486519\n",
      "Training batch 62 with loss 0.25816\n",
      "------------------------------------------\n",
      "L1 loss: 0.06373796612024307\n",
      "Training batch 63 with loss 0.25023\n",
      "------------------------------------------\n",
      "L1 loss: 0.0711885541677475\n",
      "Training batch 64 with loss 0.28774\n",
      "------------------------------------------\n",
      "L1 loss: 0.05980801209807396\n",
      "Training batch 65 with loss 0.23211\n",
      "------------------------------------------\n",
      "L1 loss: 0.06747255474328995\n",
      "Training batch 66 with loss 0.29371\n",
      "------------------------------------------\n",
      "L1 loss: 0.06534288823604584\n",
      "Training batch 67 with loss 0.26394\n",
      "------------------------------------------\n",
      "L1 loss: 0.07507459074258804\n",
      "Training batch 68 with loss 0.30844\n",
      "------------------------------------------\n",
      "L1 loss: 0.05091352015733719\n",
      "Training batch 69 with loss 0.25973\n",
      "------------------------------------------\n",
      "L1 loss: 0.07675789296627045\n",
      "Training batch 70 with loss 0.30688\n",
      "------------------------------------------\n",
      "L1 loss: 0.07177376747131348\n",
      "Training batch 71 with loss 0.30354\n",
      "------------------------------------------\n",
      "L1 loss: 0.08737292140722275\n",
      "Training batch 72 with loss 0.33771\n",
      "------------------------------------------\n",
      "L1 loss: 0.06938491761684418\n",
      "Training batch 73 with loss 0.28206\n",
      "------------------------------------------\n",
      "L1 loss: 0.09931449592113495\n",
      "Training batch 74 with loss 0.32405\n",
      "------------------------------------------\n",
      "L1 loss: 0.0744885727763176\n",
      "Training batch 75 with loss 0.28938\n",
      "------------------------------------------\n",
      "L1 loss: 0.08701096475124359\n",
      "Training batch 76 with loss 0.28160\n",
      "------------------------------------------\n",
      "L1 loss: 0.07667645812034607\n",
      "Training batch 77 with loss 0.26712\n",
      "------------------------------------------\n",
      "L1 loss: 0.08486558496952057\n",
      "Training batch 78 with loss 0.30901\n",
      "------------------------------------------\n",
      "L1 loss: 0.10931209474802017\n",
      "Training batch 79 with loss 0.38549\n",
      "------------------------------------------\n",
      "L1 loss: 0.07672322541475296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 80 with loss 0.28382\n",
      "------------------------------------------\n",
      "L1 loss: 0.07426301389932632\n",
      "Training batch 81 with loss 0.28599\n",
      "------------------------------------------\n",
      "L1 loss: 0.08453398942947388\n",
      "Training batch 82 with loss 0.36243\n",
      "------------------------------------------\n",
      "L1 loss: 0.08026017993688583\n",
      "Training batch 83 with loss 0.28229\n",
      "------------------------------------------\n",
      "L1 loss: 0.07016509026288986\n",
      "Training batch 84 with loss 0.30937\n",
      "------------------------------------------\n",
      "L1 loss: 0.06141430512070656\n",
      "Training batch 85 with loss 0.28415\n",
      "------------------------------------------\n",
      "L1 loss: 0.0809655711054802\n",
      "Training batch 86 with loss 0.30677\n",
      "------------------------------------------\n",
      "L1 loss: 0.10253690183162689\n",
      "Training batch 87 with loss 0.37672\n",
      "------------------------------------------\n",
      "L1 loss: 0.07876353710889816\n",
      "Training batch 88 with loss 0.33315\n",
      "------------------------------------------\n",
      "L1 loss: 0.07037214189767838\n",
      "Training batch 89 with loss 0.27206\n",
      "------------------------------------------\n",
      "L1 loss: 0.06907191872596741\n",
      "Training batch 90 with loss 0.26702\n",
      "------------------------------------------\n",
      "L1 loss: 0.063250333070755\n",
      "Training batch 91 with loss 0.25692\n",
      "------------------------------------------\n",
      "L1 loss: 0.09008951485157013\n",
      "Training batch 92 with loss 0.29636\n",
      "------------------------------------------\n",
      "L1 loss: 0.06334270536899567\n",
      "Training batch 93 with loss 0.25588\n",
      "------------------------------------------\n",
      "L1 loss: 0.08750510960817337\n",
      "Training batch 94 with loss 0.32552\n",
      "------------------------------------------\n",
      "L1 loss: 0.07149049639701843\n",
      "Training batch 95 with loss 0.26510\n",
      "------------------------------------------\n",
      "L1 loss: 0.05857064202427864\n",
      "Training batch 96 with loss 0.29172\n",
      "------------------------------------------\n",
      "L1 loss: 0.07972615212202072\n",
      "Training batch 97 with loss 0.68407\n",
      "------------------------------------------\n",
      "L1 loss: 0.0585077665746212\n",
      "Training batch 98 with loss 0.23740\n",
      "------------------------------------------\n",
      "L1 loss: 0.06402959674596786\n",
      "Training batch 99 with loss 0.26994\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2970574866235256\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4826\n",
      "------------------------------------------\n",
      "L1 loss: 0.05964800715446472\n",
      "Training batch 0 with loss 0.25668\n",
      "------------------------------------------\n",
      "L1 loss: 0.08225112408399582\n",
      "Training batch 1 with loss 0.29771\n",
      "------------------------------------------\n",
      "L1 loss: 0.0956619530916214\n",
      "Training batch 2 with loss 0.31647\n",
      "------------------------------------------\n",
      "L1 loss: 0.06488524377346039\n",
      "Training batch 3 with loss 0.26053\n",
      "------------------------------------------\n",
      "L1 loss: 0.08297161757946014\n",
      "Training batch 4 with loss 0.32981\n",
      "------------------------------------------\n",
      "L1 loss: 0.06443893909454346\n",
      "Training batch 5 with loss 0.61438\n",
      "------------------------------------------\n",
      "L1 loss: 0.08042314648628235\n",
      "Training batch 6 with loss 0.27839\n",
      "------------------------------------------\n",
      "L1 loss: 0.08386323601007462\n",
      "Training batch 7 with loss 0.33100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08077073097229004\n",
      "Training batch 8 with loss 0.33121\n",
      "------------------------------------------\n",
      "L1 loss: 0.05830483138561249\n",
      "Training batch 9 with loss 0.25076\n",
      "------------------------------------------\n",
      "L1 loss: 0.08193611353635788\n",
      "Training batch 10 with loss 0.28922\n",
      "------------------------------------------\n",
      "L1 loss: 0.05742672085762024\n",
      "Training batch 11 with loss 0.29292\n",
      "------------------------------------------\n",
      "L1 loss: 0.07441175729036331\n",
      "Training batch 12 with loss 0.30142\n",
      "------------------------------------------\n",
      "L1 loss: 0.06821773201227188\n",
      "Training batch 13 with loss 0.24607\n",
      "------------------------------------------\n",
      "L1 loss: 0.053075265139341354\n",
      "Training batch 14 with loss 0.25216\n",
      "------------------------------------------\n",
      "L1 loss: 0.08286133408546448\n",
      "Training batch 15 with loss 0.27701\n",
      "------------------------------------------\n",
      "L1 loss: 0.06151451915502548\n",
      "Training batch 16 with loss 0.28053\n",
      "------------------------------------------\n",
      "L1 loss: 0.08540238440036774\n",
      "Training batch 17 with loss 0.33785\n",
      "------------------------------------------\n",
      "L1 loss: 0.08079621940851212\n",
      "Training batch 18 with loss 0.30483\n",
      "------------------------------------------\n",
      "L1 loss: 0.06228829175233841\n",
      "Training batch 19 with loss 0.23005\n",
      "------------------------------------------\n",
      "L1 loss: 0.0729701817035675\n",
      "Training batch 20 with loss 0.33340\n",
      "------------------------------------------\n",
      "L1 loss: 0.07308609783649445\n",
      "Training batch 21 with loss 0.29031\n",
      "------------------------------------------\n",
      "L1 loss: 0.06997451931238174\n",
      "Training batch 22 with loss 0.27873\n",
      "------------------------------------------\n",
      "L1 loss: 0.0908273234963417\n",
      "Training batch 23 with loss 0.36966\n",
      "------------------------------------------\n",
      "L1 loss: 0.06738907098770142\n",
      "Training batch 24 with loss 0.29961\n",
      "------------------------------------------\n",
      "L1 loss: 0.07661093771457672\n",
      "Training batch 25 with loss 0.29046\n",
      "------------------------------------------\n",
      "L1 loss: 0.06955110281705856\n",
      "Training batch 26 with loss 0.26989\n",
      "------------------------------------------\n",
      "L1 loss: 0.05606202036142349\n",
      "Training batch 27 with loss 0.28923\n",
      "------------------------------------------\n",
      "L1 loss: 0.058753035962581635\n",
      "Training batch 28 with loss 0.29503\n",
      "------------------------------------------\n",
      "L1 loss: 0.05681376904249191\n",
      "Training batch 29 with loss 0.24582\n",
      "------------------------------------------\n",
      "L1 loss: 0.062261827290058136\n",
      "Training batch 30 with loss 0.26538\n",
      "------------------------------------------\n",
      "L1 loss: 0.06426965445280075\n",
      "Training batch 31 with loss 0.29326\n",
      "------------------------------------------\n",
      "L1 loss: 0.07232268899679184\n",
      "Training batch 32 with loss 0.28666\n",
      "------------------------------------------\n",
      "L1 loss: 0.062276240438222885\n",
      "Training batch 33 with loss 0.24880\n",
      "------------------------------------------\n",
      "L1 loss: 0.07656105607748032\n",
      "Training batch 34 with loss 0.30345\n",
      "------------------------------------------\n",
      "L1 loss: 0.09433996677398682\n",
      "Training batch 35 with loss 0.32037\n",
      "------------------------------------------\n",
      "L1 loss: 0.08555640280246735\n",
      "Training batch 36 with loss 0.28636\n",
      "------------------------------------------\n",
      "L1 loss: 0.06827903538942337\n",
      "Training batch 37 with loss 0.26410\n",
      "------------------------------------------\n",
      "L1 loss: 0.06108229234814644\n",
      "Training batch 38 with loss 0.21918\n",
      "------------------------------------------\n",
      "L1 loss: 0.06516283750534058\n",
      "Training batch 39 with loss 0.29703\n",
      "------------------------------------------\n",
      "L1 loss: 0.07428444921970367\n",
      "Training batch 40 with loss 0.27340\n",
      "------------------------------------------\n",
      "L1 loss: 0.058591313660144806\n",
      "Training batch 41 with loss 0.28841\n",
      "------------------------------------------\n",
      "L1 loss: 0.08293633908033371\n",
      "Training batch 42 with loss 0.28624\n",
      "------------------------------------------\n",
      "L1 loss: 0.06719321012496948\n",
      "Training batch 43 with loss 0.27070\n",
      "------------------------------------------\n",
      "L1 loss: 0.05542702227830887\n",
      "Training batch 44 with loss 0.25874\n",
      "------------------------------------------\n",
      "L1 loss: 0.09209449589252472\n",
      "Training batch 45 with loss 0.35797\n",
      "------------------------------------------\n",
      "L1 loss: 0.07348141074180603\n",
      "Training batch 46 with loss 0.26942\n",
      "------------------------------------------\n",
      "L1 loss: 0.11159420013427734\n",
      "Training batch 47 with loss 0.38445\n",
      "------------------------------------------\n",
      "L1 loss: 0.04524492844939232\n",
      "Training batch 48 with loss 0.23425\n",
      "------------------------------------------\n",
      "L1 loss: 0.0705299898982048\n",
      "Training batch 49 with loss 0.60072\n",
      "------------------------------------------\n",
      "L1 loss: 0.07333269715309143\n",
      "Training batch 50 with loss 0.29437\n",
      "------------------------------------------\n",
      "L1 loss: 0.0799182578921318\n",
      "Training batch 51 with loss 0.33102\n",
      "------------------------------------------\n",
      "L1 loss: 0.08999879658222198\n",
      "Training batch 52 with loss 0.29537\n",
      "------------------------------------------\n",
      "L1 loss: 0.07916060090065002\n",
      "Training batch 53 with loss 0.28925\n",
      "------------------------------------------\n",
      "L1 loss: 0.0765162780880928\n",
      "Training batch 54 with loss 0.27601\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06366751343011856\n",
      "Training batch 55 with loss 0.29968\n",
      "------------------------------------------\n",
      "L1 loss: 0.04528380185365677\n",
      "Training batch 56 with loss 0.33020\n",
      "------------------------------------------\n",
      "L1 loss: 0.06877637654542923\n",
      "Training batch 57 with loss 0.27636\n",
      "------------------------------------------\n",
      "L1 loss: 0.06482433527708054\n",
      "Training batch 58 with loss 0.25730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07942631840705872\n",
      "Training batch 59 with loss 0.29841\n",
      "------------------------------------------\n",
      "L1 loss: 0.06321612000465393\n",
      "Training batch 60 with loss 0.26601\n",
      "------------------------------------------\n",
      "L1 loss: 0.09213376045227051\n",
      "Training batch 61 with loss 0.33022\n",
      "------------------------------------------\n",
      "L1 loss: 0.056097958236932755\n",
      "Training batch 62 with loss 0.25775\n",
      "------------------------------------------\n",
      "L1 loss: 0.062396787106990814\n",
      "Training batch 63 with loss 0.25395\n",
      "------------------------------------------\n",
      "L1 loss: 0.06867461651563644\n",
      "Training batch 64 with loss 0.27940\n",
      "------------------------------------------\n",
      "L1 loss: 0.06268336623907089\n",
      "Training batch 65 with loss 0.24409\n",
      "------------------------------------------\n",
      "L1 loss: 0.06454416364431381\n",
      "Training batch 66 with loss 0.26115\n",
      "------------------------------------------\n",
      "L1 loss: 0.06647584587335587\n",
      "Training batch 67 with loss 0.26483\n",
      "------------------------------------------\n",
      "L1 loss: 0.07289630174636841\n",
      "Training batch 68 with loss 0.31787\n",
      "------------------------------------------\n",
      "L1 loss: 0.04857297241687775\n",
      "Training batch 69 with loss 0.25143\n",
      "------------------------------------------\n",
      "L1 loss: 0.08015561103820801\n",
      "Training batch 70 with loss 0.32559\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709133967757225\n",
      "Training batch 71 with loss 0.31062\n",
      "------------------------------------------\n",
      "L1 loss: 0.08459864556789398\n",
      "Training batch 72 with loss 0.30629\n",
      "------------------------------------------\n",
      "L1 loss: 0.07403957098722458\n",
      "Training batch 73 with loss 0.29785\n",
      "------------------------------------------\n",
      "L1 loss: 0.09418877959251404\n",
      "Training batch 74 with loss 0.31176\n",
      "------------------------------------------\n",
      "L1 loss: 0.07329698652029037\n",
      "Training batch 75 with loss 0.29463\n",
      "------------------------------------------\n",
      "L1 loss: 0.08642292767763138\n",
      "Training batch 76 with loss 0.30071\n",
      "------------------------------------------\n",
      "L1 loss: 0.07931207120418549\n",
      "Training batch 77 with loss 0.26809\n",
      "------------------------------------------\n",
      "L1 loss: 0.08158591389656067\n",
      "Training batch 78 with loss 0.29763\n",
      "------------------------------------------\n",
      "L1 loss: 0.10802435874938965\n",
      "Training batch 79 with loss 0.38818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07736261188983917\n",
      "Training batch 80 with loss 0.27735\n",
      "------------------------------------------\n",
      "L1 loss: 0.08254899084568024\n",
      "Training batch 81 with loss 0.30042\n",
      "------------------------------------------\n",
      "L1 loss: 0.08729370683431625\n",
      "Training batch 82 with loss 0.33852\n",
      "------------------------------------------\n",
      "L1 loss: 0.07740911096334457\n",
      "Training batch 83 with loss 0.30232\n",
      "------------------------------------------\n",
      "L1 loss: 0.06963008642196655\n",
      "Training batch 84 with loss 0.29301\n",
      "------------------------------------------\n",
      "L1 loss: 0.058646634221076965\n",
      "Training batch 85 with loss 0.27481\n",
      "------------------------------------------\n",
      "L1 loss: 0.08174627274274826\n",
      "Training batch 86 with loss 0.29662\n",
      "------------------------------------------\n",
      "L1 loss: 0.10094276815652847\n",
      "Training batch 87 with loss 0.41076\n",
      "------------------------------------------\n",
      "L1 loss: 0.07367406785488129\n",
      "Training batch 88 with loss 0.30468\n",
      "------------------------------------------\n",
      "L1 loss: 0.07061103731393814\n",
      "Training batch 89 with loss 0.26749\n",
      "------------------------------------------\n",
      "L1 loss: 0.07323265820741653\n",
      "Training batch 90 with loss 0.27948\n",
      "------------------------------------------\n",
      "L1 loss: 0.06403163820505142\n",
      "Training batch 91 with loss 0.26728\n",
      "------------------------------------------\n",
      "L1 loss: 0.0908171609044075\n",
      "Training batch 92 with loss 0.29002\n",
      "------------------------------------------\n",
      "L1 loss: 0.06812801957130432\n",
      "Training batch 93 with loss 0.25024\n",
      "------------------------------------------\n",
      "L1 loss: 0.08424530178308487\n",
      "Training batch 94 with loss 0.31056\n",
      "------------------------------------------\n",
      "L1 loss: 0.07266078144311905\n",
      "Training batch 95 with loss 0.25946\n",
      "------------------------------------------\n",
      "L1 loss: 0.05978355556726456\n",
      "Training batch 96 with loss 0.26929\n",
      "------------------------------------------\n",
      "L1 loss: 0.0851164236664772\n",
      "Training batch 97 with loss 0.36172\n",
      "------------------------------------------\n",
      "L1 loss: 0.056280650198459625\n",
      "Training batch 98 with loss 0.23949\n",
      "------------------------------------------\n",
      "L1 loss: 0.06507674604654312\n",
      "Training batch 99 with loss 0.26595\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2968554292619228\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4827\n",
      "------------------------------------------\n",
      "L1 loss: 0.06222277879714966\n",
      "Training batch 0 with loss 0.26082\n",
      "------------------------------------------\n",
      "L1 loss: 0.07987820357084274\n",
      "Training batch 1 with loss 0.28811\n",
      "------------------------------------------\n",
      "L1 loss: 0.09298968315124512\n",
      "Training batch 2 with loss 0.31983\n",
      "------------------------------------------\n",
      "L1 loss: 0.06639112532138824\n",
      "Training batch 3 with loss 0.24544\n",
      "------------------------------------------\n",
      "L1 loss: 0.08265481144189835\n",
      "Training batch 4 with loss 0.32724\n",
      "------------------------------------------\n",
      "L1 loss: 0.06585764139890671\n",
      "Training batch 5 with loss 0.29787\n",
      "------------------------------------------\n",
      "L1 loss: 0.08136396110057831\n",
      "Training batch 6 with loss 0.27710\n",
      "------------------------------------------\n",
      "L1 loss: 0.08243483304977417\n",
      "Training batch 7 with loss 0.32712\n",
      "------------------------------------------\n",
      "L1 loss: 0.08254139870405197\n",
      "Training batch 8 with loss 0.35265\n",
      "------------------------------------------\n",
      "L1 loss: 0.05754433944821358\n",
      "Training batch 9 with loss 0.24668\n",
      "------------------------------------------\n",
      "L1 loss: 0.08144122362136841\n",
      "Training batch 10 with loss 0.29467\n",
      "------------------------------------------\n",
      "L1 loss: 0.060972925275564194\n",
      "Training batch 11 with loss 0.27982\n",
      "------------------------------------------\n",
      "L1 loss: 0.07732723653316498\n",
      "Training batch 12 with loss 0.32346\n",
      "------------------------------------------\n",
      "L1 loss: 0.07008044421672821\n",
      "Training batch 13 with loss 0.25658\n",
      "------------------------------------------\n",
      "L1 loss: 0.05370333418250084\n",
      "Training batch 14 with loss 0.26349\n",
      "------------------------------------------\n",
      "L1 loss: 0.08407825231552124\n",
      "Training batch 15 with loss 0.28002\n",
      "------------------------------------------\n",
      "L1 loss: 0.06219229847192764\n",
      "Training batch 16 with loss 0.26797\n",
      "------------------------------------------\n",
      "L1 loss: 0.08675725758075714\n",
      "Training batch 17 with loss 0.34238\n",
      "------------------------------------------\n",
      "L1 loss: 0.08053188025951385\n",
      "Training batch 18 with loss 0.31152\n",
      "------------------------------------------\n",
      "L1 loss: 0.06325589865446091\n",
      "Training batch 19 with loss 0.25126\n",
      "------------------------------------------\n",
      "L1 loss: 0.07330987602472305\n",
      "Training batch 20 with loss 0.31904\n",
      "------------------------------------------\n",
      "L1 loss: 0.07208006829023361\n",
      "Training batch 21 with loss 0.30184\n",
      "------------------------------------------\n",
      "L1 loss: 0.06816118210554123\n",
      "Training batch 22 with loss 0.27071\n",
      "------------------------------------------\n",
      "L1 loss: 0.08738592267036438\n",
      "Training batch 23 with loss 0.36623\n",
      "------------------------------------------\n",
      "L1 loss: 0.06895751506090164\n",
      "Training batch 24 with loss 0.30009\n",
      "------------------------------------------\n",
      "L1 loss: 0.0769752562046051\n",
      "Training batch 25 with loss 0.31555\n",
      "------------------------------------------\n",
      "L1 loss: 0.07488184422254562\n",
      "Training batch 26 with loss 0.26937\n",
      "------------------------------------------\n",
      "L1 loss: 0.057549405843019485\n",
      "Training batch 27 with loss 0.28866\n",
      "------------------------------------------\n",
      "L1 loss: 0.05959022417664528\n",
      "Training batch 28 with loss 0.30479\n",
      "------------------------------------------\n",
      "L1 loss: 0.060058970004320145\n",
      "Training batch 29 with loss 0.24505\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06015726551413536\n",
      "Training batch 30 with loss 0.26671\n",
      "------------------------------------------\n",
      "L1 loss: 0.06431851536035538\n",
      "Training batch 31 with loss 0.29634\n",
      "------------------------------------------\n",
      "L1 loss: 0.06900414079427719\n",
      "Training batch 32 with loss 0.27869\n",
      "------------------------------------------\n",
      "L1 loss: 0.06595045328140259\n",
      "Training batch 33 with loss 0.25776\n",
      "------------------------------------------\n",
      "L1 loss: 0.0767015740275383\n",
      "Training batch 34 with loss 0.30341\n",
      "------------------------------------------\n",
      "L1 loss: 0.09415886551141739\n",
      "Training batch 35 with loss 0.32609\n",
      "------------------------------------------\n",
      "L1 loss: 0.08531557768583298\n",
      "Training batch 36 with loss 0.26946\n",
      "------------------------------------------\n",
      "L1 loss: 0.06683988869190216\n",
      "Training batch 37 with loss 0.25259\n",
      "------------------------------------------\n",
      "L1 loss: 0.06200836971402168\n",
      "Training batch 38 with loss 0.22174\n",
      "------------------------------------------\n",
      "L1 loss: 0.06737774610519409\n",
      "Training batch 39 with loss 0.30106\n",
      "------------------------------------------\n",
      "L1 loss: 0.06888557225465775\n",
      "Training batch 40 with loss 0.24989\n",
      "------------------------------------------\n",
      "L1 loss: 0.061149705201387405\n",
      "Training batch 41 with loss 0.30209\n",
      "------------------------------------------\n",
      "L1 loss: 0.08020148426294327\n",
      "Training batch 42 with loss 0.29990\n",
      "------------------------------------------\n",
      "L1 loss: 0.06886488199234009\n",
      "Training batch 43 with loss 0.27216\n",
      "------------------------------------------\n",
      "L1 loss: 0.054319798946380615\n",
      "Training batch 44 with loss 0.26530\n",
      "------------------------------------------\n",
      "L1 loss: 0.09323716908693314\n",
      "Training batch 45 with loss 0.36619\n",
      "------------------------------------------\n",
      "L1 loss: 0.07385341078042984\n",
      "Training batch 46 with loss 0.27402\n",
      "------------------------------------------\n",
      "L1 loss: 0.11157959699630737\n",
      "Training batch 47 with loss 0.37248\n",
      "------------------------------------------\n",
      "L1 loss: 0.04703456908464432\n",
      "Training batch 48 with loss 0.23585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07816807180643082\n",
      "Training batch 49 with loss 0.35950\n",
      "------------------------------------------\n",
      "L1 loss: 0.07206036895513535\n",
      "Training batch 50 with loss 0.28513\n",
      "------------------------------------------\n",
      "L1 loss: 0.0841779112815857\n",
      "Training batch 51 with loss 0.34258\n",
      "------------------------------------------\n",
      "L1 loss: 0.09024012833833694\n",
      "Training batch 52 with loss 0.30972\n",
      "------------------------------------------\n",
      "L1 loss: 0.08090762794017792\n",
      "Training batch 53 with loss 0.28708\n",
      "------------------------------------------\n",
      "L1 loss: 0.07688451558351517\n",
      "Training batch 54 with loss 0.28441\n",
      "------------------------------------------\n",
      "L1 loss: 0.061225298792123795\n",
      "Training batch 55 with loss 0.31431\n",
      "------------------------------------------\n",
      "L1 loss: 0.05976754426956177\n",
      "Training batch 56 with loss 0.27091\n",
      "------------------------------------------\n",
      "L1 loss: 0.06793196499347687\n",
      "Training batch 57 with loss 0.27157\n",
      "------------------------------------------\n",
      "L1 loss: 0.06843804568052292\n",
      "Training batch 58 with loss 0.26402\n",
      "------------------------------------------\n",
      "L1 loss: 0.07773973047733307\n",
      "Training batch 59 with loss 0.30520\n",
      "------------------------------------------\n",
      "L1 loss: 0.05779826268553734\n",
      "Training batch 60 with loss 0.38958\n",
      "------------------------------------------\n",
      "L1 loss: 0.09059986472129822\n",
      "Training batch 61 with loss 0.33980\n",
      "------------------------------------------\n",
      "L1 loss: 0.055789560079574585\n",
      "Training batch 62 with loss 0.25092\n",
      "------------------------------------------\n",
      "L1 loss: 0.0606202557682991\n",
      "Training batch 63 with loss 0.26028\n",
      "------------------------------------------\n",
      "L1 loss: 0.07142405956983566\n",
      "Training batch 64 with loss 0.27731\n",
      "------------------------------------------\n",
      "L1 loss: 0.06197422742843628\n",
      "Training batch 65 with loss 0.23431\n",
      "------------------------------------------\n",
      "L1 loss: 0.06732115149497986\n",
      "Training batch 66 with loss 0.28146\n",
      "------------------------------------------\n",
      "L1 loss: 0.06639916449785233\n",
      "Training batch 67 with loss 0.27362\n",
      "------------------------------------------\n",
      "L1 loss: 0.07237599045038223\n",
      "Training batch 68 with loss 0.31513\n",
      "------------------------------------------\n",
      "L1 loss: 0.05054827034473419\n",
      "Training batch 69 with loss 0.25069\n",
      "------------------------------------------\n",
      "L1 loss: 0.07741133123636246\n",
      "Training batch 70 with loss 0.31365\n",
      "------------------------------------------\n",
      "L1 loss: 0.07059292495250702\n",
      "Training batch 71 with loss 0.29830\n",
      "------------------------------------------\n",
      "L1 loss: 0.08271446824073792\n",
      "Training batch 72 with loss 0.30717\n",
      "------------------------------------------\n",
      "L1 loss: 0.06977948546409607\n",
      "Training batch 73 with loss 0.27892\n",
      "------------------------------------------\n",
      "L1 loss: 0.09803397953510284\n",
      "Training batch 74 with loss 0.30882\n",
      "------------------------------------------\n",
      "L1 loss: 0.07315134257078171\n",
      "Training batch 75 with loss 0.28407\n",
      "------------------------------------------\n",
      "L1 loss: 0.08544496446847916\n",
      "Training batch 76 with loss 0.31617\n",
      "------------------------------------------\n",
      "L1 loss: 0.07602978497743607\n",
      "Training batch 77 with loss 0.26442\n",
      "------------------------------------------\n",
      "L1 loss: 0.07768867909908295\n",
      "Training batch 78 with loss 0.28609\n",
      "------------------------------------------\n",
      "L1 loss: 0.1088285893201828\n",
      "Training batch 79 with loss 0.37664\n",
      "------------------------------------------\n",
      "L1 loss: 0.07675598561763763\n",
      "Training batch 80 with loss 0.30010\n",
      "------------------------------------------\n",
      "L1 loss: 0.07703536003828049\n",
      "Training batch 81 with loss 0.28121\n",
      "------------------------------------------\n",
      "L1 loss: 0.08117908239364624\n",
      "Training batch 82 with loss 0.35223\n",
      "------------------------------------------\n",
      "L1 loss: 0.07631014287471771\n",
      "Training batch 83 with loss 0.27941\n",
      "------------------------------------------\n",
      "L1 loss: 0.06591106951236725\n",
      "Training batch 84 with loss 0.29138\n",
      "------------------------------------------\n",
      "L1 loss: 0.06597054749727249\n",
      "Training batch 85 with loss 0.28675\n",
      "------------------------------------------\n",
      "L1 loss: 0.07728534191846848\n",
      "Training batch 86 with loss 0.28406\n",
      "------------------------------------------\n",
      "L1 loss: 0.0833674743771553\n",
      "Training batch 87 with loss 0.58112\n",
      "------------------------------------------\n",
      "L1 loss: 0.07904564589262009\n",
      "Training batch 88 with loss 0.32676\n",
      "------------------------------------------\n",
      "L1 loss: 0.06852088123559952\n",
      "Training batch 89 with loss 0.26329\n",
      "------------------------------------------\n",
      "L1 loss: 0.06951043754816055\n",
      "Training batch 90 with loss 0.27481\n",
      "------------------------------------------\n",
      "L1 loss: 0.054352909326553345\n",
      "Training batch 91 with loss 0.32292\n",
      "------------------------------------------\n",
      "L1 loss: 0.09590089321136475\n",
      "Training batch 92 with loss 0.31414\n",
      "------------------------------------------\n",
      "L1 loss: 0.06729365140199661\n",
      "Training batch 93 with loss 0.25997\n",
      "------------------------------------------\n",
      "L1 loss: 0.08534502983093262\n",
      "Training batch 94 with loss 0.29893\n",
      "------------------------------------------\n",
      "L1 loss: 0.0699857771396637\n",
      "Training batch 95 with loss 0.26124\n",
      "------------------------------------------\n",
      "L1 loss: 0.05927567183971405\n",
      "Training batch 96 with loss 0.26931\n",
      "------------------------------------------\n",
      "L1 loss: 0.08258375525474548\n",
      "Training batch 97 with loss 0.36798\n",
      "------------------------------------------\n",
      "L1 loss: 0.05838356167078018\n",
      "Training batch 98 with loss 0.24683\n",
      "------------------------------------------\n",
      "L1 loss: 0.06243453547358513\n",
      "Training batch 99 with loss 0.27359\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29544869661331175\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4828\n",
      "------------------------------------------\n",
      "L1 loss: 0.05822577700018883\n",
      "Training batch 0 with loss 0.24971\n",
      "------------------------------------------\n",
      "L1 loss: 0.08269890397787094\n",
      "Training batch 1 with loss 0.28700\n",
      "------------------------------------------\n",
      "L1 loss: 0.09511458873748779\n",
      "Training batch 2 with loss 0.32806\n",
      "------------------------------------------\n",
      "L1 loss: 0.07585431635379791\n",
      "Training batch 3 with loss 0.27076\n",
      "------------------------------------------\n",
      "L1 loss: 0.0830453410744667\n",
      "Training batch 4 with loss 0.33770\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07054885476827621\n",
      "Training batch 5 with loss 0.33839\n",
      "------------------------------------------\n",
      "L1 loss: 0.07941434532403946\n",
      "Training batch 6 with loss 0.26994\n",
      "------------------------------------------\n",
      "L1 loss: 0.0836077481508255\n",
      "Training batch 7 with loss 0.33923\n",
      "------------------------------------------\n",
      "L1 loss: 0.07337065041065216\n",
      "Training batch 8 with loss 0.29683\n",
      "------------------------------------------\n",
      "L1 loss: 0.05863519012928009\n",
      "Training batch 9 with loss 0.26297\n",
      "------------------------------------------\n",
      "L1 loss: 0.08271215856075287\n",
      "Training batch 10 with loss 0.30116\n",
      "------------------------------------------\n",
      "L1 loss: 0.05602845549583435\n",
      "Training batch 11 with loss 0.30269\n",
      "------------------------------------------\n",
      "L1 loss: 0.0738041028380394\n",
      "Training batch 12 with loss 0.31414\n",
      "------------------------------------------\n",
      "L1 loss: 0.07052469253540039\n",
      "Training batch 13 with loss 0.24103\n",
      "------------------------------------------\n",
      "L1 loss: 0.05344264581799507\n",
      "Training batch 14 with loss 0.25479\n",
      "------------------------------------------\n",
      "L1 loss: 0.08597234636545181\n",
      "Training batch 15 with loss 0.27534\n",
      "------------------------------------------\n",
      "L1 loss: 0.06332333385944366\n",
      "Training batch 16 with loss 0.26535\n",
      "------------------------------------------\n",
      "L1 loss: 0.09131809324026108\n",
      "Training batch 17 with loss 0.35906\n",
      "------------------------------------------\n",
      "L1 loss: 0.08027235418558121\n",
      "Training batch 18 with loss 0.30401\n",
      "------------------------------------------\n",
      "L1 loss: 0.06302390992641449\n",
      "Training batch 19 with loss 0.23606\n",
      "------------------------------------------\n",
      "L1 loss: 0.07339615374803543\n",
      "Training batch 20 with loss 0.32730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07038598507642746\n",
      "Training batch 21 with loss 0.27888\n",
      "------------------------------------------\n",
      "L1 loss: 0.0691695287823677\n",
      "Training batch 22 with loss 0.28291\n",
      "------------------------------------------\n",
      "L1 loss: 0.08670854568481445\n",
      "Training batch 23 with loss 0.36614\n",
      "------------------------------------------\n",
      "L1 loss: 0.06726394593715668\n",
      "Training batch 24 with loss 0.29457\n",
      "------------------------------------------\n",
      "L1 loss: 0.07615800946950912\n",
      "Training batch 25 with loss 0.29278\n",
      "------------------------------------------\n",
      "L1 loss: 0.07474621385335922\n",
      "Training batch 26 with loss 0.28189\n",
      "------------------------------------------\n",
      "L1 loss: 0.05432836338877678\n",
      "Training batch 27 with loss 0.27982\n",
      "------------------------------------------\n",
      "L1 loss: 0.057263609021902084\n",
      "Training batch 28 with loss 0.28319\n",
      "------------------------------------------\n",
      "L1 loss: 0.06345019489526749\n",
      "Training batch 29 with loss 0.27584\n",
      "------------------------------------------\n",
      "L1 loss: 0.059905555099248886\n",
      "Training batch 30 with loss 0.27043\n",
      "------------------------------------------\n",
      "L1 loss: 0.062234096229076385\n",
      "Training batch 31 with loss 0.29421\n",
      "------------------------------------------\n",
      "L1 loss: 0.06995397806167603\n",
      "Training batch 32 with loss 0.31233\n",
      "------------------------------------------\n",
      "L1 loss: 0.06479896605014801\n",
      "Training batch 33 with loss 0.26791\n",
      "------------------------------------------\n",
      "L1 loss: 0.07413376122713089\n",
      "Training batch 34 with loss 0.31546\n",
      "------------------------------------------\n",
      "L1 loss: 0.09225717186927795\n",
      "Training batch 35 with loss 0.33970\n",
      "------------------------------------------\n",
      "L1 loss: 0.08492833375930786\n",
      "Training batch 36 with loss 0.27260\n",
      "------------------------------------------\n",
      "L1 loss: 0.06745699048042297\n",
      "Training batch 37 with loss 0.26215\n",
      "------------------------------------------\n",
      "L1 loss: 0.06285857409238815\n",
      "Training batch 38 with loss 0.22628\n",
      "------------------------------------------\n",
      "L1 loss: 0.06445496529340744\n",
      "Training batch 39 with loss 0.30098\n",
      "------------------------------------------\n",
      "L1 loss: 0.07330896705389023\n",
      "Training batch 40 with loss 0.26242\n",
      "------------------------------------------\n",
      "L1 loss: 0.05971206724643707\n",
      "Training batch 41 with loss 0.28054\n",
      "------------------------------------------\n",
      "L1 loss: 0.07577773183584213\n",
      "Training batch 42 with loss 0.28931\n",
      "------------------------------------------\n",
      "L1 loss: 0.06956013292074203\n",
      "Training batch 43 with loss 0.27918\n",
      "------------------------------------------\n",
      "L1 loss: 0.05457229167222977\n",
      "Training batch 44 with loss 0.25908\n",
      "------------------------------------------\n",
      "L1 loss: 0.09315352886915207\n",
      "Training batch 45 with loss 0.35090\n",
      "------------------------------------------\n",
      "L1 loss: 0.07147905975580215\n",
      "Training batch 46 with loss 0.26864\n",
      "------------------------------------------\n",
      "L1 loss: 0.11206455528736115\n",
      "Training batch 47 with loss 0.37581\n",
      "------------------------------------------\n",
      "L1 loss: 0.0444960817694664\n",
      "Training batch 48 with loss 0.23626\n",
      "------------------------------------------\n",
      "L1 loss: 0.08244477212429047\n",
      "Training batch 49 with loss 0.36498\n",
      "------------------------------------------\n",
      "L1 loss: 0.07199467718601227\n",
      "Training batch 50 with loss 0.28776\n",
      "------------------------------------------\n",
      "L1 loss: 0.0829465463757515\n",
      "Training batch 51 with loss 0.33622\n",
      "------------------------------------------\n",
      "L1 loss: 0.09062401205301285\n",
      "Training batch 52 with loss 0.28257\n",
      "------------------------------------------\n",
      "L1 loss: 0.07874990254640579\n",
      "Training batch 53 with loss 0.29341\n",
      "------------------------------------------\n",
      "L1 loss: 0.07199832051992416\n",
      "Training batch 54 with loss 0.26682\n",
      "------------------------------------------\n",
      "L1 loss: 0.05794026702642441\n",
      "Training batch 55 with loss 0.32248\n",
      "------------------------------------------\n",
      "L1 loss: 0.05346500128507614\n",
      "Training batch 56 with loss 0.24565\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937181949615479\n",
      "Training batch 57 with loss 0.27620\n",
      "------------------------------------------\n",
      "L1 loss: 0.05725531280040741\n",
      "Training batch 58 with loss 0.28091\n",
      "------------------------------------------\n",
      "L1 loss: 0.07927623391151428\n",
      "Training batch 59 with loss 0.28269\n",
      "------------------------------------------\n",
      "L1 loss: 0.06428119540214539\n",
      "Training batch 60 with loss 0.28061\n",
      "------------------------------------------\n",
      "L1 loss: 0.09937338531017303\n",
      "Training batch 61 with loss 0.36271\n",
      "------------------------------------------\n",
      "L1 loss: 0.05657560005784035\n",
      "Training batch 62 with loss 0.24936\n",
      "------------------------------------------\n",
      "L1 loss: 0.05739409849047661\n",
      "Training batch 63 with loss 0.26351\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937761604785919\n",
      "Training batch 64 with loss 0.27294\n",
      "------------------------------------------\n",
      "L1 loss: 0.06004716455936432\n",
      "Training batch 65 with loss 0.22021\n",
      "------------------------------------------\n",
      "L1 loss: 0.06683319061994553\n",
      "Training batch 66 with loss 0.28009\n",
      "------------------------------------------\n",
      "L1 loss: 0.065608911216259\n",
      "Training batch 67 with loss 0.26877\n",
      "------------------------------------------\n",
      "L1 loss: 0.07185550034046173\n",
      "Training batch 68 with loss 0.30130\n",
      "------------------------------------------\n",
      "L1 loss: 0.051531773060560226\n",
      "Training batch 69 with loss 0.25649\n",
      "------------------------------------------\n",
      "L1 loss: 0.0831882581114769\n",
      "Training batch 70 with loss 0.32771\n",
      "------------------------------------------\n",
      "L1 loss: 0.07017206400632858\n",
      "Training batch 71 with loss 0.32255\n",
      "------------------------------------------\n",
      "L1 loss: 0.08334140479564667\n",
      "Training batch 72 with loss 0.29919\n",
      "------------------------------------------\n",
      "L1 loss: 0.06553308665752411\n",
      "Training batch 73 with loss 0.48942\n",
      "------------------------------------------\n",
      "L1 loss: 0.09541149437427521\n",
      "Training batch 74 with loss 0.31070\n",
      "------------------------------------------\n",
      "L1 loss: 0.0750860646367073\n",
      "Training batch 75 with loss 0.29158\n",
      "------------------------------------------\n",
      "L1 loss: 0.08655896037817001\n",
      "Training batch 76 with loss 0.29921\n",
      "------------------------------------------\n",
      "L1 loss: 0.0797230675816536\n",
      "Training batch 77 with loss 0.26691\n",
      "------------------------------------------\n",
      "L1 loss: 0.08152050524950027\n",
      "Training batch 78 with loss 0.31793\n",
      "------------------------------------------\n",
      "L1 loss: 0.10507601499557495\n",
      "Training batch 79 with loss 0.36891\n",
      "------------------------------------------\n",
      "L1 loss: 0.07694172859191895\n",
      "Training batch 80 with loss 0.27778\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08336472511291504\n",
      "Training batch 81 with loss 0.31318\n",
      "------------------------------------------\n",
      "L1 loss: 0.0854247510433197\n",
      "Training batch 82 with loss 0.36745\n",
      "------------------------------------------\n",
      "L1 loss: 0.07521243393421173\n",
      "Training batch 83 with loss 0.27378\n",
      "------------------------------------------\n",
      "L1 loss: 0.06918299943208694\n",
      "Training batch 84 with loss 0.31355\n",
      "------------------------------------------\n",
      "L1 loss: 0.0641559585928917\n",
      "Training batch 85 with loss 0.27856\n",
      "------------------------------------------\n",
      "L1 loss: 0.08178877830505371\n",
      "Training batch 86 with loss 0.29630\n",
      "------------------------------------------\n",
      "L1 loss: 0.10162680596113205\n",
      "Training batch 87 with loss 0.40722\n",
      "------------------------------------------\n",
      "L1 loss: 0.07571811974048615\n",
      "Training batch 88 with loss 0.31425\n",
      "------------------------------------------\n",
      "L1 loss: 0.06648220121860504\n",
      "Training batch 89 with loss 0.26729\n",
      "------------------------------------------\n",
      "L1 loss: 0.0761583149433136\n",
      "Training batch 90 with loss 0.28501\n",
      "------------------------------------------\n",
      "L1 loss: 0.06202756613492966\n",
      "Training batch 91 with loss 0.25789\n",
      "------------------------------------------\n",
      "L1 loss: 0.09236786514520645\n",
      "Training batch 92 with loss 0.29846\n",
      "------------------------------------------\n",
      "L1 loss: 0.062251199036836624\n",
      "Training batch 93 with loss 0.25252\n",
      "------------------------------------------\n",
      "L1 loss: 0.08466791361570358\n",
      "Training batch 94 with loss 0.30549\n",
      "------------------------------------------\n",
      "L1 loss: 0.07077410817146301\n",
      "Training batch 95 with loss 0.25958\n",
      "------------------------------------------\n",
      "L1 loss: 0.063789002597332\n",
      "Training batch 96 with loss 0.29399\n",
      "------------------------------------------\n",
      "L1 loss: 0.08200617879629135\n",
      "Training batch 97 with loss 0.35027\n",
      "------------------------------------------\n",
      "L1 loss: 0.05609659478068352\n",
      "Training batch 98 with loss 0.23540\n",
      "------------------------------------------\n",
      "L1 loss: 0.06537366658449173\n",
      "Training batch 99 with loss 0.27104\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29470505356788634\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4829\n",
      "------------------------------------------\n",
      "L1 loss: 0.06573028862476349\n",
      "Training batch 0 with loss 0.25323\n",
      "------------------------------------------\n",
      "L1 loss: 0.08384980261325836\n",
      "Training batch 1 with loss 0.27931\n",
      "------------------------------------------\n",
      "L1 loss: 0.06661652028560638\n",
      "Training batch 2 with loss 0.25109\n",
      "------------------------------------------\n",
      "L1 loss: 0.06853584200143814\n",
      "Training batch 3 with loss 0.26033\n",
      "------------------------------------------\n",
      "L1 loss: 0.08534657210111618\n",
      "Training batch 4 with loss 0.33480\n",
      "------------------------------------------\n",
      "L1 loss: 0.06693974137306213\n",
      "Training batch 5 with loss 0.33215\n",
      "------------------------------------------\n",
      "L1 loss: 0.08073649555444717\n",
      "Training batch 6 with loss 0.26848\n",
      "------------------------------------------\n",
      "L1 loss: 0.08173991739749908\n",
      "Training batch 7 with loss 0.32105\n",
      "------------------------------------------\n",
      "L1 loss: 0.08169426769018173\n",
      "Training batch 8 with loss 0.33072\n",
      "------------------------------------------\n",
      "L1 loss: 0.05783229321241379\n",
      "Training batch 9 with loss 0.24328\n",
      "------------------------------------------\n",
      "L1 loss: 0.08309357613325119\n",
      "Training batch 10 with loss 0.28480\n",
      "------------------------------------------\n",
      "L1 loss: 0.058681268244981766\n",
      "Training batch 11 with loss 0.30056\n",
      "------------------------------------------\n",
      "L1 loss: 0.07346425205469131\n",
      "Training batch 12 with loss 0.30611\n",
      "------------------------------------------\n",
      "L1 loss: 0.07021187245845795\n",
      "Training batch 13 with loss 0.26103\n",
      "------------------------------------------\n",
      "L1 loss: 0.05332005396485329\n",
      "Training batch 14 with loss 0.24790\n",
      "------------------------------------------\n",
      "L1 loss: 0.08217491954565048\n",
      "Training batch 15 with loss 0.27122\n",
      "------------------------------------------\n",
      "L1 loss: 0.06281983852386475\n",
      "Training batch 16 with loss 0.26956\n",
      "------------------------------------------\n",
      "L1 loss: 0.0863928571343422\n",
      "Training batch 17 with loss 0.33044\n",
      "------------------------------------------\n",
      "L1 loss: 0.07987112551927567\n",
      "Training batch 18 with loss 0.29436\n",
      "------------------------------------------\n",
      "L1 loss: 0.06077961623668671\n",
      "Training batch 19 with loss 0.22348\n",
      "------------------------------------------\n",
      "L1 loss: 0.07146882265806198\n",
      "Training batch 20 with loss 0.32608\n",
      "------------------------------------------\n",
      "L1 loss: 0.06397536396980286\n",
      "Training batch 21 with loss 0.42250\n",
      "------------------------------------------\n",
      "L1 loss: 0.06815794110298157\n",
      "Training batch 22 with loss 0.27103\n",
      "------------------------------------------\n",
      "L1 loss: 0.08753922581672668\n",
      "Training batch 23 with loss 0.35208\n",
      "------------------------------------------\n",
      "L1 loss: 0.06842251867055893\n",
      "Training batch 24 with loss 0.30469\n",
      "------------------------------------------\n",
      "L1 loss: 0.07694520801305771\n",
      "Training batch 25 with loss 0.29161\n",
      "------------------------------------------\n",
      "L1 loss: 0.07491731643676758\n",
      "Training batch 26 with loss 0.27603\n",
      "------------------------------------------\n",
      "L1 loss: 0.05737525597214699\n",
      "Training batch 27 with loss 0.29911\n",
      "------------------------------------------\n",
      "L1 loss: 0.05662023648619652\n",
      "Training batch 28 with loss 0.28444\n",
      "------------------------------------------\n",
      "L1 loss: 0.06132442504167557\n",
      "Training batch 29 with loss 0.25223\n",
      "------------------------------------------\n",
      "L1 loss: 0.06176342815160751\n",
      "Training batch 30 with loss 0.25964\n",
      "------------------------------------------\n",
      "L1 loss: 0.0638948604464531\n",
      "Training batch 31 with loss 0.29684\n",
      "------------------------------------------\n",
      "L1 loss: 0.06985554844141006\n",
      "Training batch 32 with loss 0.27795\n",
      "------------------------------------------\n",
      "L1 loss: 0.06277301907539368\n",
      "Training batch 33 with loss 0.26048\n",
      "------------------------------------------\n",
      "L1 loss: 0.07212954759597778\n",
      "Training batch 34 with loss 0.29923\n",
      "------------------------------------------\n",
      "L1 loss: 0.09516356140375137\n",
      "Training batch 35 with loss 0.33147\n",
      "------------------------------------------\n",
      "L1 loss: 0.08757361769676208\n",
      "Training batch 36 with loss 0.28790\n",
      "------------------------------------------\n",
      "L1 loss: 0.0697510614991188\n",
      "Training batch 37 with loss 0.27291\n",
      "------------------------------------------\n",
      "L1 loss: 0.06438358128070831\n",
      "Training batch 38 with loss 0.23214\n",
      "------------------------------------------\n",
      "L1 loss: 0.06709569692611694\n",
      "Training batch 39 with loss 0.31443\n",
      "------------------------------------------\n",
      "L1 loss: 0.06997407227754593\n",
      "Training batch 40 with loss 0.26198\n",
      "------------------------------------------\n",
      "L1 loss: 0.058065157383680344\n",
      "Training batch 41 with loss 0.28688\n",
      "------------------------------------------\n",
      "L1 loss: 0.07880163192749023\n",
      "Training batch 42 with loss 0.27703\n",
      "------------------------------------------\n",
      "L1 loss: 0.07173706591129303\n",
      "Training batch 43 with loss 0.28511\n",
      "------------------------------------------\n",
      "L1 loss: 0.05670391395688057\n",
      "Training batch 44 with loss 0.25742\n",
      "------------------------------------------\n",
      "L1 loss: 0.09362069517374039\n",
      "Training batch 45 with loss 0.38942\n",
      "------------------------------------------\n",
      "L1 loss: 0.07472656667232513\n",
      "Training batch 46 with loss 0.26561\n",
      "------------------------------------------\n",
      "L1 loss: 0.1095661073923111\n",
      "Training batch 47 with loss 0.35376\n",
      "------------------------------------------\n",
      "L1 loss: 0.04994218051433563\n",
      "Training batch 48 with loss 0.25945\n",
      "------------------------------------------\n",
      "L1 loss: 0.08251234143972397\n",
      "Training batch 49 with loss 0.35360\n",
      "------------------------------------------\n",
      "L1 loss: 0.07471255958080292\n",
      "Training batch 50 with loss 0.30383\n",
      "------------------------------------------\n",
      "L1 loss: 0.07991114258766174\n",
      "Training batch 51 with loss 0.33772\n",
      "------------------------------------------\n",
      "L1 loss: 0.08893954008817673\n",
      "Training batch 52 with loss 0.29190\n",
      "------------------------------------------\n",
      "L1 loss: 0.07601779699325562\n",
      "Training batch 53 with loss 0.29810\n",
      "------------------------------------------\n",
      "L1 loss: 0.07579775899648666\n",
      "Training batch 54 with loss 0.26012\n",
      "------------------------------------------\n",
      "L1 loss: 0.05851912498474121\n",
      "Training batch 55 with loss 0.28586\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0539088174700737\n",
      "Training batch 56 with loss 0.24777\n",
      "------------------------------------------\n",
      "L1 loss: 0.0655609518289566\n",
      "Training batch 57 with loss 0.26345\n",
      "------------------------------------------\n",
      "L1 loss: 0.06856301426887512\n",
      "Training batch 58 with loss 0.26290\n",
      "------------------------------------------\n",
      "L1 loss: 0.07733645290136337\n",
      "Training batch 59 with loss 0.29732\n",
      "------------------------------------------\n",
      "L1 loss: 0.06666775792837143\n",
      "Training batch 60 with loss 0.26911\n",
      "------------------------------------------\n",
      "L1 loss: 0.0992734357714653\n",
      "Training batch 61 with loss 0.36155\n",
      "------------------------------------------\n",
      "L1 loss: 0.05479668825864792\n",
      "Training batch 62 with loss 0.25077\n",
      "------------------------------------------\n",
      "L1 loss: 0.05894085764884949\n",
      "Training batch 63 with loss 0.26875\n",
      "------------------------------------------\n",
      "L1 loss: 0.07159218192100525\n",
      "Training batch 64 with loss 0.29241\n",
      "------------------------------------------\n",
      "L1 loss: 0.062037281692028046\n",
      "Training batch 65 with loss 0.24401\n",
      "------------------------------------------\n",
      "L1 loss: 0.06629060208797455\n",
      "Training batch 66 with loss 0.27621\n",
      "------------------------------------------\n",
      "L1 loss: 0.06473416835069656\n",
      "Training batch 67 with loss 0.25895\n",
      "------------------------------------------\n",
      "L1 loss: 0.07687191665172577\n",
      "Training batch 68 with loss 0.31415\n",
      "------------------------------------------\n",
      "L1 loss: 0.03720107674598694\n",
      "Training batch 69 with loss 0.33385\n",
      "------------------------------------------\n",
      "L1 loss: 0.08113789558410645\n",
      "Training batch 70 with loss 0.30691\n",
      "------------------------------------------\n",
      "L1 loss: 0.07142366468906403\n",
      "Training batch 71 with loss 0.31005\n",
      "------------------------------------------\n",
      "L1 loss: 0.08786005526781082\n",
      "Training batch 72 with loss 0.31099\n",
      "------------------------------------------\n",
      "L1 loss: 0.06846752762794495\n",
      "Training batch 73 with loss 0.28981\n",
      "------------------------------------------\n",
      "L1 loss: 0.09710751473903656\n",
      "Training batch 74 with loss 0.31139\n",
      "------------------------------------------\n",
      "L1 loss: 0.07400358468294144\n",
      "Training batch 75 with loss 0.28024\n",
      "------------------------------------------\n",
      "L1 loss: 0.0866423025727272\n",
      "Training batch 76 with loss 0.29767\n",
      "------------------------------------------\n",
      "L1 loss: 0.07902749627828598\n",
      "Training batch 77 with loss 0.27097\n",
      "------------------------------------------\n",
      "L1 loss: 0.07813552767038345\n",
      "Training batch 78 with loss 0.29311\n",
      "------------------------------------------\n",
      "L1 loss: 0.1032678633928299\n",
      "Training batch 79 with loss 0.37246\n",
      "------------------------------------------\n",
      "L1 loss: 0.07726108282804489\n",
      "Training batch 80 with loss 0.28300\n",
      "------------------------------------------\n",
      "L1 loss: 0.07084258645772934\n",
      "Training batch 81 with loss 0.29974\n",
      "------------------------------------------\n",
      "L1 loss: 0.08506546914577484\n",
      "Training batch 82 with loss 0.38058\n",
      "------------------------------------------\n",
      "L1 loss: 0.077753446996212\n",
      "Training batch 83 with loss 0.28517\n",
      "------------------------------------------\n",
      "L1 loss: 0.07114607095718384\n",
      "Training batch 84 with loss 0.29898\n",
      "------------------------------------------\n",
      "L1 loss: 0.05909029766917229\n",
      "Training batch 85 with loss 0.27426\n",
      "------------------------------------------\n",
      "L1 loss: 0.08047810196876526\n",
      "Training batch 86 with loss 0.31961\n",
      "------------------------------------------\n",
      "L1 loss: 0.09977570176124573\n",
      "Training batch 87 with loss 0.40598\n",
      "------------------------------------------\n",
      "L1 loss: 0.07737747579813004\n",
      "Training batch 88 with loss 0.31857\n",
      "------------------------------------------\n",
      "L1 loss: 0.0658211037516594\n",
      "Training batch 89 with loss 0.26704\n",
      "------------------------------------------\n",
      "L1 loss: 0.07249143719673157\n",
      "Training batch 90 with loss 0.27489\n",
      "------------------------------------------\n",
      "L1 loss: 0.062189534306526184\n",
      "Training batch 91 with loss 0.27014\n",
      "------------------------------------------\n",
      "L1 loss: 0.09269518405199051\n",
      "Training batch 92 with loss 0.29739\n",
      "------------------------------------------\n",
      "L1 loss: 0.06469342857599258\n",
      "Training batch 93 with loss 0.24817\n",
      "------------------------------------------\n",
      "L1 loss: 0.08523048460483551\n",
      "Training batch 94 with loss 0.30347\n",
      "------------------------------------------\n",
      "L1 loss: 0.07325989007949829\n",
      "Training batch 95 with loss 0.25596\n",
      "------------------------------------------\n",
      "L1 loss: 0.06836866587400436\n",
      "Training batch 96 with loss 0.30370\n",
      "------------------------------------------\n",
      "L1 loss: 0.07403142750263214\n",
      "Training batch 97 with loss 0.35353\n",
      "------------------------------------------\n",
      "L1 loss: 0.05526792258024216\n",
      "Training batch 98 with loss 0.24522\n",
      "------------------------------------------\n",
      "L1 loss: 0.06489063799381256\n",
      "Training batch 99 with loss 0.27275\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2928745126724243\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4830\n",
      "------------------------------------------\n",
      "L1 loss: 0.06185993552207947\n",
      "Training batch 0 with loss 0.23794\n",
      "------------------------------------------\n",
      "L1 loss: 0.0813070610165596\n",
      "Training batch 1 with loss 0.29478\n",
      "------------------------------------------\n",
      "L1 loss: 0.09302033483982086\n",
      "Training batch 2 with loss 0.32148\n",
      "------------------------------------------\n",
      "L1 loss: 0.0717306062579155\n",
      "Training batch 3 with loss 0.25154\n",
      "------------------------------------------\n",
      "L1 loss: 0.08385499566793442\n",
      "Training batch 4 with loss 0.32609\n",
      "------------------------------------------\n",
      "L1 loss: 0.0634613186120987\n",
      "Training batch 5 with loss 0.32100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08078020811080933\n",
      "Training batch 6 with loss 0.27530\n",
      "------------------------------------------\n",
      "L1 loss: 0.0846032053232193\n",
      "Training batch 7 with loss 0.33194\n",
      "------------------------------------------\n",
      "L1 loss: 0.08310448378324509\n",
      "Training batch 8 with loss 0.33874\n",
      "------------------------------------------\n",
      "L1 loss: 0.059781938791275024\n",
      "Training batch 9 with loss 0.25336\n",
      "------------------------------------------\n",
      "L1 loss: 0.07108306139707565\n",
      "Training batch 10 with loss 0.37794\n",
      "------------------------------------------\n",
      "L1 loss: 0.0641312524676323\n",
      "Training batch 11 with loss 0.29284\n",
      "------------------------------------------\n",
      "L1 loss: 0.07422081381082535\n",
      "Training batch 12 with loss 0.30847\n",
      "------------------------------------------\n",
      "L1 loss: 0.06979811191558838\n",
      "Training batch 13 with loss 0.26339\n",
      "------------------------------------------\n",
      "L1 loss: 0.05194205418229103\n",
      "Training batch 14 with loss 0.25752\n",
      "------------------------------------------\n",
      "L1 loss: 0.08510745316743851\n",
      "Training batch 15 with loss 0.27158\n",
      "------------------------------------------\n",
      "L1 loss: 0.06025858223438263\n",
      "Training batch 16 with loss 0.25762\n",
      "------------------------------------------\n",
      "L1 loss: 0.08744056522846222\n",
      "Training batch 17 with loss 0.34827\n",
      "------------------------------------------\n",
      "L1 loss: 0.08142423629760742\n",
      "Training batch 18 with loss 0.30277\n",
      "------------------------------------------\n",
      "L1 loss: 0.06216726452112198\n",
      "Training batch 19 with loss 0.23665\n",
      "------------------------------------------\n",
      "L1 loss: 0.07199906557798386\n",
      "Training batch 20 with loss 0.32925\n",
      "------------------------------------------\n",
      "L1 loss: 0.07064437121152878\n",
      "Training batch 21 with loss 0.27634\n",
      "------------------------------------------\n",
      "L1 loss: 0.06773895025253296\n",
      "Training batch 22 with loss 0.28108\n",
      "------------------------------------------\n",
      "L1 loss: 0.08578669279813766\n",
      "Training batch 23 with loss 0.36497\n",
      "------------------------------------------\n",
      "L1 loss: 0.06917914748191833\n",
      "Training batch 24 with loss 0.29360\n",
      "------------------------------------------\n",
      "L1 loss: 0.0766037330031395\n",
      "Training batch 25 with loss 0.31821\n",
      "------------------------------------------\n",
      "L1 loss: 0.07423041760921478\n",
      "Training batch 26 with loss 0.26644\n",
      "------------------------------------------\n",
      "L1 loss: 0.05742186680436134\n",
      "Training batch 27 with loss 0.28917\n",
      "------------------------------------------\n",
      "L1 loss: 0.05805183947086334\n",
      "Training batch 28 with loss 0.28704\n",
      "------------------------------------------\n",
      "L1 loss: 0.05552220344543457\n",
      "Training batch 29 with loss 0.24067\n",
      "------------------------------------------\n",
      "L1 loss: 0.06189071387052536\n",
      "Training batch 30 with loss 0.27369\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06491675972938538\n",
      "Training batch 31 with loss 0.30033\n",
      "------------------------------------------\n",
      "L1 loss: 0.06961096823215485\n",
      "Training batch 32 with loss 0.27662\n",
      "------------------------------------------\n",
      "L1 loss: 0.06400995701551437\n",
      "Training batch 33 with loss 0.26094\n",
      "------------------------------------------\n",
      "L1 loss: 0.07678169757127762\n",
      "Training batch 34 with loss 0.31911\n",
      "------------------------------------------\n",
      "L1 loss: 0.09567654877901077\n",
      "Training batch 35 with loss 0.33648\n",
      "------------------------------------------\n",
      "L1 loss: 0.08634896576404572\n",
      "Training batch 36 with loss 0.27927\n",
      "------------------------------------------\n",
      "L1 loss: 0.06848637014627457\n",
      "Training batch 37 with loss 0.26617\n",
      "------------------------------------------\n",
      "L1 loss: 0.06297515332698822\n",
      "Training batch 38 with loss 0.22245\n",
      "------------------------------------------\n",
      "L1 loss: 0.05116099864244461\n",
      "Training batch 39 with loss 0.49196\n",
      "------------------------------------------\n",
      "L1 loss: 0.07570471614599228\n",
      "Training batch 40 with loss 0.27829\n",
      "------------------------------------------\n",
      "L1 loss: 0.057141754776239395\n",
      "Training batch 41 with loss 0.29849\n",
      "------------------------------------------\n",
      "L1 loss: 0.07985316216945648\n",
      "Training batch 42 with loss 0.27168\n",
      "------------------------------------------\n",
      "L1 loss: 0.07101862877607346\n",
      "Training batch 43 with loss 0.26080\n",
      "------------------------------------------\n",
      "L1 loss: 0.05747099220752716\n",
      "Training batch 44 with loss 0.26554\n",
      "------------------------------------------\n",
      "L1 loss: 0.0924234539270401\n",
      "Training batch 45 with loss 0.37869\n",
      "------------------------------------------\n",
      "L1 loss: 0.07563606649637222\n",
      "Training batch 46 with loss 0.26585\n",
      "------------------------------------------\n",
      "L1 loss: 0.1097877025604248\n",
      "Training batch 47 with loss 0.35281\n",
      "------------------------------------------\n",
      "L1 loss: 0.045107584446668625\n",
      "Training batch 48 with loss 0.22009\n",
      "------------------------------------------\n",
      "L1 loss: 0.08246051520109177\n",
      "Training batch 49 with loss 0.36166\n",
      "------------------------------------------\n",
      "L1 loss: 0.07381138950586319\n",
      "Training batch 50 with loss 0.30050\n",
      "------------------------------------------\n",
      "L1 loss: 0.07773379981517792\n",
      "Training batch 51 with loss 0.33708\n",
      "------------------------------------------\n",
      "L1 loss: 0.09028035402297974\n",
      "Training batch 52 with loss 0.30127\n",
      "------------------------------------------\n",
      "L1 loss: 0.07859016954898834\n",
      "Training batch 53 with loss 0.28836\n",
      "------------------------------------------\n",
      "L1 loss: 0.07850149273872375\n",
      "Training batch 54 with loss 0.27249\n",
      "------------------------------------------\n",
      "L1 loss: 0.06372477114200592\n",
      "Training batch 55 with loss 0.30672\n",
      "------------------------------------------\n",
      "L1 loss: 0.05189503729343414\n",
      "Training batch 56 with loss 0.24463\n",
      "------------------------------------------\n",
      "L1 loss: 0.0658552497625351\n",
      "Training batch 57 with loss 0.27689\n",
      "------------------------------------------\n",
      "L1 loss: 0.06863552331924438\n",
      "Training batch 58 with loss 0.26192\n",
      "------------------------------------------\n",
      "L1 loss: 0.08121532201766968\n",
      "Training batch 59 with loss 0.28565\n",
      "------------------------------------------\n",
      "L1 loss: 0.06281687319278717\n",
      "Training batch 60 with loss 0.27385\n",
      "------------------------------------------\n",
      "L1 loss: 0.10160861164331436\n",
      "Training batch 61 with loss 0.36078\n",
      "------------------------------------------\n",
      "L1 loss: 0.056600943207740784\n",
      "Training batch 62 with loss 0.25868\n",
      "------------------------------------------\n",
      "L1 loss: 0.06216549873352051\n",
      "Training batch 63 with loss 0.26695\n",
      "------------------------------------------\n",
      "L1 loss: 0.0705665722489357\n",
      "Training batch 64 with loss 0.27392\n",
      "------------------------------------------\n",
      "L1 loss: 0.058908771723508835\n",
      "Training batch 65 with loss 0.22192\n",
      "------------------------------------------\n",
      "L1 loss: 0.06778328865766525\n",
      "Training batch 66 with loss 0.28419\n",
      "------------------------------------------\n",
      "L1 loss: 0.06431706994771957\n",
      "Training batch 67 with loss 0.26411\n",
      "------------------------------------------\n",
      "L1 loss: 0.07301390916109085\n",
      "Training batch 68 with loss 0.30452\n",
      "------------------------------------------\n",
      "L1 loss: 0.0502958819270134\n",
      "Training batch 69 with loss 0.25881\n",
      "------------------------------------------\n",
      "L1 loss: 0.08279917389154434\n",
      "Training batch 70 with loss 0.33056\n",
      "------------------------------------------\n",
      "L1 loss: 0.07301973551511765\n",
      "Training batch 71 with loss 0.29434\n",
      "------------------------------------------\n",
      "L1 loss: 0.08506922423839569\n",
      "Training batch 72 with loss 0.31146\n",
      "------------------------------------------\n",
      "L1 loss: 0.07199020683765411\n",
      "Training batch 73 with loss 0.30558\n",
      "------------------------------------------\n",
      "L1 loss: 0.09568777680397034\n",
      "Training batch 74 with loss 0.31569\n",
      "------------------------------------------\n",
      "L1 loss: 0.07513648271560669\n",
      "Training batch 75 with loss 0.28146\n",
      "------------------------------------------\n",
      "L1 loss: 0.0863712728023529\n",
      "Training batch 76 with loss 0.29737\n",
      "------------------------------------------\n",
      "L1 loss: 0.07937821000814438\n",
      "Training batch 77 with loss 0.26786\n",
      "------------------------------------------\n",
      "L1 loss: 0.08206811547279358\n",
      "Training batch 78 with loss 0.29632\n",
      "------------------------------------------\n",
      "L1 loss: 0.10597014427185059\n",
      "Training batch 79 with loss 0.36442\n",
      "------------------------------------------\n",
      "L1 loss: 0.07665450870990753\n",
      "Training batch 80 with loss 0.27894\n",
      "------------------------------------------\n",
      "L1 loss: 0.06621433049440384\n",
      "Training batch 81 with loss 0.26704\n",
      "------------------------------------------\n",
      "L1 loss: 0.08143634349107742\n",
      "Training batch 82 with loss 0.35572\n",
      "------------------------------------------\n",
      "L1 loss: 0.07543706893920898\n",
      "Training batch 83 with loss 0.27264\n",
      "------------------------------------------\n",
      "L1 loss: 0.07016973197460175\n",
      "Training batch 84 with loss 0.27726\n",
      "------------------------------------------\n",
      "L1 loss: 0.060524359345436096\n",
      "Training batch 85 with loss 0.25794\n",
      "------------------------------------------\n",
      "L1 loss: 0.08340632170438766\n",
      "Training batch 86 with loss 0.28193\n",
      "------------------------------------------\n",
      "L1 loss: 0.10084786266088486\n",
      "Training batch 87 with loss 0.39939\n",
      "------------------------------------------\n",
      "L1 loss: 0.07754096388816833\n",
      "Training batch 88 with loss 0.31714\n",
      "------------------------------------------\n",
      "L1 loss: 0.069346584379673\n",
      "Training batch 89 with loss 0.26212\n",
      "------------------------------------------\n",
      "L1 loss: 0.07498548924922943\n",
      "Training batch 90 with loss 0.28682\n",
      "------------------------------------------\n",
      "L1 loss: 0.06104972958564758\n",
      "Training batch 91 with loss 0.25053\n",
      "------------------------------------------\n",
      "L1 loss: 0.0956491008400917\n",
      "Training batch 92 with loss 0.30766\n",
      "------------------------------------------\n",
      "L1 loss: 0.06816800683736801\n",
      "Training batch 93 with loss 0.25267\n",
      "------------------------------------------\n",
      "L1 loss: 0.08615004271268845\n",
      "Training batch 94 with loss 0.32169\n",
      "------------------------------------------\n",
      "L1 loss: 0.06872133165597916\n",
      "Training batch 95 with loss 0.25274\n",
      "------------------------------------------\n",
      "L1 loss: 0.06084273383021355\n",
      "Training batch 96 with loss 0.26279\n",
      "------------------------------------------\n",
      "L1 loss: 0.08818278461694717\n",
      "Training batch 97 with loss 0.38420\n",
      "------------------------------------------\n",
      "L1 loss: 0.05744470655918121\n",
      "Training batch 98 with loss 0.23291\n",
      "------------------------------------------\n",
      "L1 loss: 0.06698451936244965\n",
      "Training batch 99 with loss 0.27416\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2930148854851723\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4831\n",
      "------------------------------------------\n",
      "L1 loss: 0.06506538391113281\n",
      "Training batch 0 with loss 0.26578\n",
      "------------------------------------------\n",
      "L1 loss: 0.09133632481098175\n",
      "Training batch 1 with loss 0.41727\n",
      "------------------------------------------\n",
      "L1 loss: 0.09237727522850037\n",
      "Training batch 2 with loss 0.31730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07132594287395477\n",
      "Training batch 3 with loss 0.26498\n",
      "------------------------------------------\n",
      "L1 loss: 0.08433564752340317\n",
      "Training batch 4 with loss 0.31882\n",
      "------------------------------------------\n",
      "L1 loss: 0.06660977005958557\n",
      "Training batch 5 with loss 0.31014\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07745017856359482\n",
      "Training batch 6 with loss 0.26947\n",
      "------------------------------------------\n",
      "L1 loss: 0.08382701873779297\n",
      "Training batch 7 with loss 0.32397\n",
      "------------------------------------------\n",
      "L1 loss: 0.08127811551094055\n",
      "Training batch 8 with loss 0.35586\n",
      "------------------------------------------\n",
      "L1 loss: 0.059661634266376495\n",
      "Training batch 9 with loss 0.25337\n",
      "------------------------------------------\n",
      "L1 loss: 0.08272828906774521\n",
      "Training batch 10 with loss 0.29983\n",
      "------------------------------------------\n",
      "L1 loss: 0.05950932204723358\n",
      "Training batch 11 with loss 0.30020\n",
      "------------------------------------------\n",
      "L1 loss: 0.07526186108589172\n",
      "Training batch 12 with loss 0.29910\n",
      "------------------------------------------\n",
      "L1 loss: 0.06862848997116089\n",
      "Training batch 13 with loss 0.24570\n",
      "------------------------------------------\n",
      "L1 loss: 0.05413244292140007\n",
      "Training batch 14 with loss 0.26471\n",
      "------------------------------------------\n",
      "L1 loss: 0.08349405229091644\n",
      "Training batch 15 with loss 0.26113\n",
      "------------------------------------------\n",
      "L1 loss: 0.06186855584383011\n",
      "Training batch 16 with loss 0.26592\n",
      "------------------------------------------\n",
      "L1 loss: 0.08643628656864166\n",
      "Training batch 17 with loss 0.33685\n",
      "------------------------------------------\n",
      "L1 loss: 0.08066725730895996\n",
      "Training batch 18 with loss 0.29017\n",
      "------------------------------------------\n",
      "L1 loss: 0.06338885426521301\n",
      "Training batch 19 with loss 0.23817\n",
      "------------------------------------------\n",
      "L1 loss: 0.07189805805683136\n",
      "Training batch 20 with loss 0.33206\n",
      "------------------------------------------\n",
      "L1 loss: 0.06920059770345688\n",
      "Training batch 21 with loss 0.27348\n",
      "------------------------------------------\n",
      "L1 loss: 0.06744706630706787\n",
      "Training batch 22 with loss 0.28464\n",
      "------------------------------------------\n",
      "L1 loss: 0.08923500031232834\n",
      "Training batch 23 with loss 0.35398\n",
      "------------------------------------------\n",
      "L1 loss: 0.06733055412769318\n",
      "Training batch 24 with loss 0.30273\n",
      "------------------------------------------\n",
      "L1 loss: 0.07392016798257828\n",
      "Training batch 25 with loss 0.29345\n",
      "------------------------------------------\n",
      "L1 loss: 0.07777975499629974\n",
      "Training batch 26 with loss 0.26972\n",
      "------------------------------------------\n",
      "L1 loss: 0.05674581974744797\n",
      "Training batch 27 with loss 0.29444\n",
      "------------------------------------------\n",
      "L1 loss: 0.058419644832611084\n",
      "Training batch 28 with loss 0.27122\n",
      "------------------------------------------\n",
      "L1 loss: 0.05561627447605133\n",
      "Training batch 29 with loss 0.24500\n",
      "------------------------------------------\n",
      "L1 loss: 0.061078764498233795\n",
      "Training batch 30 with loss 0.28043\n",
      "------------------------------------------\n",
      "L1 loss: 0.06643329560756683\n",
      "Training batch 31 with loss 0.30722\n",
      "------------------------------------------\n",
      "L1 loss: 0.06997285038232803\n",
      "Training batch 32 with loss 0.27762\n",
      "------------------------------------------\n",
      "L1 loss: 0.0652606263756752\n",
      "Training batch 33 with loss 0.26457\n",
      "------------------------------------------\n",
      "L1 loss: 0.07707816362380981\n",
      "Training batch 34 with loss 0.33806\n",
      "------------------------------------------\n",
      "L1 loss: 0.09345550090074539\n",
      "Training batch 35 with loss 0.35845\n",
      "------------------------------------------\n",
      "L1 loss: 0.08496168255805969\n",
      "Training batch 36 with loss 0.29521\n",
      "------------------------------------------\n",
      "L1 loss: 0.06859459728002548\n",
      "Training batch 37 with loss 0.26680\n",
      "------------------------------------------\n",
      "L1 loss: 0.0614357590675354\n",
      "Training batch 38 with loss 0.21749\n",
      "------------------------------------------\n",
      "L1 loss: 0.0664045587182045\n",
      "Training batch 39 with loss 0.30036\n",
      "------------------------------------------\n",
      "L1 loss: 0.06695784628391266\n",
      "Training batch 40 with loss 0.24399\n",
      "------------------------------------------\n",
      "L1 loss: 0.0573180690407753\n",
      "Training batch 41 with loss 0.29367\n",
      "------------------------------------------\n",
      "L1 loss: 0.07996142655611038\n",
      "Training batch 42 with loss 0.27178\n",
      "------------------------------------------\n",
      "L1 loss: 0.07228149473667145\n",
      "Training batch 43 with loss 0.27635\n",
      "------------------------------------------\n",
      "L1 loss: 0.057309988886117935\n",
      "Training batch 44 with loss 0.27119\n",
      "------------------------------------------\n",
      "L1 loss: 0.09314735978841782\n",
      "Training batch 45 with loss 0.36384\n",
      "------------------------------------------\n",
      "L1 loss: 0.0746283009648323\n",
      "Training batch 46 with loss 0.26247\n",
      "------------------------------------------\n",
      "L1 loss: 0.10946835577487946\n",
      "Training batch 47 with loss 0.36413\n",
      "------------------------------------------\n",
      "L1 loss: 0.04862665385007858\n",
      "Training batch 48 with loss 0.27421\n",
      "------------------------------------------\n",
      "L1 loss: 0.08058471977710724\n",
      "Training batch 49 with loss 0.35348\n",
      "------------------------------------------\n",
      "L1 loss: 0.0716431513428688\n",
      "Training batch 50 with loss 0.28450\n",
      "------------------------------------------\n",
      "L1 loss: 0.08105815947055817\n",
      "Training batch 51 with loss 0.33645\n",
      "------------------------------------------\n",
      "L1 loss: 0.08998457342386246\n",
      "Training batch 52 with loss 0.28819\n",
      "------------------------------------------\n",
      "L1 loss: 0.0806240513920784\n",
      "Training batch 53 with loss 0.28210\n",
      "------------------------------------------\n",
      "L1 loss: 0.07889259606599808\n",
      "Training batch 54 with loss 0.27318\n",
      "------------------------------------------\n",
      "L1 loss: 0.06309626996517181\n",
      "Training batch 55 with loss 0.31365\n",
      "------------------------------------------\n",
      "L1 loss: 0.05901887267827988\n",
      "Training batch 56 with loss 0.27339\n",
      "------------------------------------------\n",
      "L1 loss: 0.07223749160766602\n",
      "Training batch 57 with loss 0.28382\n",
      "------------------------------------------\n",
      "L1 loss: 0.06479106843471527\n",
      "Training batch 58 with loss 0.25727\n",
      "------------------------------------------\n",
      "L1 loss: 0.08129243552684784\n",
      "Training batch 59 with loss 0.29271\n",
      "------------------------------------------\n",
      "L1 loss: 0.06617916375398636\n",
      "Training batch 60 with loss 0.27213\n",
      "------------------------------------------\n",
      "L1 loss: 0.1027761921286583\n",
      "Training batch 61 with loss 0.36354\n",
      "------------------------------------------\n",
      "L1 loss: 0.057356834411621094\n",
      "Training batch 62 with loss 0.25004\n",
      "------------------------------------------\n",
      "L1 loss: 0.06127872318029404\n",
      "Training batch 63 with loss 0.25458\n",
      "------------------------------------------\n",
      "L1 loss: 0.07188941538333893\n",
      "Training batch 64 with loss 0.27326\n",
      "------------------------------------------\n",
      "L1 loss: 0.05852486565709114\n",
      "Training batch 65 with loss 0.22940\n",
      "------------------------------------------\n",
      "L1 loss: 0.06566648185253143\n",
      "Training batch 66 with loss 0.27876\n",
      "------------------------------------------\n",
      "L1 loss: 0.06454496085643768\n",
      "Training batch 67 with loss 0.27074\n",
      "------------------------------------------\n",
      "L1 loss: 0.07226721942424774\n",
      "Training batch 68 with loss 0.30346\n",
      "------------------------------------------\n",
      "L1 loss: 0.04964370280504227\n",
      "Training batch 69 with loss 0.25816\n",
      "------------------------------------------\n",
      "L1 loss: 0.07471335679292679\n",
      "Training batch 70 with loss 0.31201\n",
      "------------------------------------------\n",
      "L1 loss: 0.07123756408691406\n",
      "Training batch 71 with loss 0.29337\n",
      "------------------------------------------\n",
      "L1 loss: 0.06216005980968475\n",
      "Training batch 72 with loss 0.33540\n",
      "------------------------------------------\n",
      "L1 loss: 0.07125512510538101\n",
      "Training batch 73 with loss 0.28965\n",
      "------------------------------------------\n",
      "L1 loss: 0.09879043698310852\n",
      "Training batch 74 with loss 0.31744\n",
      "------------------------------------------\n",
      "L1 loss: 0.07578559964895248\n",
      "Training batch 75 with loss 0.28895\n",
      "------------------------------------------\n",
      "L1 loss: 0.08566880971193314\n",
      "Training batch 76 with loss 0.30613\n",
      "------------------------------------------\n",
      "L1 loss: 0.07951284199953079\n",
      "Training batch 77 with loss 0.26187\n",
      "------------------------------------------\n",
      "L1 loss: 0.08288636803627014\n",
      "Training batch 78 with loss 0.30557\n",
      "------------------------------------------\n",
      "L1 loss: 0.10343111306428909\n",
      "Training batch 79 with loss 0.37127\n",
      "------------------------------------------\n",
      "L1 loss: 0.07575162500143051\n",
      "Training batch 80 with loss 0.28102\n",
      "------------------------------------------\n",
      "L1 loss: 0.07575422525405884\n",
      "Training batch 81 with loss 0.30766\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08502502739429474\n",
      "Training batch 82 with loss 0.36406\n",
      "------------------------------------------\n",
      "L1 loss: 0.07854355871677399\n",
      "Training batch 83 with loss 0.27968\n",
      "------------------------------------------\n",
      "L1 loss: 0.06977862864732742\n",
      "Training batch 84 with loss 0.29843\n",
      "------------------------------------------\n",
      "L1 loss: 0.062345240265131\n",
      "Training batch 85 with loss 0.26233\n",
      "------------------------------------------\n",
      "L1 loss: 0.0781557634472847\n",
      "Training batch 86 with loss 0.27901\n",
      "------------------------------------------\n",
      "L1 loss: 0.1016417071223259\n",
      "Training batch 87 with loss 0.40010\n",
      "------------------------------------------\n",
      "L1 loss: 0.07872600853443146\n",
      "Training batch 88 with loss 0.31727\n",
      "------------------------------------------\n",
      "L1 loss: 0.06929484009742737\n",
      "Training batch 89 with loss 0.26710\n",
      "------------------------------------------\n",
      "L1 loss: 0.07510615140199661\n",
      "Training batch 90 with loss 0.29335\n",
      "------------------------------------------\n",
      "L1 loss: 0.0569574348628521\n",
      "Training batch 91 with loss 0.24433\n",
      "------------------------------------------\n",
      "L1 loss: 0.08145871013402939\n",
      "Training batch 92 with loss 0.39047\n",
      "------------------------------------------\n",
      "L1 loss: 0.061963144689798355\n",
      "Training batch 93 with loss 0.24505\n",
      "------------------------------------------\n",
      "L1 loss: 0.08607590943574905\n",
      "Training batch 94 with loss 0.30028\n",
      "------------------------------------------\n",
      "L1 loss: 0.07534952461719513\n",
      "Training batch 95 with loss 0.37594\n",
      "------------------------------------------\n",
      "L1 loss: 0.06350604444742203\n",
      "Training batch 96 with loss 0.28305\n",
      "------------------------------------------\n",
      "L1 loss: 0.08786438405513763\n",
      "Training batch 97 with loss 0.36826\n",
      "------------------------------------------\n",
      "L1 loss: 0.05650950223207474\n",
      "Training batch 98 with loss 0.25142\n",
      "------------------------------------------\n",
      "L1 loss: 0.0643182173371315\n",
      "Training batch 99 with loss 0.28351\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2951406726241112\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4832\n",
      "------------------------------------------\n",
      "L1 loss: 0.06091885268688202\n",
      "Training batch 0 with loss 0.24802\n",
      "------------------------------------------\n",
      "L1 loss: 0.08404826372861862\n",
      "Training batch 1 with loss 0.29007\n",
      "------------------------------------------\n",
      "L1 loss: 0.09414346516132355\n",
      "Training batch 2 with loss 0.32137\n",
      "------------------------------------------\n",
      "L1 loss: 0.0708482414484024\n",
      "Training batch 3 with loss 0.26672\n",
      "------------------------------------------\n",
      "L1 loss: 0.08477049320936203\n",
      "Training batch 4 with loss 0.32362\n",
      "------------------------------------------\n",
      "L1 loss: 0.06939402967691422\n",
      "Training batch 5 with loss 0.36818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07835457473993301\n",
      "Training batch 6 with loss 0.26920\n",
      "------------------------------------------\n",
      "L1 loss: 0.0831310898065567\n",
      "Training batch 7 with loss 0.33258\n",
      "------------------------------------------\n",
      "L1 loss: 0.07806466519832611\n",
      "Training batch 8 with loss 0.32730\n",
      "------------------------------------------\n",
      "L1 loss: 0.05833439528942108\n",
      "Training batch 9 with loss 0.23797\n",
      "------------------------------------------\n",
      "L1 loss: 0.08279763162136078\n",
      "Training batch 10 with loss 0.28147\n",
      "------------------------------------------\n",
      "L1 loss: 0.06116466596722603\n",
      "Training batch 11 with loss 0.29367\n",
      "------------------------------------------\n",
      "L1 loss: 0.07781931012868881\n",
      "Training batch 12 with loss 0.32357\n",
      "------------------------------------------\n",
      "L1 loss: 0.06886530667543411\n",
      "Training batch 13 with loss 0.26079\n",
      "------------------------------------------\n",
      "L1 loss: 0.05389745533466339\n",
      "Training batch 14 with loss 0.27602\n",
      "------------------------------------------\n",
      "L1 loss: 0.08471476286649704\n",
      "Training batch 15 with loss 0.28745\n",
      "------------------------------------------\n",
      "L1 loss: 0.0644194558262825\n",
      "Training batch 16 with loss 0.26551\n",
      "------------------------------------------\n",
      "L1 loss: 0.08749891817569733\n",
      "Training batch 17 with loss 0.35841\n",
      "------------------------------------------\n",
      "L1 loss: 0.07908247411251068\n",
      "Training batch 18 with loss 0.31526\n",
      "------------------------------------------\n",
      "L1 loss: 0.06070353463292122\n",
      "Training batch 19 with loss 0.23279\n",
      "------------------------------------------\n",
      "L1 loss: 0.07247812300920486\n",
      "Training batch 20 with loss 0.32445\n",
      "------------------------------------------\n",
      "L1 loss: 0.0724591389298439\n",
      "Training batch 21 with loss 0.28133\n",
      "------------------------------------------\n",
      "L1 loss: 0.06877220422029495\n",
      "Training batch 22 with loss 0.30671\n",
      "------------------------------------------\n",
      "L1 loss: 0.09262288361787796\n",
      "Training batch 23 with loss 0.38473\n",
      "------------------------------------------\n",
      "L1 loss: 0.06843805313110352\n",
      "Training batch 24 with loss 0.29198\n",
      "------------------------------------------\n",
      "L1 loss: 0.07059755176305771\n",
      "Training batch 25 with loss 0.27868\n",
      "------------------------------------------\n",
      "L1 loss: 0.0740051344037056\n",
      "Training batch 26 with loss 0.28102\n",
      "------------------------------------------\n",
      "L1 loss: 0.05748932436108589\n",
      "Training batch 27 with loss 0.28775\n",
      "------------------------------------------\n",
      "L1 loss: 0.0575760118663311\n",
      "Training batch 28 with loss 0.27613\n",
      "------------------------------------------\n",
      "L1 loss: 0.06153840199112892\n",
      "Training batch 29 with loss 0.24978\n",
      "------------------------------------------\n",
      "L1 loss: 0.06232063099741936\n",
      "Training batch 30 with loss 0.27073\n",
      "------------------------------------------\n",
      "L1 loss: 0.06455762684345245\n",
      "Training batch 31 with loss 0.30011\n",
      "------------------------------------------\n",
      "L1 loss: 0.07068571448326111\n",
      "Training batch 32 with loss 0.28060\n",
      "------------------------------------------\n",
      "L1 loss: 0.06285808235406876\n",
      "Training batch 33 with loss 0.27160\n",
      "------------------------------------------\n",
      "L1 loss: 0.07499537616968155\n",
      "Training batch 34 with loss 0.31369\n",
      "------------------------------------------\n",
      "L1 loss: 0.09554996341466904\n",
      "Training batch 35 with loss 0.33635\n",
      "------------------------------------------\n",
      "L1 loss: 0.08397968113422394\n",
      "Training batch 36 with loss 0.28540\n",
      "------------------------------------------\n",
      "L1 loss: 0.06772025674581528\n",
      "Training batch 37 with loss 0.25924\n",
      "------------------------------------------\n",
      "L1 loss: 0.06148175895214081\n",
      "Training batch 38 with loss 0.22741\n",
      "------------------------------------------\n",
      "L1 loss: 0.0657753050327301\n",
      "Training batch 39 with loss 0.29547\n",
      "------------------------------------------\n",
      "L1 loss: 0.06792245805263519\n",
      "Training batch 40 with loss 0.24547\n",
      "------------------------------------------\n",
      "L1 loss: 0.0586087703704834\n",
      "Training batch 41 with loss 0.29450\n",
      "------------------------------------------\n",
      "L1 loss: 0.07605523616075516\n",
      "Training batch 42 with loss 0.26847\n",
      "------------------------------------------\n",
      "L1 loss: 0.07151208072900772\n",
      "Training batch 43 with loss 0.27337\n",
      "------------------------------------------\n",
      "L1 loss: 0.056695666164159775\n",
      "Training batch 44 with loss 0.25223\n",
      "------------------------------------------\n",
      "L1 loss: 0.0928262248635292\n",
      "Training batch 45 with loss 0.36942\n",
      "------------------------------------------\n",
      "L1 loss: 0.07344783842563629\n",
      "Training batch 46 with loss 0.45920\n",
      "------------------------------------------\n",
      "L1 loss: 0.11159338057041168\n",
      "Training batch 47 with loss 0.37726\n",
      "------------------------------------------\n",
      "L1 loss: 0.047780875116586685\n",
      "Training batch 48 with loss 0.25012\n",
      "------------------------------------------\n",
      "L1 loss: 0.07966198772192001\n",
      "Training batch 49 with loss 0.35326\n",
      "------------------------------------------\n",
      "L1 loss: 0.07209669798612595\n",
      "Training batch 50 with loss 0.28395\n",
      "------------------------------------------\n",
      "L1 loss: 0.07834172993898392\n",
      "Training batch 51 with loss 0.32681\n",
      "------------------------------------------\n",
      "L1 loss: 0.090184286236763\n",
      "Training batch 52 with loss 0.28788\n",
      "------------------------------------------\n",
      "L1 loss: 0.08050333708524704\n",
      "Training batch 53 with loss 0.29548\n",
      "------------------------------------------\n",
      "L1 loss: 0.07748798280954361\n",
      "Training batch 54 with loss 0.27562\n",
      "------------------------------------------\n",
      "L1 loss: 0.05946343019604683\n",
      "Training batch 55 with loss 0.29483\n",
      "------------------------------------------\n",
      "L1 loss: 0.05569756031036377\n",
      "Training batch 56 with loss 0.25322\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06695137172937393\n",
      "Training batch 57 with loss 0.27113\n",
      "------------------------------------------\n",
      "L1 loss: 0.06796987354755402\n",
      "Training batch 58 with loss 0.26992\n",
      "------------------------------------------\n",
      "L1 loss: 0.07957585155963898\n",
      "Training batch 59 with loss 0.30413\n",
      "------------------------------------------\n",
      "L1 loss: 0.06711921095848083\n",
      "Training batch 60 with loss 0.29260\n",
      "------------------------------------------\n",
      "L1 loss: 0.09649328887462616\n",
      "Training batch 61 with loss 0.35219\n",
      "------------------------------------------\n",
      "L1 loss: 0.04759140685200691\n",
      "Training batch 62 with loss 0.25050\n",
      "------------------------------------------\n",
      "L1 loss: 0.06048445403575897\n",
      "Training batch 63 with loss 0.24660\n",
      "------------------------------------------\n",
      "L1 loss: 0.0718584954738617\n",
      "Training batch 64 with loss 0.28607\n",
      "------------------------------------------\n",
      "L1 loss: 0.060109492391347885\n",
      "Training batch 65 with loss 0.23448\n",
      "------------------------------------------\n",
      "L1 loss: 0.06478054821491241\n",
      "Training batch 66 with loss 0.26603\n",
      "------------------------------------------\n",
      "L1 loss: 0.0660301148891449\n",
      "Training batch 67 with loss 0.26828\n",
      "------------------------------------------\n",
      "L1 loss: 0.07177618145942688\n",
      "Training batch 68 with loss 0.31213\n",
      "------------------------------------------\n",
      "L1 loss: 0.05098314583301544\n",
      "Training batch 69 with loss 0.27027\n",
      "------------------------------------------\n",
      "L1 loss: 0.08217859268188477\n",
      "Training batch 70 with loss 0.30860\n",
      "------------------------------------------\n",
      "L1 loss: 0.07160444557666779\n",
      "Training batch 71 with loss 0.33772\n",
      "------------------------------------------\n",
      "L1 loss: 0.0866546779870987\n",
      "Training batch 72 with loss 0.31561\n",
      "------------------------------------------\n",
      "L1 loss: 0.0701766237616539\n",
      "Training batch 73 with loss 0.29986\n",
      "------------------------------------------\n",
      "L1 loss: 0.09585309773683548\n",
      "Training batch 74 with loss 0.31674\n",
      "------------------------------------------\n",
      "L1 loss: 0.07529205828905106\n",
      "Training batch 75 with loss 0.27843\n",
      "------------------------------------------\n",
      "L1 loss: 0.08671916276216507\n",
      "Training batch 76 with loss 0.28768\n",
      "------------------------------------------\n",
      "L1 loss: 0.07862677425146103\n",
      "Training batch 77 with loss 0.26735\n",
      "------------------------------------------\n",
      "L1 loss: 0.08348806947469711\n",
      "Training batch 78 with loss 0.31342\n",
      "------------------------------------------\n",
      "L1 loss: 0.104206383228302\n",
      "Training batch 79 with loss 0.36850\n",
      "------------------------------------------\n",
      "L1 loss: 0.07610227167606354\n",
      "Training batch 80 with loss 0.25966\n",
      "------------------------------------------\n",
      "L1 loss: 0.08173082023859024\n",
      "Training batch 81 with loss 0.31068\n",
      "------------------------------------------\n",
      "L1 loss: 0.08098812401294708\n",
      "Training batch 82 with loss 0.34855\n",
      "------------------------------------------\n",
      "L1 loss: 0.07650219649076462\n",
      "Training batch 83 with loss 0.27826\n",
      "------------------------------------------\n",
      "L1 loss: 0.05328971520066261\n",
      "Training batch 84 with loss 0.39588\n",
      "------------------------------------------\n",
      "L1 loss: 0.06067425012588501\n",
      "Training batch 85 with loss 0.25675\n",
      "------------------------------------------\n",
      "L1 loss: 0.08178534358739853\n",
      "Training batch 86 with loss 0.29669\n",
      "------------------------------------------\n",
      "L1 loss: 0.10128818452358246\n",
      "Training batch 87 with loss 0.39037\n",
      "------------------------------------------\n",
      "L1 loss: 0.07786701619625092\n",
      "Training batch 88 with loss 0.30790\n",
      "------------------------------------------\n",
      "L1 loss: 0.07009217888116837\n",
      "Training batch 89 with loss 0.26083\n",
      "------------------------------------------\n",
      "L1 loss: 0.07463424652814865\n",
      "Training batch 90 with loss 0.28251\n",
      "------------------------------------------\n",
      "L1 loss: 0.06131312996149063\n",
      "Training batch 91 with loss 0.25480\n",
      "------------------------------------------\n",
      "L1 loss: 0.0957024022936821\n",
      "Training batch 92 with loss 0.31676\n",
      "------------------------------------------\n",
      "L1 loss: 0.05995900556445122\n",
      "Training batch 93 with loss 0.24622\n",
      "------------------------------------------\n",
      "L1 loss: 0.08511990308761597\n",
      "Training batch 94 with loss 0.31253\n",
      "------------------------------------------\n",
      "L1 loss: 0.07156893610954285\n",
      "Training batch 95 with loss 0.26394\n",
      "------------------------------------------\n",
      "L1 loss: 0.06206747516989708\n",
      "Training batch 96 with loss 0.27893\n",
      "------------------------------------------\n",
      "L1 loss: 0.089578777551651\n",
      "Training batch 97 with loss 0.35405\n",
      "------------------------------------------\n",
      "L1 loss: 0.056440193206071854\n",
      "Training batch 98 with loss 0.24085\n",
      "------------------------------------------\n",
      "L1 loss: 0.06692389398813248\n",
      "Training batch 99 with loss 0.26765\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2948626410961151\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4833\n",
      "------------------------------------------\n",
      "L1 loss: 0.060405880212783813\n",
      "Training batch 0 with loss 0.24459\n",
      "------------------------------------------\n",
      "L1 loss: 0.08647742122411728\n",
      "Training batch 1 with loss 0.27454\n",
      "------------------------------------------\n",
      "L1 loss: 0.09523186832666397\n",
      "Training batch 2 with loss 0.34318\n",
      "------------------------------------------\n",
      "L1 loss: 0.07234974205493927\n",
      "Training batch 3 with loss 0.25553\n",
      "------------------------------------------\n",
      "L1 loss: 0.08104506134986877\n",
      "Training batch 4 with loss 0.32522\n",
      "------------------------------------------\n",
      "L1 loss: 0.0655088797211647\n",
      "Training batch 5 with loss 0.32977\n",
      "------------------------------------------\n",
      "L1 loss: 0.07902064919471741\n",
      "Training batch 6 with loss 0.26928\n",
      "------------------------------------------\n",
      "L1 loss: 0.08251126110553741\n",
      "Training batch 7 with loss 0.32511\n",
      "------------------------------------------\n",
      "L1 loss: 0.08361633121967316\n",
      "Training batch 8 with loss 0.41460\n",
      "------------------------------------------\n",
      "L1 loss: 0.0623764730989933\n",
      "Training batch 9 with loss 0.26137\n",
      "------------------------------------------\n",
      "L1 loss: 0.08194106817245483\n",
      "Training batch 10 with loss 0.30569\n",
      "------------------------------------------\n",
      "L1 loss: 0.06092824786901474\n",
      "Training batch 11 with loss 0.29335\n",
      "------------------------------------------\n",
      "L1 loss: 0.07607417553663254\n",
      "Training batch 12 with loss 0.30911\n",
      "------------------------------------------\n",
      "L1 loss: 0.06849578022956848\n",
      "Training batch 13 with loss 0.25613\n",
      "------------------------------------------\n",
      "L1 loss: 0.052570752799510956\n",
      "Training batch 14 with loss 0.24885\n",
      "------------------------------------------\n",
      "L1 loss: 0.08355260640382767\n",
      "Training batch 15 with loss 0.26384\n",
      "------------------------------------------\n",
      "L1 loss: 0.04307745024561882\n",
      "Training batch 16 with loss 0.31648\n",
      "------------------------------------------\n",
      "L1 loss: 0.08348869532346725\n",
      "Training batch 17 with loss 0.32246\n",
      "------------------------------------------\n",
      "L1 loss: 0.08091852813959122\n",
      "Training batch 18 with loss 0.31382\n",
      "------------------------------------------\n",
      "L1 loss: 0.060534387826919556\n",
      "Training batch 19 with loss 0.24024\n",
      "------------------------------------------\n",
      "L1 loss: 0.07351699471473694\n",
      "Training batch 20 with loss 0.32517\n",
      "------------------------------------------\n",
      "L1 loss: 0.0725536122918129\n",
      "Training batch 21 with loss 0.28028\n",
      "------------------------------------------\n",
      "L1 loss: 0.06864612549543381\n",
      "Training batch 22 with loss 0.29206\n",
      "------------------------------------------\n",
      "L1 loss: 0.08373339474201202\n",
      "Training batch 23 with loss 0.34423\n",
      "------------------------------------------\n",
      "L1 loss: 0.06752309948205948\n",
      "Training batch 24 with loss 0.29229\n",
      "------------------------------------------\n",
      "L1 loss: 0.07742134481668472\n",
      "Training batch 25 with loss 0.31077\n",
      "------------------------------------------\n",
      "L1 loss: 0.07772812992334366\n",
      "Training batch 26 with loss 0.27117\n",
      "------------------------------------------\n",
      "L1 loss: 0.05791115388274193\n",
      "Training batch 27 with loss 0.30323\n",
      "------------------------------------------\n",
      "L1 loss: 0.05758878216147423\n",
      "Training batch 28 with loss 0.29877\n",
      "------------------------------------------\n",
      "L1 loss: 0.060620635747909546\n",
      "Training batch 29 with loss 0.25108\n",
      "------------------------------------------\n",
      "L1 loss: 0.06129085645079613\n",
      "Training batch 30 with loss 0.28095\n",
      "------------------------------------------\n",
      "L1 loss: 0.06631512939929962\n",
      "Training batch 31 with loss 0.30586\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07093171030282974\n",
      "Training batch 32 with loss 0.30110\n",
      "------------------------------------------\n",
      "L1 loss: 0.0632198229432106\n",
      "Training batch 33 with loss 0.25571\n",
      "------------------------------------------\n",
      "L1 loss: 0.07089437544345856\n",
      "Training batch 34 with loss 0.46251\n",
      "------------------------------------------\n",
      "L1 loss: 0.0943184420466423\n",
      "Training batch 35 with loss 0.32973\n",
      "------------------------------------------\n",
      "L1 loss: 0.0836862176656723\n",
      "Training batch 36 with loss 0.28446\n",
      "------------------------------------------\n",
      "L1 loss: 0.06749234348535538\n",
      "Training batch 37 with loss 0.25914\n",
      "------------------------------------------\n",
      "L1 loss: 0.06214902177453041\n",
      "Training batch 38 with loss 0.22165\n",
      "------------------------------------------\n",
      "L1 loss: 0.05943907052278519\n",
      "Training batch 39 with loss 0.47688\n",
      "------------------------------------------\n",
      "L1 loss: 0.06875716149806976\n",
      "Training batch 40 with loss 0.24395\n",
      "------------------------------------------\n",
      "L1 loss: 0.05836034566164017\n",
      "Training batch 41 with loss 0.28044\n",
      "------------------------------------------\n",
      "L1 loss: 0.07663523405790329\n",
      "Training batch 42 with loss 0.27928\n",
      "------------------------------------------\n",
      "L1 loss: 0.07232852280139923\n",
      "Training batch 43 with loss 0.29799\n",
      "------------------------------------------\n",
      "L1 loss: 0.05701376870274544\n",
      "Training batch 44 with loss 0.27039\n",
      "------------------------------------------\n",
      "L1 loss: 0.09263692796230316\n",
      "Training batch 45 with loss 0.36177\n",
      "------------------------------------------\n",
      "L1 loss: 0.07488618791103363\n",
      "Training batch 46 with loss 0.28180\n",
      "------------------------------------------\n",
      "L1 loss: 0.11174052208662033\n",
      "Training batch 47 with loss 0.37928\n",
      "------------------------------------------\n",
      "L1 loss: 0.04456048458814621\n",
      "Training batch 48 with loss 0.24651\n",
      "------------------------------------------\n",
      "L1 loss: 0.08223260939121246\n",
      "Training batch 49 with loss 0.36953\n",
      "------------------------------------------\n",
      "L1 loss: 0.07227255403995514\n",
      "Training batch 50 with loss 0.29035\n",
      "------------------------------------------\n",
      "L1 loss: 0.08177918195724487\n",
      "Training batch 51 with loss 0.35070\n",
      "------------------------------------------\n",
      "L1 loss: 0.08932723104953766\n",
      "Training batch 52 with loss 0.29754\n",
      "------------------------------------------\n",
      "L1 loss: 0.07733868807554245\n",
      "Training batch 53 with loss 0.29241\n",
      "------------------------------------------\n",
      "L1 loss: 0.0764760822057724\n",
      "Training batch 54 with loss 0.27361\n",
      "------------------------------------------\n",
      "L1 loss: 0.06044387072324753\n",
      "Training batch 55 with loss 0.30860\n",
      "------------------------------------------\n",
      "L1 loss: 0.05475080385804176\n",
      "Training batch 56 with loss 0.35122\n",
      "------------------------------------------\n",
      "L1 loss: 0.06689011305570602\n",
      "Training batch 57 with loss 0.28474\n",
      "------------------------------------------\n",
      "L1 loss: 0.06430722028017044\n",
      "Training batch 58 with loss 0.26919\n",
      "------------------------------------------\n",
      "L1 loss: 0.06342357397079468\n",
      "Training batch 59 with loss 0.41684\n",
      "------------------------------------------\n",
      "L1 loss: 0.06433629989624023\n",
      "Training batch 60 with loss 0.26942\n",
      "------------------------------------------\n",
      "L1 loss: 0.09793534129858017\n",
      "Training batch 61 with loss 0.36011\n",
      "------------------------------------------\n",
      "L1 loss: 0.05583205074071884\n",
      "Training batch 62 with loss 0.25286\n",
      "------------------------------------------\n",
      "L1 loss: 0.060835953801870346\n",
      "Training batch 63 with loss 0.27601\n",
      "------------------------------------------\n",
      "L1 loss: 0.06939306855201721\n",
      "Training batch 64 with loss 0.28769\n",
      "------------------------------------------\n",
      "L1 loss: 0.06123552843928337\n",
      "Training batch 65 with loss 0.24995\n",
      "------------------------------------------\n",
      "L1 loss: 0.06717943400144577\n",
      "Training batch 66 with loss 0.30019\n",
      "------------------------------------------\n",
      "L1 loss: 0.06379301846027374\n",
      "Training batch 67 with loss 0.26386\n",
      "------------------------------------------\n",
      "L1 loss: 0.07108791917562485\n",
      "Training batch 68 with loss 0.30161\n",
      "------------------------------------------\n",
      "L1 loss: 0.04658672213554382\n",
      "Training batch 69 with loss 0.27854\n",
      "------------------------------------------\n",
      "L1 loss: 0.06396540254354477\n",
      "Training batch 70 with loss 0.26471\n",
      "------------------------------------------\n",
      "L1 loss: 0.07209914177656174\n",
      "Training batch 71 with loss 0.31547\n",
      "------------------------------------------\n",
      "L1 loss: 0.08002199977636337\n",
      "Training batch 72 with loss 0.31347\n",
      "------------------------------------------\n",
      "L1 loss: 0.07090740650892258\n",
      "Training batch 73 with loss 0.28548\n",
      "------------------------------------------\n",
      "L1 loss: 0.09732368588447571\n",
      "Training batch 74 with loss 0.30578\n",
      "------------------------------------------\n",
      "L1 loss: 0.07380536198616028\n",
      "Training batch 75 with loss 0.28184\n",
      "------------------------------------------\n",
      "L1 loss: 0.08674455434083939\n",
      "Training batch 76 with loss 0.28443\n",
      "------------------------------------------\n",
      "L1 loss: 0.07925418764352798\n",
      "Training batch 77 with loss 0.26485\n",
      "------------------------------------------\n",
      "L1 loss: 0.08164668083190918\n",
      "Training batch 78 with loss 0.31833\n",
      "------------------------------------------\n",
      "L1 loss: 0.08118653297424316\n",
      "Training batch 79 with loss 0.70024\n",
      "------------------------------------------\n",
      "L1 loss: 0.07622421532869339\n",
      "Training batch 80 with loss 0.27269\n",
      "------------------------------------------\n",
      "L1 loss: 0.081526018679142\n",
      "Training batch 81 with loss 0.29818\n",
      "------------------------------------------\n",
      "L1 loss: 0.08523557335138321\n",
      "Training batch 82 with loss 0.34785\n",
      "------------------------------------------\n",
      "L1 loss: 0.07825318723917007\n",
      "Training batch 83 with loss 0.29145\n",
      "------------------------------------------\n",
      "L1 loss: 0.06967706978321075\n",
      "Training batch 84 with loss 0.29854\n",
      "------------------------------------------\n",
      "L1 loss: 0.06163504347205162\n",
      "Training batch 85 with loss 0.27769\n",
      "------------------------------------------\n",
      "L1 loss: 0.081896111369133\n",
      "Training batch 86 with loss 0.30038\n",
      "------------------------------------------\n",
      "L1 loss: 0.10008609294891357\n",
      "Training batch 87 with loss 0.44304\n",
      "------------------------------------------\n",
      "L1 loss: 0.07569219172000885\n",
      "Training batch 88 with loss 0.31464\n",
      "------------------------------------------\n",
      "L1 loss: 0.07026563584804535\n",
      "Training batch 89 with loss 0.26745\n",
      "------------------------------------------\n",
      "L1 loss: 0.07394557446241379\n",
      "Training batch 90 with loss 0.28715\n",
      "------------------------------------------\n",
      "L1 loss: 0.06130876764655113\n",
      "Training batch 91 with loss 0.25310\n",
      "------------------------------------------\n",
      "L1 loss: 0.0931684747338295\n",
      "Training batch 92 with loss 0.29181\n",
      "------------------------------------------\n",
      "L1 loss: 0.07040220499038696\n",
      "Training batch 93 with loss 0.27329\n",
      "------------------------------------------\n",
      "L1 loss: 0.08458112180233002\n",
      "Training batch 94 with loss 0.29367\n",
      "------------------------------------------\n",
      "L1 loss: 0.07253331691026688\n",
      "Training batch 95 with loss 0.24473\n",
      "------------------------------------------\n",
      "L1 loss: 0.06745627522468567\n",
      "Training batch 96 with loss 0.28241\n",
      "------------------------------------------\n",
      "L1 loss: 0.08667720854282379\n",
      "Training batch 97 with loss 0.36542\n",
      "------------------------------------------\n",
      "L1 loss: 0.057505372911691666\n",
      "Training batch 98 with loss 0.23689\n",
      "------------------------------------------\n",
      "L1 loss: 0.061584919691085815\n",
      "Training batch 99 with loss 0.24906\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.30270678535103795\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4834\n",
      "------------------------------------------\n",
      "L1 loss: 0.0593709759414196\n",
      "Training batch 0 with loss 0.25083\n",
      "------------------------------------------\n",
      "L1 loss: 0.08129030466079712\n",
      "Training batch 1 with loss 0.28296\n",
      "------------------------------------------\n",
      "L1 loss: 0.0914016142487526\n",
      "Training batch 2 with loss 0.32110\n",
      "------------------------------------------\n",
      "L1 loss: 0.06732936203479767\n",
      "Training batch 3 with loss 0.25573\n",
      "------------------------------------------\n",
      "L1 loss: 0.0828336700797081\n",
      "Training batch 4 with loss 0.33348\n",
      "------------------------------------------\n",
      "L1 loss: 0.0664181038737297\n",
      "Training batch 5 with loss 0.32986\n",
      "------------------------------------------\n",
      "L1 loss: 0.08016633987426758\n",
      "Training batch 6 with loss 0.27166\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08344422280788422\n",
      "Training batch 7 with loss 0.32405\n",
      "------------------------------------------\n",
      "L1 loss: 0.08222497254610062\n",
      "Training batch 8 with loss 0.34272\n",
      "------------------------------------------\n",
      "L1 loss: 0.06378522515296936\n",
      "Training batch 9 with loss 0.25689\n",
      "------------------------------------------\n",
      "L1 loss: 0.0830233246088028\n",
      "Training batch 10 with loss 0.28589\n",
      "------------------------------------------\n",
      "L1 loss: 0.06325533986091614\n",
      "Training batch 11 with loss 0.31966\n",
      "------------------------------------------\n",
      "L1 loss: 0.0749925747513771\n",
      "Training batch 12 with loss 0.30893\n",
      "------------------------------------------\n",
      "L1 loss: 0.0676487609744072\n",
      "Training batch 13 with loss 0.23841\n",
      "------------------------------------------\n",
      "L1 loss: 0.05384029448032379\n",
      "Training batch 14 with loss 0.26620\n",
      "------------------------------------------\n",
      "L1 loss: 0.0850469246506691\n",
      "Training batch 15 with loss 0.27262\n",
      "------------------------------------------\n",
      "L1 loss: 0.061707403510808945\n",
      "Training batch 16 with loss 0.27559\n",
      "------------------------------------------\n",
      "L1 loss: 0.08494074642658234\n",
      "Training batch 17 with loss 0.33661\n",
      "------------------------------------------\n",
      "L1 loss: 0.08071592450141907\n",
      "Training batch 18 with loss 0.29232\n",
      "------------------------------------------\n",
      "L1 loss: 0.06341653317213058\n",
      "Training batch 19 with loss 0.23245\n",
      "------------------------------------------\n",
      "L1 loss: 0.07362764328718185\n",
      "Training batch 20 with loss 0.33396\n",
      "------------------------------------------\n",
      "L1 loss: 0.06946440786123276\n",
      "Training batch 21 with loss 0.27493\n",
      "------------------------------------------\n",
      "L1 loss: 0.0677964836359024\n",
      "Training batch 22 with loss 0.29222\n",
      "------------------------------------------\n",
      "L1 loss: 0.08904547244310379\n",
      "Training batch 23 with loss 0.36060\n",
      "------------------------------------------\n",
      "L1 loss: 0.06945246458053589\n",
      "Training batch 24 with loss 0.30769\n",
      "------------------------------------------\n",
      "L1 loss: 0.07699522376060486\n",
      "Training batch 25 with loss 0.30132\n",
      "------------------------------------------\n",
      "L1 loss: 0.07261038571596146\n",
      "Training batch 26 with loss 0.26973\n",
      "------------------------------------------\n",
      "L1 loss: 0.055354442447423935\n",
      "Training batch 27 with loss 0.28948\n",
      "------------------------------------------\n",
      "L1 loss: 0.05821968987584114\n",
      "Training batch 28 with loss 0.28395\n",
      "------------------------------------------\n",
      "L1 loss: 0.05723203718662262\n",
      "Training batch 29 with loss 0.24027\n",
      "------------------------------------------\n",
      "L1 loss: 0.06337786465883255\n",
      "Training batch 30 with loss 0.27025\n",
      "------------------------------------------\n",
      "L1 loss: 0.06360465288162231\n",
      "Training batch 31 with loss 0.30743\n",
      "------------------------------------------\n",
      "L1 loss: 0.07085368782281876\n",
      "Training batch 32 with loss 0.29415\n",
      "------------------------------------------\n",
      "L1 loss: 0.06114453077316284\n",
      "Training batch 33 with loss 0.25239\n",
      "------------------------------------------\n",
      "L1 loss: 0.07810474932193756\n",
      "Training batch 34 with loss 0.32347\n",
      "------------------------------------------\n",
      "L1 loss: 0.094559445977211\n",
      "Training batch 35 with loss 0.35121\n",
      "------------------------------------------\n",
      "L1 loss: 0.08483602106571198\n",
      "Training batch 36 with loss 0.28632\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709189772605896\n",
      "Training batch 37 with loss 0.28314\n",
      "------------------------------------------\n",
      "L1 loss: 0.06386207044124603\n",
      "Training batch 38 with loss 0.22353\n",
      "------------------------------------------\n",
      "L1 loss: 0.0671154037117958\n",
      "Training batch 39 with loss 0.30581\n",
      "------------------------------------------\n",
      "L1 loss: 0.07470579445362091\n",
      "Training batch 40 with loss 0.27580\n",
      "------------------------------------------\n",
      "L1 loss: 0.06052638590335846\n",
      "Training batch 41 with loss 0.28579\n",
      "------------------------------------------\n",
      "L1 loss: 0.07911009341478348\n",
      "Training batch 42 with loss 0.29358\n",
      "------------------------------------------\n",
      "L1 loss: 0.0693158507347107\n",
      "Training batch 43 with loss 0.31098\n",
      "------------------------------------------\n",
      "L1 loss: 0.05613172799348831\n",
      "Training batch 44 with loss 0.26009\n",
      "------------------------------------------\n",
      "L1 loss: 0.09171827882528305\n",
      "Training batch 45 with loss 0.36149\n",
      "------------------------------------------\n",
      "L1 loss: 0.07550626248121262\n",
      "Training batch 46 with loss 0.27892\n",
      "------------------------------------------\n",
      "L1 loss: 0.10912540555000305\n",
      "Training batch 47 with loss 0.36233\n",
      "------------------------------------------\n",
      "L1 loss: 0.048799846321344376\n",
      "Training batch 48 with loss 0.24371\n",
      "------------------------------------------\n",
      "L1 loss: 0.0843002125620842\n",
      "Training batch 49 with loss 0.37641\n",
      "------------------------------------------\n",
      "L1 loss: 0.07357914745807648\n",
      "Training batch 50 with loss 0.29046\n",
      "------------------------------------------\n",
      "L1 loss: 0.07925492525100708\n",
      "Training batch 51 with loss 0.31870\n",
      "------------------------------------------\n",
      "L1 loss: 0.08961454778909683\n",
      "Training batch 52 with loss 0.28153\n",
      "------------------------------------------\n",
      "L1 loss: 0.07821917533874512\n",
      "Training batch 53 with loss 0.27919\n",
      "------------------------------------------\n",
      "L1 loss: 0.07481899857521057\n",
      "Training batch 54 with loss 0.27326\n",
      "------------------------------------------\n",
      "L1 loss: 0.06150929257273674\n",
      "Training batch 55 with loss 0.28760\n",
      "------------------------------------------\n",
      "L1 loss: 0.05388142168521881\n",
      "Training batch 56 with loss 0.25967\n",
      "------------------------------------------\n",
      "L1 loss: 0.06792039424180984\n",
      "Training batch 57 with loss 0.27957\n",
      "------------------------------------------\n",
      "L1 loss: 0.06878164410591125\n",
      "Training batch 58 with loss 0.27284\n",
      "------------------------------------------\n",
      "L1 loss: 0.07849230617284775\n",
      "Training batch 59 with loss 0.29183\n",
      "------------------------------------------\n",
      "L1 loss: 0.0636834055185318\n",
      "Training batch 60 with loss 0.28014\n",
      "------------------------------------------\n",
      "L1 loss: 0.10595708340406418\n",
      "Training batch 61 with loss 0.39393\n",
      "------------------------------------------\n",
      "L1 loss: 0.05694028362631798\n",
      "Training batch 62 with loss 0.25360\n",
      "------------------------------------------\n",
      "L1 loss: 0.0635262280702591\n",
      "Training batch 63 with loss 0.25033\n",
      "------------------------------------------\n",
      "L1 loss: 0.07066068798303604\n",
      "Training batch 64 with loss 0.28177\n",
      "------------------------------------------\n",
      "L1 loss: 0.05825986713171005\n",
      "Training batch 65 with loss 0.22007\n",
      "------------------------------------------\n",
      "L1 loss: 0.06420274823904037\n",
      "Training batch 66 with loss 0.27090\n",
      "------------------------------------------\n",
      "L1 loss: 0.06384894996881485\n",
      "Training batch 67 with loss 0.26121\n",
      "------------------------------------------\n",
      "L1 loss: 0.07110535353422165\n",
      "Training batch 68 with loss 0.29946\n",
      "------------------------------------------\n",
      "L1 loss: 0.047823745757341385\n",
      "Training batch 69 with loss 0.25696\n",
      "------------------------------------------\n",
      "L1 loss: 0.07923833280801773\n",
      "Training batch 70 with loss 0.31225\n",
      "------------------------------------------\n",
      "L1 loss: 0.07188930362462997\n",
      "Training batch 71 with loss 0.30465\n",
      "------------------------------------------\n",
      "L1 loss: 0.0856848657131195\n",
      "Training batch 72 with loss 0.30839\n",
      "------------------------------------------\n",
      "L1 loss: 0.0721605196595192\n",
      "Training batch 73 with loss 0.31113\n",
      "------------------------------------------\n",
      "L1 loss: 0.0966019406914711\n",
      "Training batch 74 with loss 0.33007\n",
      "------------------------------------------\n",
      "L1 loss: 0.07335620373487473\n",
      "Training batch 75 with loss 0.27590\n",
      "------------------------------------------\n",
      "L1 loss: 0.08560341596603394\n",
      "Training batch 76 with loss 0.29389\n",
      "------------------------------------------\n",
      "L1 loss: 0.07901667803525925\n",
      "Training batch 77 with loss 0.27827\n",
      "------------------------------------------\n",
      "L1 loss: 0.08262759447097778\n",
      "Training batch 78 with loss 0.30290\n",
      "------------------------------------------\n",
      "L1 loss: 0.10472515225410461\n",
      "Training batch 79 with loss 0.36389\n",
      "------------------------------------------\n",
      "L1 loss: 0.07658061385154724\n",
      "Training batch 80 with loss 0.29018\n",
      "------------------------------------------\n",
      "L1 loss: 0.07698507606983185\n",
      "Training batch 81 with loss 0.28225\n",
      "------------------------------------------\n",
      "L1 loss: 0.08232041448354721\n",
      "Training batch 82 with loss 0.35362\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07372007519006729\n",
      "Training batch 83 with loss 0.29392\n",
      "------------------------------------------\n",
      "L1 loss: 0.07049724459648132\n",
      "Training batch 84 with loss 0.29749\n",
      "------------------------------------------\n",
      "L1 loss: 0.06778029352426529\n",
      "Training batch 85 with loss 0.27686\n",
      "------------------------------------------\n",
      "L1 loss: 0.08068665862083435\n",
      "Training batch 86 with loss 0.29937\n",
      "------------------------------------------\n",
      "L1 loss: 0.10049404948949814\n",
      "Training batch 87 with loss 0.40691\n",
      "------------------------------------------\n",
      "L1 loss: 0.07971849292516708\n",
      "Training batch 88 with loss 0.32684\n",
      "------------------------------------------\n",
      "L1 loss: 0.07055177539587021\n",
      "Training batch 89 with loss 0.26892\n",
      "------------------------------------------\n",
      "L1 loss: 0.07193897664546967\n",
      "Training batch 90 with loss 0.27487\n",
      "------------------------------------------\n",
      "L1 loss: 0.04811054840683937\n",
      "Training batch 91 with loss 0.43218\n",
      "------------------------------------------\n",
      "L1 loss: 0.09174732118844986\n",
      "Training batch 92 with loss 0.28738\n",
      "------------------------------------------\n",
      "L1 loss: 0.06557523459196091\n",
      "Training batch 93 with loss 0.25762\n",
      "------------------------------------------\n",
      "L1 loss: 0.08341524749994278\n",
      "Training batch 94 with loss 0.29840\n",
      "------------------------------------------\n",
      "L1 loss: 0.072256900370121\n",
      "Training batch 95 with loss 0.26018\n",
      "------------------------------------------\n",
      "L1 loss: 0.06585206836462021\n",
      "Training batch 96 with loss 0.29120\n",
      "------------------------------------------\n",
      "L1 loss: 0.08738601952791214\n",
      "Training batch 97 with loss 0.35998\n",
      "------------------------------------------\n",
      "L1 loss: 0.05734165012836456\n",
      "Training batch 98 with loss 0.23466\n",
      "------------------------------------------\n",
      "L1 loss: 0.061358917504549026\n",
      "Training batch 99 with loss 0.24916\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2942528556287289\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4835\n",
      "------------------------------------------\n",
      "L1 loss: 0.06038351356983185\n",
      "Training batch 0 with loss 0.24226\n",
      "------------------------------------------\n",
      "L1 loss: 0.08259859681129456\n",
      "Training batch 1 with loss 0.29735\n",
      "------------------------------------------\n",
      "L1 loss: 0.09416703879833221\n",
      "Training batch 2 with loss 0.33286\n",
      "------------------------------------------\n",
      "L1 loss: 0.07041376829147339\n",
      "Training batch 3 with loss 0.25506\n",
      "------------------------------------------\n",
      "L1 loss: 0.08381204307079315\n",
      "Training batch 4 with loss 0.34225\n",
      "------------------------------------------\n",
      "L1 loss: 0.06706146895885468\n",
      "Training batch 5 with loss 0.34046\n",
      "------------------------------------------\n",
      "L1 loss: 0.07968581467866898\n",
      "Training batch 6 with loss 0.26497\n",
      "------------------------------------------\n",
      "L1 loss: 0.08315019309520721\n",
      "Training batch 7 with loss 0.32643\n",
      "------------------------------------------\n",
      "L1 loss: 0.08000403642654419\n",
      "Training batch 8 with loss 0.32154\n",
      "------------------------------------------\n",
      "L1 loss: 0.061171770095825195\n",
      "Training batch 9 with loss 0.24903\n",
      "------------------------------------------\n",
      "L1 loss: 0.08301831781864166\n",
      "Training batch 10 with loss 0.30540\n",
      "------------------------------------------\n",
      "L1 loss: 0.06083870679140091\n",
      "Training batch 11 with loss 0.29099\n",
      "------------------------------------------\n",
      "L1 loss: 0.07309257239103317\n",
      "Training batch 12 with loss 0.32743\n",
      "------------------------------------------\n",
      "L1 loss: 0.06744811683893204\n",
      "Training batch 13 with loss 0.26321\n",
      "------------------------------------------\n",
      "L1 loss: 0.04148993641138077\n",
      "Training batch 14 with loss 0.25517\n",
      "------------------------------------------\n",
      "L1 loss: 0.0827283263206482\n",
      "Training batch 15 with loss 0.26808\n",
      "------------------------------------------\n",
      "L1 loss: 0.06295295804738998\n",
      "Training batch 16 with loss 0.26989\n",
      "------------------------------------------\n",
      "L1 loss: 0.09068339318037033\n",
      "Training batch 17 with loss 0.36208\n",
      "------------------------------------------\n",
      "L1 loss: 0.0788523331284523\n",
      "Training batch 18 with loss 0.29439\n",
      "------------------------------------------\n",
      "L1 loss: 0.062458354979753494\n",
      "Training batch 19 with loss 0.22903\n",
      "------------------------------------------\n",
      "L1 loss: 0.07312358915805817\n",
      "Training batch 20 with loss 0.32326\n",
      "------------------------------------------\n",
      "L1 loss: 0.06477664411067963\n",
      "Training batch 21 with loss 0.30567\n",
      "------------------------------------------\n",
      "L1 loss: 0.06822557747364044\n",
      "Training batch 22 with loss 0.28765\n",
      "------------------------------------------\n",
      "L1 loss: 0.08989396691322327\n",
      "Training batch 23 with loss 0.34471\n",
      "------------------------------------------\n",
      "L1 loss: 0.06779193878173828\n",
      "Training batch 24 with loss 0.30896\n",
      "------------------------------------------\n",
      "L1 loss: 0.07554886490106583\n",
      "Training batch 25 with loss 0.30824\n",
      "------------------------------------------\n",
      "L1 loss: 0.07227407395839691\n",
      "Training batch 26 with loss 0.27508\n",
      "------------------------------------------\n",
      "L1 loss: 0.05671079084277153\n",
      "Training batch 27 with loss 0.29642\n",
      "------------------------------------------\n",
      "L1 loss: 0.05920713394880295\n",
      "Training batch 28 with loss 0.29929\n",
      "------------------------------------------\n",
      "L1 loss: 0.06372140347957611\n",
      "Training batch 29 with loss 0.26066\n",
      "------------------------------------------\n",
      "L1 loss: 0.05992696434259415\n",
      "Training batch 30 with loss 0.26376\n",
      "------------------------------------------\n",
      "L1 loss: 0.0647701844573021\n",
      "Training batch 31 with loss 0.29889\n",
      "------------------------------------------\n",
      "L1 loss: 0.06869764626026154\n",
      "Training batch 32 with loss 0.27383\n",
      "------------------------------------------\n",
      "L1 loss: 0.06492208689451218\n",
      "Training batch 33 with loss 0.26659\n",
      "------------------------------------------\n",
      "L1 loss: 0.07474350929260254\n",
      "Training batch 34 with loss 0.33924\n",
      "------------------------------------------\n",
      "L1 loss: 0.09410789608955383\n",
      "Training batch 35 with loss 0.32986\n",
      "------------------------------------------\n",
      "L1 loss: 0.08368636667728424\n",
      "Training batch 36 with loss 0.27895\n",
      "------------------------------------------\n",
      "L1 loss: 0.06757451593875885\n",
      "Training batch 37 with loss 0.24914\n",
      "------------------------------------------\n",
      "L1 loss: 0.06447020918130875\n",
      "Training batch 38 with loss 0.22103\n",
      "------------------------------------------\n",
      "L1 loss: 0.060581523925065994\n",
      "Training batch 39 with loss 0.37584\n",
      "------------------------------------------\n",
      "L1 loss: 0.06787696480751038\n",
      "Training batch 40 with loss 0.24465\n",
      "------------------------------------------\n",
      "L1 loss: 0.060469381511211395\n",
      "Training batch 41 with loss 0.29666\n",
      "------------------------------------------\n",
      "L1 loss: 0.08012423664331436\n",
      "Training batch 42 with loss 0.28638\n",
      "------------------------------------------\n",
      "L1 loss: 0.07022970914840698\n",
      "Training batch 43 with loss 0.27883\n",
      "------------------------------------------\n",
      "L1 loss: 0.05735371634364128\n",
      "Training batch 44 with loss 0.26594\n",
      "------------------------------------------\n",
      "L1 loss: 0.09243039786815643\n",
      "Training batch 45 with loss 0.35561\n",
      "------------------------------------------\n",
      "L1 loss: 0.07016955316066742\n",
      "Training batch 46 with loss 0.26934\n",
      "------------------------------------------\n",
      "L1 loss: 0.10905561596155167\n",
      "Training batch 47 with loss 0.37766\n",
      "------------------------------------------\n",
      "L1 loss: 0.04715297743678093\n",
      "Training batch 48 with loss 0.23654\n",
      "------------------------------------------\n",
      "L1 loss: 0.06541424989700317\n",
      "Training batch 49 with loss 0.49798\n",
      "------------------------------------------\n",
      "L1 loss: 0.07223420590162277\n",
      "Training batch 50 with loss 0.27235\n",
      "------------------------------------------\n",
      "L1 loss: 0.08325657248497009\n",
      "Training batch 51 with loss 0.35449\n",
      "------------------------------------------\n",
      "L1 loss: 0.09104612469673157\n",
      "Training batch 52 with loss 0.30492\n",
      "------------------------------------------\n",
      "L1 loss: 0.07830800116062164\n",
      "Training batch 53 with loss 0.27668\n",
      "------------------------------------------\n",
      "L1 loss: 0.07043957710266113\n",
      "Training batch 54 with loss 0.25860\n",
      "------------------------------------------\n",
      "L1 loss: 0.05826086550951004\n",
      "Training batch 55 with loss 0.32474\n",
      "------------------------------------------\n",
      "L1 loss: 0.057629913091659546\n",
      "Training batch 56 with loss 0.25886\n",
      "------------------------------------------\n",
      "L1 loss: 0.067184217274189\n",
      "Training batch 57 with loss 0.26570\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06803502142429352\n",
      "Training batch 58 with loss 0.27033\n",
      "------------------------------------------\n",
      "L1 loss: 0.07979271560907364\n",
      "Training batch 59 with loss 0.29985\n",
      "------------------------------------------\n",
      "L1 loss: 0.06457775086164474\n",
      "Training batch 60 with loss 0.25867\n",
      "------------------------------------------\n",
      "L1 loss: 0.09749797731637955\n",
      "Training batch 61 with loss 0.34414\n",
      "------------------------------------------\n",
      "L1 loss: 0.056778766214847565\n",
      "Training batch 62 with loss 0.24712\n",
      "------------------------------------------\n",
      "L1 loss: 0.06177227571606636\n",
      "Training batch 63 with loss 0.25505\n",
      "------------------------------------------\n",
      "L1 loss: 0.06883028149604797\n",
      "Training batch 64 with loss 0.27877\n",
      "------------------------------------------\n",
      "L1 loss: 0.060616977512836456\n",
      "Training batch 65 with loss 0.22332\n",
      "------------------------------------------\n",
      "L1 loss: 0.06823022663593292\n",
      "Training batch 66 with loss 0.27789\n",
      "------------------------------------------\n",
      "L1 loss: 0.06463456898927689\n",
      "Training batch 67 with loss 0.26241\n",
      "------------------------------------------\n",
      "L1 loss: 0.061546847224235535\n",
      "Training batch 68 with loss 0.60673\n",
      "------------------------------------------\n",
      "L1 loss: 0.04983444884419441\n",
      "Training batch 69 with loss 0.27426\n",
      "------------------------------------------\n",
      "L1 loss: 0.07708296924829483\n",
      "Training batch 70 with loss 0.29985\n",
      "------------------------------------------\n",
      "L1 loss: 0.0723659098148346\n",
      "Training batch 71 with loss 0.31984\n",
      "------------------------------------------\n",
      "L1 loss: 0.0850476324558258\n",
      "Training batch 72 with loss 0.30958\n",
      "------------------------------------------\n",
      "L1 loss: 0.07462796568870544\n",
      "Training batch 73 with loss 0.29984\n",
      "------------------------------------------\n",
      "L1 loss: 0.09814432263374329\n",
      "Training batch 74 with loss 0.30671\n",
      "------------------------------------------\n",
      "L1 loss: 0.07325553894042969\n",
      "Training batch 75 with loss 0.28067\n",
      "------------------------------------------\n",
      "L1 loss: 0.0862870141863823\n",
      "Training batch 76 with loss 0.29283\n",
      "------------------------------------------\n",
      "L1 loss: 0.07898764312267303\n",
      "Training batch 77 with loss 0.26712\n",
      "------------------------------------------\n",
      "L1 loss: 0.08411255478858948\n",
      "Training batch 78 with loss 0.30089\n",
      "------------------------------------------\n",
      "L1 loss: 0.10573547333478928\n",
      "Training batch 79 with loss 0.36154\n",
      "------------------------------------------\n",
      "L1 loss: 0.07805880904197693\n",
      "Training batch 80 with loss 0.29395\n",
      "------------------------------------------\n",
      "L1 loss: 0.07081323117017746\n",
      "Training batch 81 with loss 0.26116\n",
      "------------------------------------------\n",
      "L1 loss: 0.08535333722829819\n",
      "Training batch 82 with loss 0.36132\n",
      "------------------------------------------\n",
      "L1 loss: 0.0797794982790947\n",
      "Training batch 83 with loss 0.27903\n",
      "------------------------------------------\n",
      "L1 loss: 0.06941672414541245\n",
      "Training batch 84 with loss 0.29984\n",
      "------------------------------------------\n",
      "L1 loss: 0.0668162927031517\n",
      "Training batch 85 with loss 0.27464\n",
      "------------------------------------------\n",
      "L1 loss: 0.08279995620250702\n",
      "Training batch 86 with loss 0.29619\n",
      "------------------------------------------\n",
      "L1 loss: 0.10083256661891937\n",
      "Training batch 87 with loss 0.39141\n",
      "------------------------------------------\n",
      "L1 loss: 0.07531333714723587\n",
      "Training batch 88 with loss 0.31633\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018332183361053\n",
      "Training batch 89 with loss 0.26075\n",
      "------------------------------------------\n",
      "L1 loss: 0.07221885025501251\n",
      "Training batch 90 with loss 0.28205\n",
      "------------------------------------------\n",
      "L1 loss: 0.06466903537511826\n",
      "Training batch 91 with loss 0.27453\n",
      "------------------------------------------\n",
      "L1 loss: 0.08939261734485626\n",
      "Training batch 92 with loss 0.29311\n",
      "------------------------------------------\n",
      "L1 loss: 0.058943141251802444\n",
      "Training batch 93 with loss 0.24737\n",
      "------------------------------------------\n",
      "L1 loss: 0.08236931264400482\n",
      "Training batch 94 with loss 0.29469\n",
      "------------------------------------------\n",
      "L1 loss: 0.06925869733095169\n",
      "Training batch 95 with loss 0.24135\n",
      "------------------------------------------\n",
      "L1 loss: 0.06129344180226326\n",
      "Training batch 96 with loss 0.28031\n",
      "------------------------------------------\n",
      "L1 loss: 0.08708639442920685\n",
      "Training batch 97 with loss 0.35928\n",
      "------------------------------------------\n",
      "L1 loss: 0.05459911376237869\n",
      "Training batch 98 with loss 0.37481\n",
      "------------------------------------------\n",
      "L1 loss: 0.0651501789689064\n",
      "Training batch 99 with loss 0.27167\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29762076154351236\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4836\n",
      "------------------------------------------\n",
      "L1 loss: 0.06347371637821198\n",
      "Training batch 0 with loss 0.26514\n",
      "------------------------------------------\n",
      "L1 loss: 0.08019133657217026\n",
      "Training batch 1 with loss 0.29036\n",
      "------------------------------------------\n",
      "L1 loss: 0.09167073667049408\n",
      "Training batch 2 with loss 0.32348\n",
      "------------------------------------------\n",
      "L1 loss: 0.07392490655183792\n",
      "Training batch 3 with loss 0.25070\n",
      "------------------------------------------\n",
      "L1 loss: 0.08260806649923325\n",
      "Training batch 4 with loss 0.33380\n",
      "------------------------------------------\n",
      "L1 loss: 0.06650714576244354\n",
      "Training batch 5 with loss 0.32330\n",
      "------------------------------------------\n",
      "L1 loss: 0.08165355771780014\n",
      "Training batch 6 with loss 0.27008\n",
      "------------------------------------------\n",
      "L1 loss: 0.08249158412218094\n",
      "Training batch 7 with loss 0.34255\n",
      "------------------------------------------\n",
      "L1 loss: 0.07883893698453903\n",
      "Training batch 8 with loss 0.32631\n",
      "------------------------------------------\n",
      "L1 loss: 0.06308264285326004\n",
      "Training batch 9 with loss 0.24782\n",
      "------------------------------------------\n",
      "L1 loss: 0.08243654668331146\n",
      "Training batch 10 with loss 0.28897\n",
      "------------------------------------------\n",
      "L1 loss: 0.06120600178837776\n",
      "Training batch 11 with loss 0.28309\n",
      "------------------------------------------\n",
      "L1 loss: 0.07562609016895294\n",
      "Training batch 12 with loss 0.31371\n",
      "------------------------------------------\n",
      "L1 loss: 0.07014425098896027\n",
      "Training batch 13 with loss 0.24660\n",
      "------------------------------------------\n",
      "L1 loss: 0.05223023518919945\n",
      "Training batch 14 with loss 0.25068\n",
      "------------------------------------------\n",
      "L1 loss: 0.08179158717393875\n",
      "Training batch 15 with loss 0.27485\n",
      "------------------------------------------\n",
      "L1 loss: 0.0638800859451294\n",
      "Training batch 16 with loss 0.27561\n",
      "------------------------------------------\n",
      "L1 loss: 0.08864887803792953\n",
      "Training batch 17 with loss 0.34424\n",
      "------------------------------------------\n",
      "L1 loss: 0.08104530721902847\n",
      "Training batch 18 with loss 0.31877\n",
      "------------------------------------------\n",
      "L1 loss: 0.06313066184520721\n",
      "Training batch 19 with loss 0.24345\n",
      "------------------------------------------\n",
      "L1 loss: 0.06857609748840332\n",
      "Training batch 20 with loss 0.60213\n",
      "------------------------------------------\n",
      "L1 loss: 0.07021038979291916\n",
      "Training batch 21 with loss 0.28868\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683988630771637\n",
      "Training batch 22 with loss 0.29255\n",
      "------------------------------------------\n",
      "L1 loss: 0.0897798016667366\n",
      "Training batch 23 with loss 0.36325\n",
      "------------------------------------------\n",
      "L1 loss: 0.06837800145149231\n",
      "Training batch 24 with loss 0.29255\n",
      "------------------------------------------\n",
      "L1 loss: 0.07209407538175583\n",
      "Training batch 25 with loss 0.29911\n",
      "------------------------------------------\n",
      "L1 loss: 0.07620706409215927\n",
      "Training batch 26 with loss 0.28082\n",
      "------------------------------------------\n",
      "L1 loss: 0.055084556341171265\n",
      "Training batch 27 with loss 0.28941\n",
      "------------------------------------------\n",
      "L1 loss: 0.05732858180999756\n",
      "Training batch 28 with loss 0.28849\n",
      "------------------------------------------\n",
      "L1 loss: 0.06189526617527008\n",
      "Training batch 29 with loss 0.26444\n",
      "------------------------------------------\n",
      "L1 loss: 0.06269890069961548\n",
      "Training batch 30 with loss 0.27372\n",
      "------------------------------------------\n",
      "L1 loss: 0.06445558369159698\n",
      "Training batch 31 with loss 0.30455\n",
      "------------------------------------------\n",
      "L1 loss: 0.07125579565763474\n",
      "Training batch 32 with loss 0.27726\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0633770003914833\n",
      "Training batch 33 with loss 0.26999\n",
      "------------------------------------------\n",
      "L1 loss: 0.07621393352746964\n",
      "Training batch 34 with loss 0.29422\n",
      "------------------------------------------\n",
      "L1 loss: 0.09585527330636978\n",
      "Training batch 35 with loss 0.34738\n",
      "------------------------------------------\n",
      "L1 loss: 0.08191880583763123\n",
      "Training batch 36 with loss 0.26655\n",
      "------------------------------------------\n",
      "L1 loss: 0.06859969347715378\n",
      "Training batch 37 with loss 0.26481\n",
      "------------------------------------------\n",
      "L1 loss: 0.06197616457939148\n",
      "Training batch 38 with loss 0.21918\n",
      "------------------------------------------\n",
      "L1 loss: 0.06797006726264954\n",
      "Training batch 39 with loss 0.29993\n",
      "------------------------------------------\n",
      "L1 loss: 0.07485426962375641\n",
      "Training batch 40 with loss 0.26641\n",
      "------------------------------------------\n",
      "L1 loss: 0.060011982917785645\n",
      "Training batch 41 with loss 0.29991\n",
      "------------------------------------------\n",
      "L1 loss: 0.0773598924279213\n",
      "Training batch 42 with loss 0.27892\n",
      "------------------------------------------\n",
      "L1 loss: 0.07076290249824524\n",
      "Training batch 43 with loss 0.28892\n",
      "------------------------------------------\n",
      "L1 loss: 0.0545697845518589\n",
      "Training batch 44 with loss 0.26358\n",
      "------------------------------------------\n",
      "L1 loss: 0.0927063599228859\n",
      "Training batch 45 with loss 0.36519\n",
      "------------------------------------------\n",
      "L1 loss: 0.07425486296415329\n",
      "Training batch 46 with loss 0.26799\n",
      "------------------------------------------\n",
      "L1 loss: 0.10917965322732925\n",
      "Training batch 47 with loss 0.36030\n",
      "------------------------------------------\n",
      "L1 loss: 0.047280922532081604\n",
      "Training batch 48 with loss 0.23938\n",
      "------------------------------------------\n",
      "L1 loss: 0.08140777796506882\n",
      "Training batch 49 with loss 0.37629\n",
      "------------------------------------------\n",
      "L1 loss: 0.07499228417873383\n",
      "Training batch 50 with loss 0.31147\n",
      "------------------------------------------\n",
      "L1 loss: 0.08384475111961365\n",
      "Training batch 51 with loss 0.33967\n",
      "------------------------------------------\n",
      "L1 loss: 0.09109325706958771\n",
      "Training batch 52 with loss 0.28545\n",
      "------------------------------------------\n",
      "L1 loss: 0.07831643521785736\n",
      "Training batch 53 with loss 0.29453\n",
      "------------------------------------------\n",
      "L1 loss: 0.07904588431119919\n",
      "Training batch 54 with loss 0.27820\n",
      "------------------------------------------\n",
      "L1 loss: 0.06338440626859665\n",
      "Training batch 55 with loss 0.31453\n",
      "------------------------------------------\n",
      "L1 loss: 0.04904737323522568\n",
      "Training batch 56 with loss 0.41594\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702425166964531\n",
      "Training batch 57 with loss 0.28630\n",
      "------------------------------------------\n",
      "L1 loss: 0.0686945840716362\n",
      "Training batch 58 with loss 0.26574\n",
      "------------------------------------------\n",
      "L1 loss: 0.07869122922420502\n",
      "Training batch 59 with loss 0.28392\n",
      "------------------------------------------\n",
      "L1 loss: 0.06474104523658752\n",
      "Training batch 60 with loss 0.26306\n",
      "------------------------------------------\n",
      "L1 loss: 0.09590927511453629\n",
      "Training batch 61 with loss 0.51043\n",
      "------------------------------------------\n",
      "L1 loss: 0.05716762691736221\n",
      "Training batch 62 with loss 0.25581\n",
      "------------------------------------------\n",
      "L1 loss: 0.06411068886518478\n",
      "Training batch 63 with loss 0.26144\n",
      "------------------------------------------\n",
      "L1 loss: 0.07291232794523239\n",
      "Training batch 64 with loss 0.28052\n",
      "------------------------------------------\n",
      "L1 loss: 0.061003394424915314\n",
      "Training batch 65 with loss 0.22695\n",
      "------------------------------------------\n",
      "L1 loss: 0.06548869609832764\n",
      "Training batch 66 with loss 0.29702\n",
      "------------------------------------------\n",
      "L1 loss: 0.06469806283712387\n",
      "Training batch 67 with loss 0.25792\n",
      "------------------------------------------\n",
      "L1 loss: 0.07409220933914185\n",
      "Training batch 68 with loss 0.30812\n",
      "------------------------------------------\n",
      "L1 loss: 0.050804026424884796\n",
      "Training batch 69 with loss 0.26323\n",
      "------------------------------------------\n",
      "L1 loss: 0.07927051931619644\n",
      "Training batch 70 with loss 0.31238\n",
      "------------------------------------------\n",
      "L1 loss: 0.07122648507356644\n",
      "Training batch 71 with loss 0.30014\n",
      "------------------------------------------\n",
      "L1 loss: 0.08069871366024017\n",
      "Training batch 72 with loss 0.28280\n",
      "------------------------------------------\n",
      "L1 loss: 0.07416993379592896\n",
      "Training batch 73 with loss 0.29273\n",
      "------------------------------------------\n",
      "L1 loss: 0.09465841203927994\n",
      "Training batch 74 with loss 0.30757\n",
      "------------------------------------------\n",
      "L1 loss: 0.07424300163984299\n",
      "Training batch 75 with loss 0.27499\n",
      "------------------------------------------\n",
      "L1 loss: 0.08655848354101181\n",
      "Training batch 76 with loss 0.29452\n",
      "------------------------------------------\n",
      "L1 loss: 0.08001163601875305\n",
      "Training batch 77 with loss 0.26679\n",
      "------------------------------------------\n",
      "L1 loss: 0.08101046830415726\n",
      "Training batch 78 with loss 0.29704\n",
      "------------------------------------------\n",
      "L1 loss: 0.10681092739105225\n",
      "Training batch 79 with loss 0.37209\n",
      "------------------------------------------\n",
      "L1 loss: 0.07514224946498871\n",
      "Training batch 80 with loss 0.28002\n",
      "------------------------------------------\n",
      "L1 loss: 0.08005084097385406\n",
      "Training batch 81 with loss 0.30567\n",
      "------------------------------------------\n",
      "L1 loss: 0.08658435940742493\n",
      "Training batch 82 with loss 0.36196\n",
      "------------------------------------------\n",
      "L1 loss: 0.07624703645706177\n",
      "Training batch 83 with loss 0.28439\n",
      "------------------------------------------\n",
      "L1 loss: 0.06773151457309723\n",
      "Training batch 84 with loss 0.30379\n",
      "------------------------------------------\n",
      "L1 loss: 0.06356582790613174\n",
      "Training batch 85 with loss 0.27298\n",
      "------------------------------------------\n",
      "L1 loss: 0.08042345941066742\n",
      "Training batch 86 with loss 0.29318\n",
      "------------------------------------------\n",
      "L1 loss: 0.1018359512090683\n",
      "Training batch 87 with loss 0.40028\n",
      "------------------------------------------\n",
      "L1 loss: 0.07969657331705093\n",
      "Training batch 88 with loss 0.34170\n",
      "------------------------------------------\n",
      "L1 loss: 0.06876867264509201\n",
      "Training batch 89 with loss 0.26222\n",
      "------------------------------------------\n",
      "L1 loss: 0.07348847389221191\n",
      "Training batch 90 with loss 0.26857\n",
      "------------------------------------------\n",
      "L1 loss: 0.0597979798913002\n",
      "Training batch 91 with loss 0.24468\n",
      "------------------------------------------\n",
      "L1 loss: 0.0903686136007309\n",
      "Training batch 92 with loss 0.29612\n",
      "------------------------------------------\n",
      "L1 loss: 0.06182432919740677\n",
      "Training batch 93 with loss 0.25968\n",
      "------------------------------------------\n",
      "L1 loss: 0.08605960756540298\n",
      "Training batch 94 with loss 0.29815\n",
      "------------------------------------------\n",
      "L1 loss: 0.07050076872110367\n",
      "Training batch 95 with loss 0.24537\n",
      "------------------------------------------\n",
      "L1 loss: 0.0675496980547905\n",
      "Training batch 96 with loss 0.28895\n",
      "------------------------------------------\n",
      "L1 loss: 0.08835820108652115\n",
      "Training batch 97 with loss 0.36798\n",
      "------------------------------------------\n",
      "L1 loss: 0.05873745679855347\n",
      "Training batch 98 with loss 0.25021\n",
      "------------------------------------------\n",
      "L1 loss: 0.06568622589111328\n",
      "Training batch 99 with loss 0.27190\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2979851704835892\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4837\n",
      "------------------------------------------\n",
      "L1 loss: 0.06255263090133667\n",
      "Training batch 0 with loss 0.25305\n",
      "------------------------------------------\n",
      "L1 loss: 0.0829993188381195\n",
      "Training batch 1 with loss 0.29629\n",
      "------------------------------------------\n",
      "L1 loss: 0.09240061044692993\n",
      "Training batch 2 with loss 0.31950\n",
      "------------------------------------------\n",
      "L1 loss: 0.07388582825660706\n",
      "Training batch 3 with loss 0.27185\n",
      "------------------------------------------\n",
      "L1 loss: 0.08535126596689224\n",
      "Training batch 4 with loss 0.34509\n",
      "------------------------------------------\n",
      "L1 loss: 0.06754980236291885\n",
      "Training batch 5 with loss 0.31786\n",
      "------------------------------------------\n",
      "L1 loss: 0.07782918214797974\n",
      "Training batch 6 with loss 0.26071\n",
      "------------------------------------------\n",
      "L1 loss: 0.0824996829032898\n",
      "Training batch 7 with loss 0.33264\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07907963544130325\n",
      "Training batch 8 with loss 0.31035\n",
      "------------------------------------------\n",
      "L1 loss: 0.058072593063116074\n",
      "Training batch 9 with loss 0.24278\n",
      "------------------------------------------\n",
      "L1 loss: 0.08278051763772964\n",
      "Training batch 10 with loss 0.31045\n",
      "------------------------------------------\n",
      "L1 loss: 0.05977954715490341\n",
      "Training batch 11 with loss 0.28045\n",
      "------------------------------------------\n",
      "L1 loss: 0.07766050100326538\n",
      "Training batch 12 with loss 0.31758\n",
      "------------------------------------------\n",
      "L1 loss: 0.0695706233382225\n",
      "Training batch 13 with loss 0.26381\n",
      "------------------------------------------\n",
      "L1 loss: 0.05323193594813347\n",
      "Training batch 14 with loss 0.25539\n",
      "------------------------------------------\n",
      "L1 loss: 0.08525796979665756\n",
      "Training batch 15 with loss 0.27514\n",
      "------------------------------------------\n",
      "L1 loss: 0.061816420406103134\n",
      "Training batch 16 with loss 0.26898\n",
      "------------------------------------------\n",
      "L1 loss: 0.08586104959249496\n",
      "Training batch 17 with loss 0.33438\n",
      "------------------------------------------\n",
      "L1 loss: 0.08095432072877884\n",
      "Training batch 18 with loss 0.31051\n",
      "------------------------------------------\n",
      "L1 loss: 0.06145000085234642\n",
      "Training batch 19 with loss 0.22655\n",
      "------------------------------------------\n",
      "L1 loss: 0.07149910181760788\n",
      "Training batch 20 with loss 0.31479\n",
      "------------------------------------------\n",
      "L1 loss: 0.07197441160678864\n",
      "Training batch 21 with loss 0.28509\n",
      "------------------------------------------\n",
      "L1 loss: 0.06936940550804138\n",
      "Training batch 22 with loss 0.30163\n",
      "------------------------------------------\n",
      "L1 loss: 0.08856072276830673\n",
      "Training batch 23 with loss 0.35524\n",
      "------------------------------------------\n",
      "L1 loss: 0.0676828995347023\n",
      "Training batch 24 with loss 0.30585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07620973885059357\n",
      "Training batch 25 with loss 0.29660\n",
      "------------------------------------------\n",
      "L1 loss: 0.07117758691310883\n",
      "Training batch 26 with loss 0.27525\n",
      "------------------------------------------\n",
      "L1 loss: 0.05652391538023949\n",
      "Training batch 27 with loss 0.29107\n",
      "------------------------------------------\n",
      "L1 loss: 0.056889843195676804\n",
      "Training batch 28 with loss 0.30337\n",
      "------------------------------------------\n",
      "L1 loss: 0.05945931747555733\n",
      "Training batch 29 with loss 0.25033\n",
      "------------------------------------------\n",
      "L1 loss: 0.06373081356287003\n",
      "Training batch 30 with loss 0.27680\n",
      "------------------------------------------\n",
      "L1 loss: 0.06533388048410416\n",
      "Training batch 31 with loss 0.28838\n",
      "------------------------------------------\n",
      "L1 loss: 0.06931881606578827\n",
      "Training batch 32 with loss 0.27479\n",
      "------------------------------------------\n",
      "L1 loss: 0.06299324333667755\n",
      "Training batch 33 with loss 0.25841\n",
      "------------------------------------------\n",
      "L1 loss: 0.062189482152462006\n",
      "Training batch 34 with loss 0.55698\n",
      "------------------------------------------\n",
      "L1 loss: 0.0969623401761055\n",
      "Training batch 35 with loss 0.35038\n",
      "------------------------------------------\n",
      "L1 loss: 0.08094081282615662\n",
      "Training batch 36 with loss 0.28423\n",
      "------------------------------------------\n",
      "L1 loss: 0.06851331144571304\n",
      "Training batch 37 with loss 0.26084\n",
      "------------------------------------------\n",
      "L1 loss: 0.06307763606309891\n",
      "Training batch 38 with loss 0.22541\n",
      "------------------------------------------\n",
      "L1 loss: 0.06820781528949738\n",
      "Training batch 39 with loss 0.29962\n",
      "------------------------------------------\n",
      "L1 loss: 0.07015440613031387\n",
      "Training batch 40 with loss 0.25849\n",
      "------------------------------------------\n",
      "L1 loss: 0.05868948996067047\n",
      "Training batch 41 with loss 0.29140\n",
      "------------------------------------------\n",
      "L1 loss: 0.08073616027832031\n",
      "Training batch 42 with loss 0.27672\n",
      "------------------------------------------\n",
      "L1 loss: 0.07030963152647018\n",
      "Training batch 43 with loss 0.29624\n",
      "------------------------------------------\n",
      "L1 loss: 0.0577867291867733\n",
      "Training batch 44 with loss 0.26826\n",
      "------------------------------------------\n",
      "L1 loss: 0.09275996685028076\n",
      "Training batch 45 with loss 0.35090\n",
      "------------------------------------------\n",
      "L1 loss: 0.07122816145420074\n",
      "Training batch 46 with loss 0.27320\n",
      "------------------------------------------\n",
      "L1 loss: 0.10998516529798508\n",
      "Training batch 47 with loss 0.36200\n",
      "------------------------------------------\n",
      "L1 loss: 0.045904893428087234\n",
      "Training batch 48 with loss 0.23501\n",
      "------------------------------------------\n",
      "L1 loss: 0.0826977789402008\n",
      "Training batch 49 with loss 0.35811\n",
      "------------------------------------------\n",
      "L1 loss: 0.07228482514619827\n",
      "Training batch 50 with loss 0.29978\n",
      "------------------------------------------\n",
      "L1 loss: 0.08047356456518173\n",
      "Training batch 51 with loss 0.32851\n",
      "------------------------------------------\n",
      "L1 loss: 0.0900595635175705\n",
      "Training batch 52 with loss 0.28548\n",
      "------------------------------------------\n",
      "L1 loss: 0.07640071958303452\n",
      "Training batch 53 with loss 0.28399\n",
      "------------------------------------------\n",
      "L1 loss: 0.07508759200572968\n",
      "Training batch 54 with loss 0.27868\n",
      "------------------------------------------\n",
      "L1 loss: 0.06060510128736496\n",
      "Training batch 55 with loss 0.30437\n",
      "------------------------------------------\n",
      "L1 loss: 0.05684368684887886\n",
      "Training batch 56 with loss 0.25190\n",
      "------------------------------------------\n",
      "L1 loss: 0.0672929584980011\n",
      "Training batch 57 with loss 0.28703\n",
      "------------------------------------------\n",
      "L1 loss: 0.06868699193000793\n",
      "Training batch 58 with loss 0.26159\n",
      "------------------------------------------\n",
      "L1 loss: 0.0792929083108902\n",
      "Training batch 59 with loss 0.28418\n",
      "------------------------------------------\n",
      "L1 loss: 0.06786617636680603\n",
      "Training batch 60 with loss 0.28786\n",
      "------------------------------------------\n",
      "L1 loss: 0.1016368716955185\n",
      "Training batch 61 with loss 0.34696\n",
      "------------------------------------------\n",
      "L1 loss: 0.05876501277089119\n",
      "Training batch 62 with loss 0.25938\n",
      "------------------------------------------\n",
      "L1 loss: 0.060473423451185226\n",
      "Training batch 63 with loss 0.27224\n",
      "------------------------------------------\n",
      "L1 loss: 0.07076200842857361\n",
      "Training batch 64 with loss 0.27488\n",
      "------------------------------------------\n",
      "L1 loss: 0.05986466631293297\n",
      "Training batch 65 with loss 0.22929\n",
      "------------------------------------------\n",
      "L1 loss: 0.06633234769105911\n",
      "Training batch 66 with loss 0.26122\n",
      "------------------------------------------\n",
      "L1 loss: 0.06198110803961754\n",
      "Training batch 67 with loss 0.26653\n",
      "------------------------------------------\n",
      "L1 loss: 0.07278350740671158\n",
      "Training batch 68 with loss 0.29592\n",
      "------------------------------------------\n",
      "L1 loss: 0.0488232783973217\n",
      "Training batch 69 with loss 0.25884\n",
      "------------------------------------------\n",
      "L1 loss: 0.07837755978107452\n",
      "Training batch 70 with loss 0.31554\n",
      "------------------------------------------\n",
      "L1 loss: 0.07207977771759033\n",
      "Training batch 71 with loss 0.29739\n",
      "------------------------------------------\n",
      "L1 loss: 0.08431949466466904\n",
      "Training batch 72 with loss 0.29362\n",
      "------------------------------------------\n",
      "L1 loss: 0.06980156153440475\n",
      "Training batch 73 with loss 0.28984\n",
      "------------------------------------------\n",
      "L1 loss: 0.09597600251436234\n",
      "Training batch 74 with loss 0.32051\n",
      "------------------------------------------\n",
      "L1 loss: 0.061856549233198166\n",
      "Training batch 75 with loss 0.51212\n",
      "------------------------------------------\n",
      "L1 loss: 0.08626792579889297\n",
      "Training batch 76 with loss 0.29488\n",
      "------------------------------------------\n",
      "L1 loss: 0.08038587123155594\n",
      "Training batch 77 with loss 0.26463\n",
      "------------------------------------------\n",
      "L1 loss: 0.08505571633577347\n",
      "Training batch 78 with loss 0.31363\n",
      "------------------------------------------\n",
      "L1 loss: 0.10350485891103745\n",
      "Training batch 79 with loss 0.35486\n",
      "------------------------------------------\n",
      "L1 loss: 0.07519534975290298\n",
      "Training batch 80 with loss 0.27584\n",
      "------------------------------------------\n",
      "L1 loss: 0.07758482545614243\n",
      "Training batch 81 with loss 0.28926\n",
      "------------------------------------------\n",
      "L1 loss: 0.08304855972528458\n",
      "Training batch 82 with loss 0.38490\n",
      "------------------------------------------\n",
      "L1 loss: 0.07513339817523956\n",
      "Training batch 83 with loss 0.43604\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07065151631832123\n",
      "Training batch 84 with loss 0.30386\n",
      "------------------------------------------\n",
      "L1 loss: 0.06114085018634796\n",
      "Training batch 85 with loss 0.28818\n",
      "------------------------------------------\n",
      "L1 loss: 0.05591161176562309\n",
      "Training batch 86 with loss 0.23568\n",
      "------------------------------------------\n",
      "L1 loss: 0.102146677672863\n",
      "Training batch 87 with loss 0.38994\n",
      "------------------------------------------\n",
      "L1 loss: 0.07630262523889542\n",
      "Training batch 88 with loss 0.31705\n",
      "------------------------------------------\n",
      "L1 loss: 0.07007668167352676\n",
      "Training batch 89 with loss 0.27006\n",
      "------------------------------------------\n",
      "L1 loss: 0.0733228549361229\n",
      "Training batch 90 with loss 0.26355\n",
      "------------------------------------------\n",
      "L1 loss: 0.06494063138961792\n",
      "Training batch 91 with loss 0.25841\n",
      "------------------------------------------\n",
      "L1 loss: 0.09395015239715576\n",
      "Training batch 92 with loss 0.30452\n",
      "------------------------------------------\n",
      "L1 loss: 0.0739908367395401\n",
      "Training batch 93 with loss 0.28546\n",
      "------------------------------------------\n",
      "L1 loss: 0.08411487191915512\n",
      "Training batch 94 with loss 0.29213\n",
      "------------------------------------------\n",
      "L1 loss: 0.07233723253011703\n",
      "Training batch 95 with loss 0.25090\n",
      "------------------------------------------\n",
      "L1 loss: 0.07216158509254456\n",
      "Training batch 96 with loss 0.31052\n",
      "------------------------------------------\n",
      "L1 loss: 0.08540340512990952\n",
      "Training batch 97 with loss 0.37082\n",
      "------------------------------------------\n",
      "L1 loss: 0.05796685814857483\n",
      "Training batch 98 with loss 0.23906\n",
      "------------------------------------------\n",
      "L1 loss: 0.064916230738163\n",
      "Training batch 99 with loss 0.26033\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29720960825681686\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4838\n",
      "------------------------------------------\n",
      "L1 loss: 0.06218351796269417\n",
      "Training batch 0 with loss 0.25816\n",
      "------------------------------------------\n",
      "L1 loss: 0.08386512100696564\n",
      "Training batch 1 with loss 0.29482\n",
      "------------------------------------------\n",
      "L1 loss: 0.09452076256275177\n",
      "Training batch 2 with loss 0.32404\n",
      "------------------------------------------\n",
      "L1 loss: 0.07972455769777298\n",
      "Training batch 3 with loss 0.28007\n",
      "------------------------------------------\n",
      "L1 loss: 0.0858754962682724\n",
      "Training batch 4 with loss 0.33199\n",
      "------------------------------------------\n",
      "L1 loss: 0.06583472341299057\n",
      "Training batch 5 with loss 0.32680\n",
      "------------------------------------------\n",
      "L1 loss: 0.0778135433793068\n",
      "Training batch 6 with loss 0.26425\n",
      "------------------------------------------\n",
      "L1 loss: 0.08358598500490189\n",
      "Training batch 7 with loss 0.32259\n",
      "------------------------------------------\n",
      "L1 loss: 0.07831969857215881\n",
      "Training batch 8 with loss 0.32775\n",
      "------------------------------------------\n",
      "L1 loss: 0.05833940580487251\n",
      "Training batch 9 with loss 0.24436\n",
      "------------------------------------------\n",
      "L1 loss: 0.08194965124130249\n",
      "Training batch 10 with loss 0.30488\n",
      "------------------------------------------\n",
      "L1 loss: 0.06120379641652107\n",
      "Training batch 11 with loss 0.29687\n",
      "------------------------------------------\n",
      "L1 loss: 0.07766948640346527\n",
      "Training batch 12 with loss 0.33025\n",
      "------------------------------------------\n",
      "L1 loss: 0.06750238686800003\n",
      "Training batch 13 with loss 0.23661\n",
      "------------------------------------------\n",
      "L1 loss: 0.052722278982400894\n",
      "Training batch 14 with loss 0.25233\n",
      "------------------------------------------\n",
      "L1 loss: 0.08463769406080246\n",
      "Training batch 15 with loss 0.28614\n",
      "------------------------------------------\n",
      "L1 loss: 0.06376735866069794\n",
      "Training batch 16 with loss 0.27289\n",
      "------------------------------------------\n",
      "L1 loss: 0.089898481965065\n",
      "Training batch 17 with loss 0.35226\n",
      "------------------------------------------\n",
      "L1 loss: 0.07981513440608978\n",
      "Training batch 18 with loss 0.28654\n",
      "------------------------------------------\n",
      "L1 loss: 0.05965438485145569\n",
      "Training batch 19 with loss 0.23577\n",
      "------------------------------------------\n",
      "L1 loss: 0.07392201572656631\n",
      "Training batch 20 with loss 0.31902\n",
      "------------------------------------------\n",
      "L1 loss: 0.07209441810846329\n",
      "Training batch 21 with loss 0.27167\n",
      "------------------------------------------\n",
      "L1 loss: 0.06844732910394669\n",
      "Training batch 22 with loss 0.27405\n",
      "------------------------------------------\n",
      "L1 loss: 0.08542010933160782\n",
      "Training batch 23 with loss 0.36430\n",
      "------------------------------------------\n",
      "L1 loss: 0.06875152885913849\n",
      "Training batch 24 with loss 0.28663\n",
      "------------------------------------------\n",
      "L1 loss: 0.07691255956888199\n",
      "Training batch 25 with loss 0.29485\n",
      "------------------------------------------\n",
      "L1 loss: 0.0744270384311676\n",
      "Training batch 26 with loss 0.27293\n",
      "------------------------------------------\n",
      "L1 loss: 0.048122044652700424\n",
      "Training batch 27 with loss 0.39820\n",
      "------------------------------------------\n",
      "L1 loss: 0.05785374343395233\n",
      "Training batch 28 with loss 0.30205\n",
      "------------------------------------------\n",
      "L1 loss: 0.06388261914253235\n",
      "Training batch 29 with loss 0.26930\n",
      "------------------------------------------\n",
      "L1 loss: 0.06071097031235695\n",
      "Training batch 30 with loss 0.26905\n",
      "------------------------------------------\n",
      "L1 loss: 0.062020521610975266\n",
      "Training batch 31 with loss 0.28511\n",
      "------------------------------------------\n",
      "L1 loss: 0.06171387434005737\n",
      "Training batch 32 with loss 0.25312\n",
      "------------------------------------------\n",
      "L1 loss: 0.06652738153934479\n",
      "Training batch 33 with loss 0.25648\n",
      "------------------------------------------\n",
      "L1 loss: 0.0728406086564064\n",
      "Training batch 34 with loss 0.29686\n",
      "------------------------------------------\n",
      "L1 loss: 0.09373458474874496\n",
      "Training batch 35 with loss 0.32473\n",
      "------------------------------------------\n",
      "L1 loss: 0.08929439634084702\n",
      "Training batch 36 with loss 0.27982\n",
      "------------------------------------------\n",
      "L1 loss: 0.06711805611848831\n",
      "Training batch 37 with loss 0.25402\n",
      "------------------------------------------\n",
      "L1 loss: 0.06289227306842804\n",
      "Training batch 38 with loss 0.21911\n",
      "------------------------------------------\n",
      "L1 loss: 0.06927676498889923\n",
      "Training batch 39 with loss 0.30633\n",
      "------------------------------------------\n",
      "L1 loss: 0.07502483576536179\n",
      "Training batch 40 with loss 0.25070\n",
      "------------------------------------------\n",
      "L1 loss: 0.05738120153546333\n",
      "Training batch 41 with loss 0.28169\n",
      "------------------------------------------\n",
      "L1 loss: 0.07586455345153809\n",
      "Training batch 42 with loss 0.29797\n",
      "------------------------------------------\n",
      "L1 loss: 0.0711398497223854\n",
      "Training batch 43 with loss 0.29970\n",
      "------------------------------------------\n",
      "L1 loss: 0.05547919124364853\n",
      "Training batch 44 with loss 0.27583\n",
      "------------------------------------------\n",
      "L1 loss: 0.09253855794668198\n",
      "Training batch 45 with loss 0.35817\n",
      "------------------------------------------\n",
      "L1 loss: 0.07420600205659866\n",
      "Training batch 46 with loss 0.26607\n",
      "------------------------------------------\n",
      "L1 loss: 0.10786989331245422\n",
      "Training batch 47 with loss 0.35745\n",
      "------------------------------------------\n",
      "L1 loss: 0.04369036480784416\n",
      "Training batch 48 with loss 0.23309\n",
      "------------------------------------------\n",
      "L1 loss: 0.07790821045637131\n",
      "Training batch 49 with loss 0.35576\n",
      "------------------------------------------\n",
      "L1 loss: 0.07397118210792542\n",
      "Training batch 50 with loss 0.28739\n",
      "------------------------------------------\n",
      "L1 loss: 0.08483577519655228\n",
      "Training batch 51 with loss 0.34463\n",
      "------------------------------------------\n",
      "L1 loss: 0.09009342640638351\n",
      "Training batch 52 with loss 0.29569\n",
      "------------------------------------------\n",
      "L1 loss: 0.08033227920532227\n",
      "Training batch 53 with loss 0.28487\n",
      "------------------------------------------\n",
      "L1 loss: 0.07673349976539612\n",
      "Training batch 54 with loss 0.28056\n",
      "------------------------------------------\n",
      "L1 loss: 0.061086803674697876\n",
      "Training batch 55 with loss 0.31477\n",
      "------------------------------------------\n",
      "L1 loss: 0.05711499974131584\n",
      "Training batch 56 with loss 0.26626\n",
      "------------------------------------------\n",
      "L1 loss: 0.06537441164255142\n",
      "Training batch 57 with loss 0.27370\n",
      "------------------------------------------\n",
      "L1 loss: 0.06568408757448196\n",
      "Training batch 58 with loss 0.25944\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08000229299068451\n",
      "Training batch 59 with loss 0.28744\n",
      "------------------------------------------\n",
      "L1 loss: 0.06440582871437073\n",
      "Training batch 60 with loss 0.27839\n",
      "------------------------------------------\n",
      "L1 loss: 0.09378643333911896\n",
      "Training batch 61 with loss 0.34201\n",
      "------------------------------------------\n",
      "L1 loss: 0.0549248643219471\n",
      "Training batch 62 with loss 0.25139\n",
      "------------------------------------------\n",
      "L1 loss: 0.06402743607759476\n",
      "Training batch 63 with loss 0.25423\n",
      "------------------------------------------\n",
      "L1 loss: 0.07061432301998138\n",
      "Training batch 64 with loss 0.27565\n",
      "------------------------------------------\n",
      "L1 loss: 0.0599096305668354\n",
      "Training batch 65 with loss 0.22369\n",
      "------------------------------------------\n",
      "L1 loss: 0.06282320618629456\n",
      "Training batch 66 with loss 0.26428\n",
      "------------------------------------------\n",
      "L1 loss: 0.0650738999247551\n",
      "Training batch 67 with loss 0.26362\n",
      "------------------------------------------\n",
      "L1 loss: 0.07301349937915802\n",
      "Training batch 68 with loss 0.31619\n",
      "------------------------------------------\n",
      "L1 loss: 0.051612455397844315\n",
      "Training batch 69 with loss 0.26777\n",
      "------------------------------------------\n",
      "L1 loss: 0.07634139806032181\n",
      "Training batch 70 with loss 0.29550\n",
      "------------------------------------------\n",
      "L1 loss: 0.07243705540895462\n",
      "Training batch 71 with loss 0.30724\n",
      "------------------------------------------\n",
      "L1 loss: 0.08497000485658646\n",
      "Training batch 72 with loss 0.30624\n",
      "------------------------------------------\n",
      "L1 loss: 0.07104546576738358\n",
      "Training batch 73 with loss 0.28768\n",
      "------------------------------------------\n",
      "L1 loss: 0.09834972769021988\n",
      "Training batch 74 with loss 0.31306\n",
      "------------------------------------------\n",
      "L1 loss: 0.07544926553964615\n",
      "Training batch 75 with loss 0.28206\n",
      "------------------------------------------\n",
      "L1 loss: 0.08647428452968597\n",
      "Training batch 76 with loss 0.29155\n",
      "------------------------------------------\n",
      "L1 loss: 0.0789848119020462\n",
      "Training batch 77 with loss 0.28436\n",
      "------------------------------------------\n",
      "L1 loss: 0.07943903654813766\n",
      "Training batch 78 with loss 0.31285\n",
      "------------------------------------------\n",
      "L1 loss: 0.10581214725971222\n",
      "Training batch 79 with loss 0.36823\n",
      "------------------------------------------\n",
      "L1 loss: 0.07537568360567093\n",
      "Training batch 80 with loss 0.27811\n",
      "------------------------------------------\n",
      "L1 loss: 0.0624258778989315\n",
      "Training batch 81 with loss 0.42371\n",
      "------------------------------------------\n",
      "L1 loss: 0.08280906826257706\n",
      "Training batch 82 with loss 0.35367\n",
      "------------------------------------------\n",
      "L1 loss: 0.07576776295900345\n",
      "Training batch 83 with loss 0.27471\n",
      "------------------------------------------\n",
      "L1 loss: 0.06886443495750427\n",
      "Training batch 84 with loss 0.29883\n",
      "------------------------------------------\n",
      "L1 loss: 0.0677361711859703\n",
      "Training batch 85 with loss 0.29720\n",
      "------------------------------------------\n",
      "L1 loss: 0.07962818443775177\n",
      "Training batch 86 with loss 0.30130\n",
      "------------------------------------------\n",
      "L1 loss: 0.10082217305898666\n",
      "Training batch 87 with loss 0.40110\n",
      "------------------------------------------\n",
      "L1 loss: 0.07721193134784698\n",
      "Training batch 88 with loss 0.32891\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709972158074379\n",
      "Training batch 89 with loss 0.27420\n",
      "------------------------------------------\n",
      "L1 loss: 0.07305645197629929\n",
      "Training batch 90 with loss 0.28764\n",
      "------------------------------------------\n",
      "L1 loss: 0.0604437030851841\n",
      "Training batch 91 with loss 0.25001\n",
      "------------------------------------------\n",
      "L1 loss: 0.09241002053022385\n",
      "Training batch 92 with loss 0.31215\n",
      "------------------------------------------\n",
      "L1 loss: 0.060583312064409256\n",
      "Training batch 93 with loss 0.24321\n",
      "------------------------------------------\n",
      "L1 loss: 0.08423107117414474\n",
      "Training batch 94 with loss 0.29650\n",
      "------------------------------------------\n",
      "L1 loss: 0.07011586427688599\n",
      "Training batch 95 with loss 0.26541\n",
      "------------------------------------------\n",
      "L1 loss: 0.0634673461318016\n",
      "Training batch 96 with loss 0.26937\n",
      "------------------------------------------\n",
      "L1 loss: 0.08640294522047043\n",
      "Training batch 97 with loss 0.35687\n",
      "------------------------------------------\n",
      "L1 loss: 0.056470632553100586\n",
      "Training batch 98 with loss 0.25265\n",
      "------------------------------------------\n",
      "L1 loss: 0.06327984482049942\n",
      "Training batch 99 with loss 0.26648\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29316150456666945\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4839\n",
      "------------------------------------------\n",
      "L1 loss: 0.06377512216567993\n",
      "Training batch 0 with loss 0.25175\n",
      "------------------------------------------\n",
      "L1 loss: 0.08228873461484909\n",
      "Training batch 1 with loss 0.29309\n",
      "------------------------------------------\n",
      "L1 loss: 0.09352660924196243\n",
      "Training batch 2 with loss 0.33342\n",
      "------------------------------------------\n",
      "L1 loss: 0.07537847012281418\n",
      "Training batch 3 with loss 0.26949\n",
      "------------------------------------------\n",
      "L1 loss: 0.08375129103660583\n",
      "Training batch 4 with loss 0.32123\n",
      "------------------------------------------\n",
      "L1 loss: 0.06466490030288696\n",
      "Training batch 5 with loss 0.31546\n",
      "------------------------------------------\n",
      "L1 loss: 0.07880605757236481\n",
      "Training batch 6 with loss 0.27427\n",
      "------------------------------------------\n",
      "L1 loss: 0.08359692245721817\n",
      "Training batch 7 with loss 0.31958\n",
      "------------------------------------------\n",
      "L1 loss: 0.08040516823530197\n",
      "Training batch 8 with loss 0.33160\n",
      "------------------------------------------\n",
      "L1 loss: 0.061880338937044144\n",
      "Training batch 9 with loss 0.25293\n",
      "------------------------------------------\n",
      "L1 loss: 0.08244959264993668\n",
      "Training batch 10 with loss 0.29639\n",
      "------------------------------------------\n",
      "L1 loss: 0.06049313396215439\n",
      "Training batch 11 with loss 0.27286\n",
      "------------------------------------------\n",
      "L1 loss: 0.07635828107595444\n",
      "Training batch 12 with loss 0.31364\n",
      "------------------------------------------\n",
      "L1 loss: 0.07054177671670914\n",
      "Training batch 13 with loss 0.26856\n",
      "------------------------------------------\n",
      "L1 loss: 0.05241124704480171\n",
      "Training batch 14 with loss 0.24994\n",
      "------------------------------------------\n",
      "L1 loss: 0.08437198400497437\n",
      "Training batch 15 with loss 0.27319\n",
      "------------------------------------------\n",
      "L1 loss: 0.061862099915742874\n",
      "Training batch 16 with loss 0.25249\n",
      "------------------------------------------\n",
      "L1 loss: 0.08819852769374847\n",
      "Training batch 17 with loss 0.35188\n",
      "------------------------------------------\n",
      "L1 loss: 0.08126391470432281\n",
      "Training batch 18 with loss 0.30045\n",
      "------------------------------------------\n",
      "L1 loss: 0.05889580771327019\n",
      "Training batch 19 with loss 0.23711\n",
      "------------------------------------------\n",
      "L1 loss: 0.07451552152633667\n",
      "Training batch 20 with loss 0.32122\n",
      "------------------------------------------\n",
      "L1 loss: 0.07233773916959763\n",
      "Training batch 21 with loss 0.30886\n",
      "------------------------------------------\n",
      "L1 loss: 0.0679718554019928\n",
      "Training batch 22 with loss 0.28310\n",
      "------------------------------------------\n",
      "L1 loss: 0.08613424748182297\n",
      "Training batch 23 with loss 0.34605\n",
      "------------------------------------------\n",
      "L1 loss: 0.06841710954904556\n",
      "Training batch 24 with loss 0.30679\n",
      "------------------------------------------\n",
      "L1 loss: 0.07214783132076263\n",
      "Training batch 25 with loss 0.29992\n",
      "------------------------------------------\n",
      "L1 loss: 0.07708439975976944\n",
      "Training batch 26 with loss 0.28268\n",
      "------------------------------------------\n",
      "L1 loss: 0.05525942146778107\n",
      "Training batch 27 with loss 0.29523\n",
      "------------------------------------------\n",
      "L1 loss: 0.05817653238773346\n",
      "Training batch 28 with loss 0.28199\n",
      "------------------------------------------\n",
      "L1 loss: 0.05444542318582535\n",
      "Training batch 29 with loss 0.23740\n",
      "------------------------------------------\n",
      "L1 loss: 0.05968165397644043\n",
      "Training batch 30 with loss 0.27633\n",
      "------------------------------------------\n",
      "L1 loss: 0.06152626499533653\n",
      "Training batch 31 with loss 0.31246\n",
      "------------------------------------------\n",
      "L1 loss: 0.07068698853254318\n",
      "Training batch 32 with loss 0.28333\n",
      "------------------------------------------\n",
      "L1 loss: 0.06295003741979599\n",
      "Training batch 33 with loss 0.26071\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0738293007016182\n",
      "Training batch 34 with loss 0.31390\n",
      "------------------------------------------\n",
      "L1 loss: 0.09054245799779892\n",
      "Training batch 35 with loss 0.33843\n",
      "------------------------------------------\n",
      "L1 loss: 0.08029112964868546\n",
      "Training batch 36 with loss 0.29128\n",
      "------------------------------------------\n",
      "L1 loss: 0.069415383040905\n",
      "Training batch 37 with loss 0.26944\n",
      "------------------------------------------\n",
      "L1 loss: 0.0612834207713604\n",
      "Training batch 38 with loss 0.22096\n",
      "------------------------------------------\n",
      "L1 loss: 0.06362989544868469\n",
      "Training batch 39 with loss 0.28894\n",
      "------------------------------------------\n",
      "L1 loss: 0.06796399503946304\n",
      "Training batch 40 with loss 0.25615\n",
      "------------------------------------------\n",
      "L1 loss: 0.05904482677578926\n",
      "Training batch 41 with loss 0.28556\n",
      "------------------------------------------\n",
      "L1 loss: 0.07862092554569244\n",
      "Training batch 42 with loss 0.26923\n",
      "------------------------------------------\n",
      "L1 loss: 0.07224982231855392\n",
      "Training batch 43 with loss 0.28888\n",
      "------------------------------------------\n",
      "L1 loss: 0.05463642627000809\n",
      "Training batch 44 with loss 0.26917\n",
      "------------------------------------------\n",
      "L1 loss: 0.09276522696018219\n",
      "Training batch 45 with loss 0.36715\n",
      "------------------------------------------\n",
      "L1 loss: 0.07216330617666245\n",
      "Training batch 46 with loss 0.26882\n",
      "------------------------------------------\n",
      "L1 loss: 0.10922056436538696\n",
      "Training batch 47 with loss 0.37034\n",
      "------------------------------------------\n",
      "L1 loss: 0.04953509569168091\n",
      "Training batch 48 with loss 0.25976\n",
      "------------------------------------------\n",
      "L1 loss: 0.07804059982299805\n",
      "Training batch 49 with loss 0.34139\n",
      "------------------------------------------\n",
      "L1 loss: 0.07235225290060043\n",
      "Training batch 50 with loss 0.28740\n",
      "------------------------------------------\n",
      "L1 loss: 0.08163933455944061\n",
      "Training batch 51 with loss 0.32514\n",
      "------------------------------------------\n",
      "L1 loss: 0.07310312986373901\n",
      "Training batch 52 with loss 0.33260\n",
      "------------------------------------------\n",
      "L1 loss: 0.08064580708742142\n",
      "Training batch 53 with loss 0.29472\n",
      "------------------------------------------\n",
      "L1 loss: 0.0770668163895607\n",
      "Training batch 54 with loss 0.26530\n",
      "------------------------------------------\n",
      "L1 loss: 0.06395532190799713\n",
      "Training batch 55 with loss 0.35227\n",
      "------------------------------------------\n",
      "L1 loss: 0.05900673195719719\n",
      "Training batch 56 with loss 0.25162\n",
      "------------------------------------------\n",
      "L1 loss: 0.06858232617378235\n",
      "Training batch 57 with loss 0.27737\n",
      "------------------------------------------\n",
      "L1 loss: 0.06646274775266647\n",
      "Training batch 58 with loss 0.26468\n",
      "------------------------------------------\n",
      "L1 loss: 0.08223745971918106\n",
      "Training batch 59 with loss 0.29139\n",
      "------------------------------------------\n",
      "L1 loss: 0.06319035589694977\n",
      "Training batch 60 with loss 0.27449\n",
      "------------------------------------------\n",
      "L1 loss: 0.09363345801830292\n",
      "Training batch 61 with loss 0.34469\n",
      "------------------------------------------\n",
      "L1 loss: 0.057494085282087326\n",
      "Training batch 62 with loss 0.24918\n",
      "------------------------------------------\n",
      "L1 loss: 0.06406545639038086\n",
      "Training batch 63 with loss 0.27390\n",
      "------------------------------------------\n",
      "L1 loss: 0.07083339244127274\n",
      "Training batch 64 with loss 0.29745\n",
      "------------------------------------------\n",
      "L1 loss: 0.06248605623841286\n",
      "Training batch 65 with loss 0.23216\n",
      "------------------------------------------\n",
      "L1 loss: 0.06596475839614868\n",
      "Training batch 66 with loss 0.28014\n",
      "------------------------------------------\n",
      "L1 loss: 0.06270720809698105\n",
      "Training batch 67 with loss 0.26516\n",
      "------------------------------------------\n",
      "L1 loss: 0.07133489847183228\n",
      "Training batch 68 with loss 0.30046\n",
      "------------------------------------------\n",
      "L1 loss: 0.0525650791823864\n",
      "Training batch 69 with loss 0.26585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07732068747282028\n",
      "Training batch 70 with loss 0.31118\n",
      "------------------------------------------\n",
      "L1 loss: 0.07266462594270706\n",
      "Training batch 71 with loss 0.32871\n",
      "------------------------------------------\n",
      "L1 loss: 0.08142969757318497\n",
      "Training batch 72 with loss 0.29544\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702415183186531\n",
      "Training batch 73 with loss 0.28707\n",
      "------------------------------------------\n",
      "L1 loss: 0.09893761575222015\n",
      "Training batch 74 with loss 0.32657\n",
      "------------------------------------------\n",
      "L1 loss: 0.07298345863819122\n",
      "Training batch 75 with loss 0.29235\n",
      "------------------------------------------\n",
      "L1 loss: 0.08670643717050552\n",
      "Training batch 76 with loss 0.29792\n",
      "------------------------------------------\n",
      "L1 loss: 0.07812228798866272\n",
      "Training batch 77 with loss 0.26974\n",
      "------------------------------------------\n",
      "L1 loss: 0.08288567513227463\n",
      "Training batch 78 with loss 0.30569\n",
      "------------------------------------------\n",
      "L1 loss: 0.10804944485425949\n",
      "Training batch 79 with loss 0.37676\n",
      "------------------------------------------\n",
      "L1 loss: 0.07630609720945358\n",
      "Training batch 80 with loss 0.28069\n",
      "------------------------------------------\n",
      "L1 loss: 0.076864093542099\n",
      "Training batch 81 with loss 0.27951\n",
      "------------------------------------------\n",
      "L1 loss: 0.07529500871896744\n",
      "Training batch 82 with loss 0.50777\n",
      "------------------------------------------\n",
      "L1 loss: 0.07568610459566116\n",
      "Training batch 83 with loss 0.28576\n",
      "------------------------------------------\n",
      "L1 loss: 0.06986946612596512\n",
      "Training batch 84 with loss 0.32133\n",
      "------------------------------------------\n",
      "L1 loss: 0.06567977368831635\n",
      "Training batch 85 with loss 0.27291\n",
      "------------------------------------------\n",
      "L1 loss: 0.07935073971748352\n",
      "Training batch 86 with loss 0.28660\n",
      "------------------------------------------\n",
      "L1 loss: 0.09877979755401611\n",
      "Training batch 87 with loss 0.41358\n",
      "------------------------------------------\n",
      "L1 loss: 0.07865932583808899\n",
      "Training batch 88 with loss 0.31431\n",
      "------------------------------------------\n",
      "L1 loss: 0.06961777806282043\n",
      "Training batch 89 with loss 0.28149\n",
      "------------------------------------------\n",
      "L1 loss: 0.07305093109607697\n",
      "Training batch 90 with loss 0.27991\n",
      "------------------------------------------\n",
      "L1 loss: 0.06195390596985817\n",
      "Training batch 91 with loss 0.26413\n",
      "------------------------------------------\n",
      "L1 loss: 0.09128434956073761\n",
      "Training batch 92 with loss 0.30384\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683954581618309\n",
      "Training batch 93 with loss 0.25900\n",
      "------------------------------------------\n",
      "L1 loss: 0.08626775443553925\n",
      "Training batch 94 with loss 0.32140\n",
      "------------------------------------------\n",
      "L1 loss: 0.06941650062799454\n",
      "Training batch 95 with loss 0.26227\n",
      "------------------------------------------\n",
      "L1 loss: 0.06661615520715714\n",
      "Training batch 96 with loss 0.30295\n",
      "------------------------------------------\n",
      "L1 loss: 0.08601407706737518\n",
      "Training batch 97 with loss 0.35417\n",
      "------------------------------------------\n",
      "L1 loss: 0.05709116533398628\n",
      "Training batch 98 with loss 0.24353\n",
      "------------------------------------------\n",
      "L1 loss: 0.061771586537361145\n",
      "Training batch 99 with loss 0.26883\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.294962008446455\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4840\n",
      "------------------------------------------\n",
      "L1 loss: 0.06350238621234894\n",
      "Training batch 0 with loss 0.26367\n",
      "------------------------------------------\n",
      "L1 loss: 0.08413133025169373\n",
      "Training batch 1 with loss 0.28850\n",
      "------------------------------------------\n",
      "L1 loss: 0.09494089335203171\n",
      "Training batch 2 with loss 0.31775\n",
      "------------------------------------------\n",
      "L1 loss: 0.07026483863592148\n",
      "Training batch 3 with loss 0.25001\n",
      "------------------------------------------\n",
      "L1 loss: 0.08513763546943665\n",
      "Training batch 4 with loss 0.31945\n",
      "------------------------------------------\n",
      "L1 loss: 0.07003255188465118\n",
      "Training batch 5 with loss 0.35892\n",
      "------------------------------------------\n",
      "L1 loss: 0.07447914779186249\n",
      "Training batch 6 with loss 0.37085\n",
      "------------------------------------------\n",
      "L1 loss: 0.0843047946691513\n",
      "Training batch 7 with loss 0.33190\n",
      "------------------------------------------\n",
      "L1 loss: 0.07897666841745377\n",
      "Training batch 8 with loss 0.33600\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06086720526218414\n",
      "Training batch 9 with loss 0.25119\n",
      "------------------------------------------\n",
      "L1 loss: 0.08072041720151901\n",
      "Training batch 10 with loss 0.28760\n",
      "------------------------------------------\n",
      "L1 loss: 0.05688869208097458\n",
      "Training batch 11 with loss 0.28754\n",
      "------------------------------------------\n",
      "L1 loss: 0.07239965349435806\n",
      "Training batch 12 with loss 0.31355\n",
      "------------------------------------------\n",
      "L1 loss: 0.07054854184389114\n",
      "Training batch 13 with loss 0.23836\n",
      "------------------------------------------\n",
      "L1 loss: 0.05061912164092064\n",
      "Training batch 14 with loss 0.24310\n",
      "------------------------------------------\n",
      "L1 loss: 0.08431711792945862\n",
      "Training batch 15 with loss 0.27259\n",
      "------------------------------------------\n",
      "L1 loss: 0.06357725709676743\n",
      "Training batch 16 with loss 0.27097\n",
      "------------------------------------------\n",
      "L1 loss: 0.08576592803001404\n",
      "Training batch 17 with loss 0.33769\n",
      "------------------------------------------\n",
      "L1 loss: 0.07961662113666534\n",
      "Training batch 18 with loss 0.30777\n",
      "------------------------------------------\n",
      "L1 loss: 0.06321585923433304\n",
      "Training batch 19 with loss 0.23747\n",
      "------------------------------------------\n",
      "L1 loss: 0.07156416028738022\n",
      "Training batch 20 with loss 0.32243\n",
      "------------------------------------------\n",
      "L1 loss: 0.07164905220270157\n",
      "Training batch 21 with loss 0.28806\n",
      "------------------------------------------\n",
      "L1 loss: 0.06828571110963821\n",
      "Training batch 22 with loss 0.26494\n",
      "------------------------------------------\n",
      "L1 loss: 0.07713231444358826\n",
      "Training batch 23 with loss 0.52422\n",
      "------------------------------------------\n",
      "L1 loss: 0.06788740307092667\n",
      "Training batch 24 with loss 0.29865\n",
      "------------------------------------------\n",
      "L1 loss: 0.07652253657579422\n",
      "Training batch 25 with loss 0.31487\n",
      "------------------------------------------\n",
      "L1 loss: 0.07546158134937286\n",
      "Training batch 26 with loss 0.27767\n",
      "------------------------------------------\n",
      "L1 loss: 0.05564286932349205\n",
      "Training batch 27 with loss 0.29820\n",
      "------------------------------------------\n",
      "L1 loss: 0.05805542320013046\n",
      "Training batch 28 with loss 0.30446\n",
      "------------------------------------------\n",
      "L1 loss: 0.05519546940922737\n",
      "Training batch 29 with loss 0.24268\n",
      "------------------------------------------\n",
      "L1 loss: 0.0628979504108429\n",
      "Training batch 30 with loss 0.26627\n",
      "------------------------------------------\n",
      "L1 loss: 0.0644695907831192\n",
      "Training batch 31 with loss 0.30662\n",
      "------------------------------------------\n",
      "L1 loss: 0.06905802339315414\n",
      "Training batch 32 with loss 0.27964\n",
      "------------------------------------------\n",
      "L1 loss: 0.06458447873592377\n",
      "Training batch 33 with loss 0.27296\n",
      "------------------------------------------\n",
      "L1 loss: 0.07707127183675766\n",
      "Training batch 34 with loss 0.30924\n",
      "------------------------------------------\n",
      "L1 loss: 0.09549515694379807\n",
      "Training batch 35 with loss 0.33926\n",
      "------------------------------------------\n",
      "L1 loss: 0.08344221115112305\n",
      "Training batch 36 with loss 0.27338\n",
      "------------------------------------------\n",
      "L1 loss: 0.06825604289770126\n",
      "Training batch 37 with loss 0.26719\n",
      "------------------------------------------\n",
      "L1 loss: 0.060602206736803055\n",
      "Training batch 38 with loss 0.21465\n",
      "------------------------------------------\n",
      "L1 loss: 0.06695578247308731\n",
      "Training batch 39 with loss 0.30607\n",
      "------------------------------------------\n",
      "L1 loss: 0.07359451055526733\n",
      "Training batch 40 with loss 0.26390\n",
      "------------------------------------------\n",
      "L1 loss: 0.05870547145605087\n",
      "Training batch 41 with loss 0.27157\n",
      "------------------------------------------\n",
      "L1 loss: 0.07891473174095154\n",
      "Training batch 42 with loss 0.28188\n",
      "------------------------------------------\n",
      "L1 loss: 0.06706630438566208\n",
      "Training batch 43 with loss 0.27203\n",
      "------------------------------------------\n",
      "L1 loss: 0.057315289974212646\n",
      "Training batch 44 with loss 0.28007\n",
      "------------------------------------------\n",
      "L1 loss: 0.09270019829273224\n",
      "Training batch 45 with loss 0.36721\n",
      "------------------------------------------\n",
      "L1 loss: 0.07561682909727097\n",
      "Training batch 46 with loss 0.26091\n",
      "------------------------------------------\n",
      "L1 loss: 0.10917948186397552\n",
      "Training batch 47 with loss 0.36732\n",
      "------------------------------------------\n",
      "L1 loss: 0.04400638863444328\n",
      "Training batch 48 with loss 0.35879\n",
      "------------------------------------------\n",
      "L1 loss: 0.07717197388410568\n",
      "Training batch 49 with loss 0.36326\n",
      "------------------------------------------\n",
      "L1 loss: 0.07304371148347855\n",
      "Training batch 50 with loss 0.28864\n",
      "------------------------------------------\n",
      "L1 loss: 0.08030027151107788\n",
      "Training batch 51 with loss 0.32595\n",
      "------------------------------------------\n",
      "L1 loss: 0.08965031057596207\n",
      "Training batch 52 with loss 0.29672\n",
      "------------------------------------------\n",
      "L1 loss: 0.08034859597682953\n",
      "Training batch 53 with loss 0.30169\n",
      "------------------------------------------\n",
      "L1 loss: 0.07290312647819519\n",
      "Training batch 54 with loss 0.27160\n",
      "------------------------------------------\n",
      "L1 loss: 0.062050044536590576\n",
      "Training batch 55 with loss 0.32137\n",
      "------------------------------------------\n",
      "L1 loss: 0.05782884359359741\n",
      "Training batch 56 with loss 0.25144\n",
      "------------------------------------------\n",
      "L1 loss: 0.06699115037918091\n",
      "Training batch 57 with loss 0.26805\n",
      "------------------------------------------\n",
      "L1 loss: 0.06464949995279312\n",
      "Training batch 58 with loss 0.25910\n",
      "------------------------------------------\n",
      "L1 loss: 0.07882851362228394\n",
      "Training batch 59 with loss 0.29429\n",
      "------------------------------------------\n",
      "L1 loss: 0.06595613807439804\n",
      "Training batch 60 with loss 0.26421\n",
      "------------------------------------------\n",
      "L1 loss: 0.10437344759702682\n",
      "Training batch 61 with loss 0.36535\n",
      "------------------------------------------\n",
      "L1 loss: 0.05882029980421066\n",
      "Training batch 62 with loss 0.25675\n",
      "------------------------------------------\n",
      "L1 loss: 0.06247512623667717\n",
      "Training batch 63 with loss 0.26770\n",
      "------------------------------------------\n",
      "L1 loss: 0.0725446566939354\n",
      "Training batch 64 with loss 0.28076\n",
      "------------------------------------------\n",
      "L1 loss: 0.06224452704191208\n",
      "Training batch 65 with loss 0.22666\n",
      "------------------------------------------\n",
      "L1 loss: 0.06416141241788864\n",
      "Training batch 66 with loss 0.27706\n",
      "------------------------------------------\n",
      "L1 loss: 0.0642985850572586\n",
      "Training batch 67 with loss 0.26295\n",
      "------------------------------------------\n",
      "L1 loss: 0.06492649763822556\n",
      "Training batch 68 with loss 0.41039\n",
      "------------------------------------------\n",
      "L1 loss: 0.05063645914196968\n",
      "Training batch 69 with loss 0.26130\n",
      "------------------------------------------\n",
      "L1 loss: 0.07654599845409393\n",
      "Training batch 70 with loss 0.31209\n",
      "------------------------------------------\n",
      "L1 loss: 0.07144489139318466\n",
      "Training batch 71 with loss 0.30430\n",
      "------------------------------------------\n",
      "L1 loss: 0.08407150954008102\n",
      "Training batch 72 with loss 0.29938\n",
      "------------------------------------------\n",
      "L1 loss: 0.07198317348957062\n",
      "Training batch 73 with loss 0.29944\n",
      "------------------------------------------\n",
      "L1 loss: 0.09782002866268158\n",
      "Training batch 74 with loss 0.31362\n",
      "------------------------------------------\n",
      "L1 loss: 0.07518579065799713\n",
      "Training batch 75 with loss 0.28778\n",
      "------------------------------------------\n",
      "L1 loss: 0.08609646558761597\n",
      "Training batch 76 with loss 0.30866\n",
      "------------------------------------------\n",
      "L1 loss: 0.07837063074111938\n",
      "Training batch 77 with loss 0.25750\n",
      "------------------------------------------\n",
      "L1 loss: 0.07704385370016098\n",
      "Training batch 78 with loss 0.29889\n",
      "------------------------------------------\n",
      "L1 loss: 0.10654427111148834\n",
      "Training batch 79 with loss 0.36750\n",
      "------------------------------------------\n",
      "L1 loss: 0.07674537599086761\n",
      "Training batch 80 with loss 0.29291\n",
      "------------------------------------------\n",
      "L1 loss: 0.06880068778991699\n",
      "Training batch 81 with loss 0.28399\n",
      "------------------------------------------\n",
      "L1 loss: 0.08472379297018051\n",
      "Training batch 82 with loss 0.34053\n",
      "------------------------------------------\n",
      "L1 loss: 0.08236528187990189\n",
      "Training batch 83 with loss 0.29434\n",
      "------------------------------------------\n",
      "L1 loss: 0.06724758446216583\n",
      "Training batch 84 with loss 0.29627\n",
      "------------------------------------------\n",
      "L1 loss: 0.06317716091871262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 85 with loss 0.27447\n",
      "------------------------------------------\n",
      "L1 loss: 0.08103132247924805\n",
      "Training batch 86 with loss 0.28760\n",
      "------------------------------------------\n",
      "L1 loss: 0.10134545713663101\n",
      "Training batch 87 with loss 0.38602\n",
      "------------------------------------------\n",
      "L1 loss: 0.07664135843515396\n",
      "Training batch 88 with loss 0.32044\n",
      "------------------------------------------\n",
      "L1 loss: 0.06915373355150223\n",
      "Training batch 89 with loss 0.27235\n",
      "------------------------------------------\n",
      "L1 loss: 0.07316252589225769\n",
      "Training batch 90 with loss 0.28144\n",
      "------------------------------------------\n",
      "L1 loss: 0.058752745389938354\n",
      "Training batch 91 with loss 0.24318\n",
      "------------------------------------------\n",
      "L1 loss: 0.09101881831884384\n",
      "Training batch 92 with loss 0.29312\n",
      "------------------------------------------\n",
      "L1 loss: 0.06597995012998581\n",
      "Training batch 93 with loss 0.26557\n",
      "------------------------------------------\n",
      "L1 loss: 0.08458075672388077\n",
      "Training batch 94 with loss 0.30987\n",
      "------------------------------------------\n",
      "L1 loss: 0.06887530535459518\n",
      "Training batch 95 with loss 0.25264\n",
      "------------------------------------------\n",
      "L1 loss: 0.0670003667473793\n",
      "Training batch 96 with loss 0.28273\n",
      "------------------------------------------\n",
      "L1 loss: 0.08511575311422348\n",
      "Training batch 97 with loss 0.35947\n",
      "------------------------------------------\n",
      "L1 loss: 0.05711553618311882\n",
      "Training batch 98 with loss 0.23575\n",
      "------------------------------------------\n",
      "L1 loss: 0.06507549434900284\n",
      "Training batch 99 with loss 0.25099\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2956806805729866\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4841\n",
      "------------------------------------------\n",
      "L1 loss: 0.06337032467126846\n",
      "Training batch 0 with loss 0.25431\n",
      "------------------------------------------\n",
      "L1 loss: 0.08132915943861008\n",
      "Training batch 1 with loss 0.28383\n",
      "------------------------------------------\n",
      "L1 loss: 0.09486258029937744\n",
      "Training batch 2 with loss 0.32737\n",
      "------------------------------------------\n",
      "L1 loss: 0.0710187703371048\n",
      "Training batch 3 with loss 0.25682\n",
      "------------------------------------------\n",
      "L1 loss: 0.08444498479366302\n",
      "Training batch 4 with loss 0.33938\n",
      "------------------------------------------\n",
      "L1 loss: 0.06628003716468811\n",
      "Training batch 5 with loss 0.32574\n",
      "------------------------------------------\n",
      "L1 loss: 0.0790075808763504\n",
      "Training batch 6 with loss 0.28426\n",
      "------------------------------------------\n",
      "L1 loss: 0.08393062651157379\n",
      "Training batch 7 with loss 0.33085\n",
      "------------------------------------------\n",
      "L1 loss: 0.07579844444990158\n",
      "Training batch 8 with loss 0.31395\n",
      "------------------------------------------\n",
      "L1 loss: 0.058545343577861786\n",
      "Training batch 9 with loss 0.24778\n",
      "------------------------------------------\n",
      "L1 loss: 0.0848650261759758\n",
      "Training batch 10 with loss 0.29143\n",
      "------------------------------------------\n",
      "L1 loss: 0.060507748275995255\n",
      "Training batch 11 with loss 0.64365\n",
      "------------------------------------------\n",
      "L1 loss: 0.07401721924543381\n",
      "Training batch 12 with loss 0.30315\n",
      "------------------------------------------\n",
      "L1 loss: 0.06765461713075638\n",
      "Training batch 13 with loss 0.24538\n",
      "------------------------------------------\n",
      "L1 loss: 0.0535886287689209\n",
      "Training batch 14 with loss 0.25910\n",
      "------------------------------------------\n",
      "L1 loss: 0.0853637084364891\n",
      "Training batch 15 with loss 0.27864\n",
      "------------------------------------------\n",
      "L1 loss: 0.061380092054605484\n",
      "Training batch 16 with loss 0.26663\n",
      "------------------------------------------\n",
      "L1 loss: 0.08800319582223892\n",
      "Training batch 17 with loss 0.35841\n",
      "------------------------------------------\n",
      "L1 loss: 0.08048278093338013\n",
      "Training batch 18 with loss 0.30410\n",
      "------------------------------------------\n",
      "L1 loss: 0.05259339138865471\n",
      "Training batch 19 with loss 0.38758\n",
      "------------------------------------------\n",
      "L1 loss: 0.07229117304086685\n",
      "Training batch 20 with loss 0.34054\n",
      "------------------------------------------\n",
      "L1 loss: 0.07027538120746613\n",
      "Training batch 21 with loss 0.27374\n",
      "------------------------------------------\n",
      "L1 loss: 0.06872909516096115\n",
      "Training batch 22 with loss 0.29367\n",
      "------------------------------------------\n",
      "L1 loss: 0.08729363977909088\n",
      "Training batch 23 with loss 0.34763\n",
      "------------------------------------------\n",
      "L1 loss: 0.06975556164979935\n",
      "Training batch 24 with loss 0.30534\n",
      "------------------------------------------\n",
      "L1 loss: 0.07338271290063858\n",
      "Training batch 25 with loss 0.29843\n",
      "------------------------------------------\n",
      "L1 loss: 0.07809995114803314\n",
      "Training batch 26 with loss 0.29349\n",
      "------------------------------------------\n",
      "L1 loss: 0.049249317497015\n",
      "Training batch 27 with loss 0.32557\n",
      "------------------------------------------\n",
      "L1 loss: 0.05741051957011223\n",
      "Training batch 28 with loss 0.30031\n",
      "------------------------------------------\n",
      "L1 loss: 0.06322874128818512\n",
      "Training batch 29 with loss 0.27733\n",
      "------------------------------------------\n",
      "L1 loss: 0.060643117874860764\n",
      "Training batch 30 with loss 0.26548\n",
      "------------------------------------------\n",
      "L1 loss: 0.06535547226667404\n",
      "Training batch 31 with loss 0.32111\n",
      "------------------------------------------\n",
      "L1 loss: 0.0701189711689949\n",
      "Training batch 32 with loss 0.28622\n",
      "------------------------------------------\n",
      "L1 loss: 0.06625739485025406\n",
      "Training batch 33 with loss 0.26313\n",
      "------------------------------------------\n",
      "L1 loss: 0.07735751569271088\n",
      "Training batch 34 with loss 0.32165\n",
      "------------------------------------------\n",
      "L1 loss: 0.09649337083101273\n",
      "Training batch 35 with loss 0.33425\n",
      "------------------------------------------\n",
      "L1 loss: 0.08654380589723587\n",
      "Training batch 36 with loss 0.30586\n",
      "------------------------------------------\n",
      "L1 loss: 0.07223091274499893\n",
      "Training batch 37 with loss 0.28459\n",
      "------------------------------------------\n",
      "L1 loss: 0.06358499079942703\n",
      "Training batch 38 with loss 0.22797\n",
      "------------------------------------------\n",
      "L1 loss: 0.06818360090255737\n",
      "Training batch 39 with loss 0.31134\n",
      "------------------------------------------\n",
      "L1 loss: 0.07600268721580505\n",
      "Training batch 40 with loss 0.26155\n",
      "------------------------------------------\n",
      "L1 loss: 0.060334376990795135\n",
      "Training batch 41 with loss 0.28355\n",
      "------------------------------------------\n",
      "L1 loss: 0.07403329014778137\n",
      "Training batch 42 with loss 0.27979\n",
      "------------------------------------------\n",
      "L1 loss: 0.061830442398786545\n",
      "Training batch 43 with loss 0.35824\n",
      "------------------------------------------\n",
      "L1 loss: 0.058130260556936264\n",
      "Training batch 44 with loss 0.27673\n",
      "------------------------------------------\n",
      "L1 loss: 0.0917152389883995\n",
      "Training batch 45 with loss 0.38194\n",
      "------------------------------------------\n",
      "L1 loss: 0.06906261295080185\n",
      "Training batch 46 with loss 0.27677\n",
      "------------------------------------------\n",
      "L1 loss: 0.11083823442459106\n",
      "Training batch 47 with loss 0.37065\n",
      "------------------------------------------\n",
      "L1 loss: 0.04987382888793945\n",
      "Training batch 48 with loss 0.27338\n",
      "------------------------------------------\n",
      "L1 loss: 0.08231081813573837\n",
      "Training batch 49 with loss 0.35185\n",
      "------------------------------------------\n",
      "L1 loss: 0.07226281613111496\n",
      "Training batch 50 with loss 0.29647\n",
      "------------------------------------------\n",
      "L1 loss: 0.08289399743080139\n",
      "Training batch 51 with loss 0.36304\n",
      "------------------------------------------\n",
      "L1 loss: 0.09085383266210556\n",
      "Training batch 52 with loss 0.28911\n",
      "------------------------------------------\n",
      "L1 loss: 0.08001264184713364\n",
      "Training batch 53 with loss 0.28364\n",
      "------------------------------------------\n",
      "L1 loss: 0.07580330967903137\n",
      "Training batch 54 with loss 0.25710\n",
      "------------------------------------------\n",
      "L1 loss: 0.061037030071020126\n",
      "Training batch 55 with loss 0.30012\n",
      "------------------------------------------\n",
      "L1 loss: 0.05228573828935623\n",
      "Training batch 56 with loss 0.24071\n",
      "------------------------------------------\n",
      "L1 loss: 0.06781136244535446\n",
      "Training batch 57 with loss 0.28591\n",
      "------------------------------------------\n",
      "L1 loss: 0.06256244331598282\n",
      "Training batch 58 with loss 0.26868\n",
      "------------------------------------------\n",
      "L1 loss: 0.08083172142505646\n",
      "Training batch 59 with loss 0.29904\n",
      "------------------------------------------\n",
      "L1 loss: 0.06693361699581146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 60 with loss 0.28378\n",
      "------------------------------------------\n",
      "L1 loss: 0.09482963383197784\n",
      "Training batch 61 with loss 0.35856\n",
      "------------------------------------------\n",
      "L1 loss: 0.05586880445480347\n",
      "Training batch 62 with loss 0.25596\n",
      "------------------------------------------\n",
      "L1 loss: 0.05965420603752136\n",
      "Training batch 63 with loss 0.25553\n",
      "------------------------------------------\n",
      "L1 loss: 0.06966394931077957\n",
      "Training batch 64 with loss 0.28248\n",
      "------------------------------------------\n",
      "L1 loss: 0.05608757212758064\n",
      "Training batch 65 with loss 0.22199\n",
      "------------------------------------------\n",
      "L1 loss: 0.06319864839315414\n",
      "Training batch 66 with loss 0.26791\n",
      "------------------------------------------\n",
      "L1 loss: 0.06432440876960754\n",
      "Training batch 67 with loss 0.25676\n",
      "------------------------------------------\n",
      "L1 loss: 0.07637958228588104\n",
      "Training batch 68 with loss 0.31807\n",
      "------------------------------------------\n",
      "L1 loss: 0.05133688077330589\n",
      "Training batch 69 with loss 0.25613\n",
      "------------------------------------------\n",
      "L1 loss: 0.07589424401521683\n",
      "Training batch 70 with loss 0.30665\n",
      "------------------------------------------\n",
      "L1 loss: 0.0719594731926918\n",
      "Training batch 71 with loss 0.31544\n",
      "------------------------------------------\n",
      "L1 loss: 0.08415131270885468\n",
      "Training batch 72 with loss 0.30813\n",
      "------------------------------------------\n",
      "L1 loss: 0.07373521476984024\n",
      "Training batch 73 with loss 0.29734\n",
      "------------------------------------------\n",
      "L1 loss: 0.09720315039157867\n",
      "Training batch 74 with loss 0.31511\n",
      "------------------------------------------\n",
      "L1 loss: 0.07759357988834381\n",
      "Training batch 75 with loss 0.28694\n",
      "------------------------------------------\n",
      "L1 loss: 0.0864546149969101\n",
      "Training batch 76 with loss 0.30210\n",
      "------------------------------------------\n",
      "L1 loss: 0.07722550630569458\n",
      "Training batch 77 with loss 0.24903\n",
      "------------------------------------------\n",
      "L1 loss: 0.08432197570800781\n",
      "Training batch 78 with loss 0.31716\n",
      "------------------------------------------\n",
      "L1 loss: 0.10581900179386139\n",
      "Training batch 79 with loss 0.36759\n",
      "------------------------------------------\n",
      "L1 loss: 0.07652061432600021\n",
      "Training batch 80 with loss 0.28503\n",
      "------------------------------------------\n",
      "L1 loss: 0.08243215829133987\n",
      "Training batch 81 with loss 0.30808\n",
      "------------------------------------------\n",
      "L1 loss: 0.08446455001831055\n",
      "Training batch 82 with loss 0.36247\n",
      "------------------------------------------\n",
      "L1 loss: 0.07680045068264008\n",
      "Training batch 83 with loss 0.27389\n",
      "------------------------------------------\n",
      "L1 loss: 0.06977671384811401\n",
      "Training batch 84 with loss 0.29506\n",
      "------------------------------------------\n",
      "L1 loss: 0.06373181939125061\n",
      "Training batch 85 with loss 0.27515\n",
      "------------------------------------------\n",
      "L1 loss: 0.08288480341434479\n",
      "Training batch 86 with loss 0.28441\n",
      "------------------------------------------\n",
      "L1 loss: 0.100356325507164\n",
      "Training batch 87 with loss 0.42129\n",
      "------------------------------------------\n",
      "L1 loss: 0.07685346901416779\n",
      "Training batch 88 with loss 0.30859\n",
      "------------------------------------------\n",
      "L1 loss: 0.06917005032300949\n",
      "Training batch 89 with loss 0.26941\n",
      "------------------------------------------\n",
      "L1 loss: 0.07461438328027725\n",
      "Training batch 90 with loss 0.28702\n",
      "------------------------------------------\n",
      "L1 loss: 0.06331034004688263\n",
      "Training batch 91 with loss 0.25446\n",
      "------------------------------------------\n",
      "L1 loss: 0.08990568667650223\n",
      "Training batch 92 with loss 0.29208\n",
      "------------------------------------------\n",
      "L1 loss: 0.057385992258787155\n",
      "Training batch 93 with loss 0.24071\n",
      "------------------------------------------\n",
      "L1 loss: 0.08577867597341537\n",
      "Training batch 94 with loss 0.28873\n",
      "------------------------------------------\n",
      "L1 loss: 0.07071970403194427\n",
      "Training batch 95 with loss 0.26013\n",
      "------------------------------------------\n",
      "L1 loss: 0.05889660865068436\n",
      "Training batch 96 with loss 0.25175\n",
      "------------------------------------------\n",
      "L1 loss: 0.08692844957113266\n",
      "Training batch 97 with loss 0.35472\n",
      "------------------------------------------\n",
      "L1 loss: 0.05662445351481438\n",
      "Training batch 98 with loss 0.24060\n",
      "------------------------------------------\n",
      "L1 loss: 0.06588335335254669\n",
      "Training batch 99 with loss 0.27194\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2992642731964588\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4842\n",
      "------------------------------------------\n",
      "L1 loss: 0.059919632971286774\n",
      "Training batch 0 with loss 0.24774\n",
      "------------------------------------------\n",
      "L1 loss: 0.0842062309384346\n",
      "Training batch 1 with loss 0.28572\n",
      "------------------------------------------\n",
      "L1 loss: 0.09375301748514175\n",
      "Training batch 2 with loss 0.32693\n",
      "------------------------------------------\n",
      "L1 loss: 0.07918243110179901\n",
      "Training batch 3 with loss 0.27946\n",
      "------------------------------------------\n",
      "L1 loss: 0.08138104528188705\n",
      "Training batch 4 with loss 0.33081\n",
      "------------------------------------------\n",
      "L1 loss: 0.07006952911615372\n",
      "Training batch 5 with loss 0.34934\n",
      "------------------------------------------\n",
      "L1 loss: 0.07876630127429962\n",
      "Training batch 6 with loss 0.26547\n",
      "------------------------------------------\n",
      "L1 loss: 0.08300042897462845\n",
      "Training batch 7 with loss 0.33157\n",
      "------------------------------------------\n",
      "L1 loss: 0.08253307640552521\n",
      "Training batch 8 with loss 0.33645\n",
      "------------------------------------------\n",
      "L1 loss: 0.05687212198972702\n",
      "Training batch 9 with loss 0.24769\n",
      "------------------------------------------\n",
      "L1 loss: 0.08236504346132278\n",
      "Training batch 10 with loss 0.29211\n",
      "------------------------------------------\n",
      "L1 loss: 0.05596482381224632\n",
      "Training batch 11 with loss 0.30063\n",
      "------------------------------------------\n",
      "L1 loss: 0.07758703082799911\n",
      "Training batch 12 with loss 0.32042\n",
      "------------------------------------------\n",
      "L1 loss: 0.06761632859706879\n",
      "Training batch 13 with loss 0.25613\n",
      "------------------------------------------\n",
      "L1 loss: 0.05310528352856636\n",
      "Training batch 14 with loss 0.26825\n",
      "------------------------------------------\n",
      "L1 loss: 0.08504947274923325\n",
      "Training batch 15 with loss 0.27166\n",
      "------------------------------------------\n",
      "L1 loss: 0.06452853232622147\n",
      "Training batch 16 with loss 0.27092\n",
      "------------------------------------------\n",
      "L1 loss: 0.0830470621585846\n",
      "Training batch 17 with loss 0.33678\n",
      "------------------------------------------\n",
      "L1 loss: 0.08141174167394638\n",
      "Training batch 18 with loss 0.30821\n",
      "------------------------------------------\n",
      "L1 loss: 0.0584540031850338\n",
      "Training batch 19 with loss 0.22539\n",
      "------------------------------------------\n",
      "L1 loss: 0.07131461054086685\n",
      "Training batch 20 with loss 0.32149\n",
      "------------------------------------------\n",
      "L1 loss: 0.07350991666316986\n",
      "Training batch 21 with loss 0.29734\n",
      "------------------------------------------\n",
      "L1 loss: 0.06751056760549545\n",
      "Training batch 22 with loss 0.29518\n",
      "------------------------------------------\n",
      "L1 loss: 0.08973143994808197\n",
      "Training batch 23 with loss 0.36111\n",
      "------------------------------------------\n",
      "L1 loss: 0.06713919341564178\n",
      "Training batch 24 with loss 0.30948\n",
      "------------------------------------------\n",
      "L1 loss: 0.0726381167769432\n",
      "Training batch 25 with loss 0.27795\n",
      "------------------------------------------\n",
      "L1 loss: 0.0745246559381485\n",
      "Training batch 26 with loss 0.26645\n",
      "------------------------------------------\n",
      "L1 loss: 0.055885497480630875\n",
      "Training batch 27 with loss 0.29209\n",
      "------------------------------------------\n",
      "L1 loss: 0.05766964703798294\n",
      "Training batch 28 with loss 0.31038\n",
      "------------------------------------------\n",
      "L1 loss: 0.05934956297278404\n",
      "Training batch 29 with loss 0.25595\n",
      "------------------------------------------\n",
      "L1 loss: 0.06406164169311523\n",
      "Training batch 30 with loss 0.27471\n",
      "------------------------------------------\n",
      "L1 loss: 0.06235616281628609\n",
      "Training batch 31 with loss 0.30754\n",
      "------------------------------------------\n",
      "L1 loss: 0.069656603038311\n",
      "Training batch 32 with loss 0.26938\n",
      "------------------------------------------\n",
      "L1 loss: 0.06263700127601624\n",
      "Training batch 33 with loss 0.27544\n",
      "------------------------------------------\n",
      "L1 loss: 0.07612591236829758\n",
      "Training batch 34 with loss 0.31490\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.09635213017463684\n",
      "Training batch 35 with loss 0.33952\n",
      "------------------------------------------\n",
      "L1 loss: 0.08636517077684402\n",
      "Training batch 36 with loss 0.28160\n",
      "------------------------------------------\n",
      "L1 loss: 0.06688400357961655\n",
      "Training batch 37 with loss 0.25493\n",
      "------------------------------------------\n",
      "L1 loss: 0.06031600758433342\n",
      "Training batch 38 with loss 0.21508\n",
      "------------------------------------------\n",
      "L1 loss: 0.06785185635089874\n",
      "Training batch 39 with loss 0.30336\n",
      "------------------------------------------\n",
      "L1 loss: 0.06897908449172974\n",
      "Training batch 40 with loss 0.24215\n",
      "------------------------------------------\n",
      "L1 loss: 0.06018891930580139\n",
      "Training batch 41 with loss 0.29268\n",
      "------------------------------------------\n",
      "L1 loss: 0.0792527049779892\n",
      "Training batch 42 with loss 0.27875\n",
      "------------------------------------------\n",
      "L1 loss: 0.06760412454605103\n",
      "Training batch 43 with loss 0.28389\n",
      "------------------------------------------\n",
      "L1 loss: 0.05968325212597847\n",
      "Training batch 44 with loss 0.27729\n",
      "------------------------------------------\n",
      "L1 loss: 0.09281346946954727\n",
      "Training batch 45 with loss 0.35972\n",
      "------------------------------------------\n",
      "L1 loss: 0.07458990812301636\n",
      "Training batch 46 with loss 0.27211\n",
      "------------------------------------------\n",
      "L1 loss: 0.1098753958940506\n",
      "Training batch 47 with loss 0.36846\n",
      "------------------------------------------\n",
      "L1 loss: 0.04486946016550064\n",
      "Training batch 48 with loss 0.24821\n",
      "------------------------------------------\n",
      "L1 loss: 0.07798894494771957\n",
      "Training batch 49 with loss 0.36730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07474754005670547\n",
      "Training batch 50 with loss 0.30124\n",
      "------------------------------------------\n",
      "L1 loss: 0.08371756225824356\n",
      "Training batch 51 with loss 0.35840\n",
      "------------------------------------------\n",
      "L1 loss: 0.08981713652610779\n",
      "Training batch 52 with loss 0.28416\n",
      "------------------------------------------\n",
      "L1 loss: 0.07982881367206573\n",
      "Training batch 53 with loss 0.28117\n",
      "------------------------------------------\n",
      "L1 loss: 0.07691440731287003\n",
      "Training batch 54 with loss 0.26713\n",
      "------------------------------------------\n",
      "L1 loss: 0.06303896009922028\n",
      "Training batch 55 with loss 0.30724\n",
      "------------------------------------------\n",
      "L1 loss: 0.050043512135744095\n",
      "Training batch 56 with loss 0.23146\n",
      "------------------------------------------\n",
      "L1 loss: 0.06865239143371582\n",
      "Training batch 57 with loss 0.28254\n",
      "------------------------------------------\n",
      "L1 loss: 0.06366974860429764\n",
      "Training batch 58 with loss 0.25318\n",
      "------------------------------------------\n",
      "L1 loss: 0.07802470028400421\n",
      "Training batch 59 with loss 0.30722\n",
      "------------------------------------------\n",
      "L1 loss: 0.06428871303796768\n",
      "Training batch 60 with loss 0.27766\n",
      "------------------------------------------\n",
      "L1 loss: 0.08892493695020676\n",
      "Training batch 61 with loss 0.67227\n",
      "------------------------------------------\n",
      "L1 loss: 0.05638756603002548\n",
      "Training batch 62 with loss 0.25983\n",
      "------------------------------------------\n",
      "L1 loss: 0.06358585506677628\n",
      "Training batch 63 with loss 0.27180\n",
      "------------------------------------------\n",
      "L1 loss: 0.07254374027252197\n",
      "Training batch 64 with loss 0.27517\n",
      "------------------------------------------\n",
      "L1 loss: 0.05816256254911423\n",
      "Training batch 65 with loss 0.21494\n",
      "------------------------------------------\n",
      "L1 loss: 0.0647578313946724\n",
      "Training batch 66 with loss 0.26889\n",
      "------------------------------------------\n",
      "L1 loss: 0.06450050324201584\n",
      "Training batch 67 with loss 0.26582\n",
      "------------------------------------------\n",
      "L1 loss: 0.07327900826931\n",
      "Training batch 68 with loss 0.29835\n",
      "------------------------------------------\n",
      "L1 loss: 0.048984136432409286\n",
      "Training batch 69 with loss 0.25388\n",
      "------------------------------------------\n",
      "L1 loss: 0.0815313458442688\n",
      "Training batch 70 with loss 0.31174\n",
      "------------------------------------------\n",
      "L1 loss: 0.07158850878477097\n",
      "Training batch 71 with loss 0.31478\n",
      "------------------------------------------\n",
      "L1 loss: 0.08504344522953033\n",
      "Training batch 72 with loss 0.32631\n",
      "------------------------------------------\n",
      "L1 loss: 0.07463675737380981\n",
      "Training batch 73 with loss 0.29538\n",
      "------------------------------------------\n",
      "L1 loss: 0.09695690870285034\n",
      "Training batch 74 with loss 0.33441\n",
      "------------------------------------------\n",
      "L1 loss: 0.07575307041406631\n",
      "Training batch 75 with loss 0.27473\n",
      "------------------------------------------\n",
      "L1 loss: 0.08653203397989273\n",
      "Training batch 76 with loss 0.28738\n",
      "------------------------------------------\n",
      "L1 loss: 0.07584124058485031\n",
      "Training batch 77 with loss 0.27710\n",
      "------------------------------------------\n",
      "L1 loss: 0.08054681867361069\n",
      "Training batch 78 with loss 0.31398\n",
      "------------------------------------------\n",
      "L1 loss: 0.10587179660797119\n",
      "Training batch 79 with loss 0.35641\n",
      "------------------------------------------\n",
      "L1 loss: 0.07657866925001144\n",
      "Training batch 80 with loss 0.28862\n",
      "------------------------------------------\n",
      "L1 loss: 0.07496501505374908\n",
      "Training batch 81 with loss 0.28388\n",
      "------------------------------------------\n",
      "L1 loss: 0.08620882779359818\n",
      "Training batch 82 with loss 0.36471\n",
      "------------------------------------------\n",
      "L1 loss: 0.07469242811203003\n",
      "Training batch 83 with loss 0.28334\n",
      "------------------------------------------\n",
      "L1 loss: 0.07391582429409027\n",
      "Training batch 84 with loss 0.32533\n",
      "------------------------------------------\n",
      "L1 loss: 0.06391815096139908\n",
      "Training batch 85 with loss 0.27233\n",
      "------------------------------------------\n",
      "L1 loss: 0.08195594698190689\n",
      "Training batch 86 with loss 0.29853\n",
      "------------------------------------------\n",
      "L1 loss: 0.10355371236801147\n",
      "Training batch 87 with loss 0.40760\n",
      "------------------------------------------\n",
      "L1 loss: 0.07419510185718536\n",
      "Training batch 88 with loss 0.30025\n",
      "------------------------------------------\n",
      "L1 loss: 0.0704924538731575\n",
      "Training batch 89 with loss 0.26597\n",
      "------------------------------------------\n",
      "L1 loss: 0.0749744325876236\n",
      "Training batch 90 with loss 0.28896\n",
      "------------------------------------------\n",
      "L1 loss: 0.0625714585185051\n",
      "Training batch 91 with loss 0.26979\n",
      "------------------------------------------\n",
      "L1 loss: 0.09480089694261551\n",
      "Training batch 92 with loss 0.30824\n",
      "------------------------------------------\n",
      "L1 loss: 0.059302717447280884\n",
      "Training batch 93 with loss 0.23995\n",
      "------------------------------------------\n",
      "L1 loss: 0.08476000279188156\n",
      "Training batch 94 with loss 0.28950\n",
      "------------------------------------------\n",
      "L1 loss: 0.07157132774591446\n",
      "Training batch 95 with loss 0.25110\n",
      "------------------------------------------\n",
      "L1 loss: 0.06423675268888474\n",
      "Training batch 96 with loss 0.28561\n",
      "------------------------------------------\n",
      "L1 loss: 0.08715231716632843\n",
      "Training batch 97 with loss 0.35233\n",
      "------------------------------------------\n",
      "L1 loss: 0.054805126041173935\n",
      "Training batch 98 with loss 0.23854\n",
      "------------------------------------------\n",
      "L1 loss: 0.06516161561012268\n",
      "Training batch 99 with loss 0.26377\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2956906661391258\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4843\n",
      "------------------------------------------\n",
      "L1 loss: 0.06264755874872208\n",
      "Training batch 0 with loss 0.26258\n",
      "------------------------------------------\n",
      "L1 loss: 0.08580831438302994\n",
      "Training batch 1 with loss 0.29011\n",
      "------------------------------------------\n",
      "L1 loss: 0.08905843645334244\n",
      "Training batch 2 with loss 0.30974\n",
      "------------------------------------------\n",
      "L1 loss: 0.07431665807962418\n",
      "Training batch 3 with loss 0.26493\n",
      "------------------------------------------\n",
      "L1 loss: 0.0826694518327713\n",
      "Training batch 4 with loss 0.32865\n",
      "------------------------------------------\n",
      "L1 loss: 0.07028742879629135\n",
      "Training batch 5 with loss 0.35498\n",
      "------------------------------------------\n",
      "L1 loss: 0.08137188851833344\n",
      "Training batch 6 with loss 0.27141\n",
      "------------------------------------------\n",
      "L1 loss: 0.0828731507062912\n",
      "Training batch 7 with loss 0.32839\n",
      "------------------------------------------\n",
      "L1 loss: 0.08056719601154327\n",
      "Training batch 8 with loss 0.30694\n",
      "------------------------------------------\n",
      "L1 loss: 0.0630471259355545\n",
      "Training batch 9 with loss 0.24078\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08372822403907776\n",
      "Training batch 10 with loss 0.29745\n",
      "------------------------------------------\n",
      "L1 loss: 0.05921546369791031\n",
      "Training batch 11 with loss 0.28289\n",
      "------------------------------------------\n",
      "L1 loss: 0.07239946722984314\n",
      "Training batch 12 with loss 0.30422\n",
      "------------------------------------------\n",
      "L1 loss: 0.06973103433847427\n",
      "Training batch 13 with loss 0.25215\n",
      "------------------------------------------\n",
      "L1 loss: 0.061118949204683304\n",
      "Training batch 14 with loss 0.49219\n",
      "------------------------------------------\n",
      "L1 loss: 0.0850490927696228\n",
      "Training batch 15 with loss 0.28211\n",
      "------------------------------------------\n",
      "L1 loss: 0.06398244947195053\n",
      "Training batch 16 with loss 0.26313\n",
      "------------------------------------------\n",
      "L1 loss: 0.08726245164871216\n",
      "Training batch 17 with loss 0.36080\n",
      "------------------------------------------\n",
      "L1 loss: 0.08065930008888245\n",
      "Training batch 18 with loss 0.29759\n",
      "------------------------------------------\n",
      "L1 loss: 0.06013457849621773\n",
      "Training batch 19 with loss 0.22197\n",
      "------------------------------------------\n",
      "L1 loss: 0.07271937280893326\n",
      "Training batch 20 with loss 0.33100\n",
      "------------------------------------------\n",
      "L1 loss: 0.07227317243814468\n",
      "Training batch 21 with loss 0.27709\n",
      "------------------------------------------\n",
      "L1 loss: 0.06798068434000015\n",
      "Training batch 22 with loss 0.28825\n",
      "------------------------------------------\n",
      "L1 loss: 0.08832832425832748\n",
      "Training batch 23 with loss 0.36615\n",
      "------------------------------------------\n",
      "L1 loss: 0.06649190187454224\n",
      "Training batch 24 with loss 0.30602\n",
      "------------------------------------------\n",
      "L1 loss: 0.07435651868581772\n",
      "Training batch 25 with loss 0.31204\n",
      "------------------------------------------\n",
      "L1 loss: 0.07123502343893051\n",
      "Training batch 26 with loss 0.27155\n",
      "------------------------------------------\n",
      "L1 loss: 0.056232139468193054\n",
      "Training batch 27 with loss 0.29838\n",
      "------------------------------------------\n",
      "L1 loss: 0.0582074336707592\n",
      "Training batch 28 with loss 0.28896\n",
      "------------------------------------------\n",
      "L1 loss: 0.041165418922901154\n",
      "Training batch 29 with loss 0.28171\n",
      "------------------------------------------\n",
      "L1 loss: 0.060055751353502274\n",
      "Training batch 30 with loss 0.27116\n",
      "------------------------------------------\n",
      "L1 loss: 0.06281100958585739\n",
      "Training batch 31 with loss 0.29702\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976871937513351\n",
      "Training batch 32 with loss 0.28019\n",
      "------------------------------------------\n",
      "L1 loss: 0.06306498497724533\n",
      "Training batch 33 with loss 0.26310\n",
      "------------------------------------------\n",
      "L1 loss: 0.07345757633447647\n",
      "Training batch 34 with loss 0.32015\n",
      "------------------------------------------\n",
      "L1 loss: 0.09350478649139404\n",
      "Training batch 35 with loss 0.32126\n",
      "------------------------------------------\n",
      "L1 loss: 0.08353939652442932\n",
      "Training batch 36 with loss 0.28772\n",
      "------------------------------------------\n",
      "L1 loss: 0.070818230509758\n",
      "Training batch 37 with loss 0.28214\n",
      "------------------------------------------\n",
      "L1 loss: 0.06428655982017517\n",
      "Training batch 38 with loss 0.22123\n",
      "------------------------------------------\n",
      "L1 loss: 0.06768281757831573\n",
      "Training batch 39 with loss 0.28629\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990304589271545\n",
      "Training batch 40 with loss 0.24828\n",
      "------------------------------------------\n",
      "L1 loss: 0.05946962907910347\n",
      "Training batch 41 with loss 0.30766\n",
      "------------------------------------------\n",
      "L1 loss: 0.07811273634433746\n",
      "Training batch 42 with loss 0.27798\n",
      "------------------------------------------\n",
      "L1 loss: 0.0673232227563858\n",
      "Training batch 43 with loss 0.27594\n",
      "------------------------------------------\n",
      "L1 loss: 0.05415480211377144\n",
      "Training batch 44 with loss 0.26727\n",
      "------------------------------------------\n",
      "L1 loss: 0.0930243581533432\n",
      "Training batch 45 with loss 0.36449\n",
      "------------------------------------------\n",
      "L1 loss: 0.07527810335159302\n",
      "Training batch 46 with loss 0.25885\n",
      "------------------------------------------\n",
      "L1 loss: 0.10893334448337555\n",
      "Training batch 47 with loss 0.36599\n",
      "------------------------------------------\n",
      "L1 loss: 0.049568016082048416\n",
      "Training batch 48 with loss 0.27452\n",
      "------------------------------------------\n",
      "L1 loss: 0.07804416865110397\n",
      "Training batch 49 with loss 0.34200\n",
      "------------------------------------------\n",
      "L1 loss: 0.07369422912597656\n",
      "Training batch 50 with loss 0.29967\n",
      "------------------------------------------\n",
      "L1 loss: 0.08365248143672943\n",
      "Training batch 51 with loss 0.33396\n",
      "------------------------------------------\n",
      "L1 loss: 0.088945671916008\n",
      "Training batch 52 with loss 0.28493\n",
      "------------------------------------------\n",
      "L1 loss: 0.0797920674085617\n",
      "Training batch 53 with loss 0.29135\n",
      "------------------------------------------\n",
      "L1 loss: 0.07617392390966415\n",
      "Training batch 54 with loss 0.27537\n",
      "------------------------------------------\n",
      "L1 loss: 0.059404224157333374\n",
      "Training batch 55 with loss 0.30731\n",
      "------------------------------------------\n",
      "L1 loss: 0.05725555494427681\n",
      "Training batch 56 with loss 0.26059\n",
      "------------------------------------------\n",
      "L1 loss: 0.0660175234079361\n",
      "Training batch 57 with loss 0.28580\n",
      "------------------------------------------\n",
      "L1 loss: 0.06648809462785721\n",
      "Training batch 58 with loss 0.25544\n",
      "------------------------------------------\n",
      "L1 loss: 0.07881006598472595\n",
      "Training batch 59 with loss 0.28718\n",
      "------------------------------------------\n",
      "L1 loss: 0.06539639830589294\n",
      "Training batch 60 with loss 0.26975\n",
      "------------------------------------------\n",
      "L1 loss: 0.09563024342060089\n",
      "Training batch 61 with loss 0.34892\n",
      "------------------------------------------\n",
      "L1 loss: 0.054448146373033524\n",
      "Training batch 62 with loss 0.25998\n",
      "------------------------------------------\n",
      "L1 loss: 0.06187603250145912\n",
      "Training batch 63 with loss 0.26501\n",
      "------------------------------------------\n",
      "L1 loss: 0.07294021546840668\n",
      "Training batch 64 with loss 0.28412\n",
      "------------------------------------------\n",
      "L1 loss: 0.0614844374358654\n",
      "Training batch 65 with loss 0.22317\n",
      "------------------------------------------\n",
      "L1 loss: 0.06616295874118805\n",
      "Training batch 66 with loss 0.27131\n",
      "------------------------------------------\n",
      "L1 loss: 0.06628197431564331\n",
      "Training batch 67 with loss 0.26731\n",
      "------------------------------------------\n",
      "L1 loss: 0.07209914177656174\n",
      "Training batch 68 with loss 0.29492\n",
      "------------------------------------------\n",
      "L1 loss: 0.04185891151428223\n",
      "Training batch 69 with loss 0.29721\n",
      "------------------------------------------\n",
      "L1 loss: 0.08260011672973633\n",
      "Training batch 70 with loss 0.34146\n",
      "------------------------------------------\n",
      "L1 loss: 0.07292883843183517\n",
      "Training batch 71 with loss 0.30554\n",
      "------------------------------------------\n",
      "L1 loss: 0.08331380784511566\n",
      "Training batch 72 with loss 0.29804\n",
      "------------------------------------------\n",
      "L1 loss: 0.07232513278722763\n",
      "Training batch 73 with loss 0.30513\n",
      "------------------------------------------\n",
      "L1 loss: 0.0995398536324501\n",
      "Training batch 74 with loss 0.31812\n",
      "------------------------------------------\n",
      "L1 loss: 0.07489223033189774\n",
      "Training batch 75 with loss 0.27128\n",
      "------------------------------------------\n",
      "L1 loss: 0.08639064431190491\n",
      "Training batch 76 with loss 0.29405\n",
      "------------------------------------------\n",
      "L1 loss: 0.07620440423488617\n",
      "Training batch 77 with loss 0.26925\n",
      "------------------------------------------\n",
      "L1 loss: 0.08206833899021149\n",
      "Training batch 78 with loss 0.31938\n",
      "------------------------------------------\n",
      "L1 loss: 0.1064726784825325\n",
      "Training batch 79 with loss 0.36715\n",
      "------------------------------------------\n",
      "L1 loss: 0.07619988173246384\n",
      "Training batch 80 with loss 0.27809\n",
      "------------------------------------------\n",
      "L1 loss: 0.07824312895536423\n",
      "Training batch 81 with loss 0.32387\n",
      "------------------------------------------\n",
      "L1 loss: 0.08395011723041534\n",
      "Training batch 82 with loss 0.37258\n",
      "------------------------------------------\n",
      "L1 loss: 0.07975052297115326\n",
      "Training batch 83 with loss 0.29694\n",
      "------------------------------------------\n",
      "L1 loss: 0.07303556799888611\n",
      "Training batch 84 with loss 0.31993\n",
      "------------------------------------------\n",
      "L1 loss: 0.05808244273066521\n",
      "Training batch 85 with loss 0.26851\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08214101940393448\n",
      "Training batch 86 with loss 0.28784\n",
      "------------------------------------------\n",
      "L1 loss: 0.10213757306337357\n",
      "Training batch 87 with loss 0.39426\n",
      "------------------------------------------\n",
      "L1 loss: 0.07938036322593689\n",
      "Training batch 88 with loss 0.32753\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990658491849899\n",
      "Training batch 89 with loss 0.26195\n",
      "------------------------------------------\n",
      "L1 loss: 0.07062480598688126\n",
      "Training batch 90 with loss 0.28797\n",
      "------------------------------------------\n",
      "L1 loss: 0.06407184898853302\n",
      "Training batch 91 with loss 0.25669\n",
      "------------------------------------------\n",
      "L1 loss: 0.08977238088846207\n",
      "Training batch 92 with loss 0.30127\n",
      "------------------------------------------\n",
      "L1 loss: 0.06332775205373764\n",
      "Training batch 93 with loss 0.24883\n",
      "------------------------------------------\n",
      "L1 loss: 0.08622322231531143\n",
      "Training batch 94 with loss 0.31124\n",
      "------------------------------------------\n",
      "L1 loss: 0.06091568246483803\n",
      "Training batch 95 with loss 0.22230\n",
      "------------------------------------------\n",
      "L1 loss: 0.06325854361057281\n",
      "Training batch 96 with loss 0.31091\n",
      "------------------------------------------\n",
      "L1 loss: 0.08742962777614594\n",
      "Training batch 97 with loss 0.34418\n",
      "------------------------------------------\n",
      "L1 loss: 0.056262459605932236\n",
      "Training batch 98 with loss 0.24405\n",
      "------------------------------------------\n",
      "L1 loss: 0.06515289098024368\n",
      "Training batch 99 with loss 0.26365\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29522736921906473\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4844\n",
      "------------------------------------------\n",
      "L1 loss: 0.0616384819149971\n",
      "Training batch 0 with loss 0.24602\n",
      "------------------------------------------\n",
      "L1 loss: 0.08498557657003403\n",
      "Training batch 1 with loss 0.29078\n",
      "------------------------------------------\n",
      "L1 loss: 0.09404174238443375\n",
      "Training batch 2 with loss 0.33007\n",
      "------------------------------------------\n",
      "L1 loss: 0.07663345336914062\n",
      "Training batch 3 with loss 0.26356\n",
      "------------------------------------------\n",
      "L1 loss: 0.08490797132253647\n",
      "Training batch 4 with loss 0.33335\n",
      "------------------------------------------\n",
      "L1 loss: 0.06807000935077667\n",
      "Training batch 5 with loss 0.33672\n",
      "------------------------------------------\n",
      "L1 loss: 0.08035364001989365\n",
      "Training batch 6 with loss 0.26806\n",
      "------------------------------------------\n",
      "L1 loss: 0.08445682376623154\n",
      "Training batch 7 with loss 0.33259\n",
      "------------------------------------------\n",
      "L1 loss: 0.07983902096748352\n",
      "Training batch 8 with loss 0.31186\n",
      "------------------------------------------\n",
      "L1 loss: 0.057962462306022644\n",
      "Training batch 9 with loss 0.24457\n",
      "------------------------------------------\n",
      "L1 loss: 0.08042953908443451\n",
      "Training batch 10 with loss 0.30001\n",
      "------------------------------------------\n",
      "L1 loss: 0.06061538681387901\n",
      "Training batch 11 with loss 0.29570\n",
      "------------------------------------------\n",
      "L1 loss: 0.07655629515647888\n",
      "Training batch 12 with loss 0.31307\n",
      "------------------------------------------\n",
      "L1 loss: 0.07047504931688309\n",
      "Training batch 13 with loss 0.24808\n",
      "------------------------------------------\n",
      "L1 loss: 0.053827181458473206\n",
      "Training batch 14 with loss 0.25036\n",
      "------------------------------------------\n",
      "L1 loss: 0.08492858707904816\n",
      "Training batch 15 with loss 0.27215\n",
      "------------------------------------------\n",
      "L1 loss: 0.06475947797298431\n",
      "Training batch 16 with loss 0.27723\n",
      "------------------------------------------\n",
      "L1 loss: 0.08916831761598587\n",
      "Training batch 17 with loss 0.34729\n",
      "------------------------------------------\n",
      "L1 loss: 0.08128897100687027\n",
      "Training batch 18 with loss 0.30070\n",
      "------------------------------------------\n",
      "L1 loss: 0.06111520901322365\n",
      "Training batch 19 with loss 0.23647\n",
      "------------------------------------------\n",
      "L1 loss: 0.06988377869129181\n",
      "Training batch 20 with loss 0.50474\n",
      "------------------------------------------\n",
      "L1 loss: 0.07121285051107407\n",
      "Training batch 21 with loss 0.28734\n",
      "------------------------------------------\n",
      "L1 loss: 0.06939910352230072\n",
      "Training batch 22 with loss 0.28012\n",
      "------------------------------------------\n",
      "L1 loss: 0.08944206684827805\n",
      "Training batch 23 with loss 0.36353\n",
      "------------------------------------------\n",
      "L1 loss: 0.06690210103988647\n",
      "Training batch 24 with loss 0.30569\n",
      "------------------------------------------\n",
      "L1 loss: 0.07531395554542542\n",
      "Training batch 25 with loss 0.29758\n",
      "------------------------------------------\n",
      "L1 loss: 0.07480844110250473\n",
      "Training batch 26 with loss 0.26684\n",
      "------------------------------------------\n",
      "L1 loss: 0.056438758969306946\n",
      "Training batch 27 with loss 0.30859\n",
      "------------------------------------------\n",
      "L1 loss: 0.05727475881576538\n",
      "Training batch 28 with loss 0.28369\n",
      "------------------------------------------\n",
      "L1 loss: 0.059040237218141556\n",
      "Training batch 29 with loss 0.24664\n",
      "------------------------------------------\n",
      "L1 loss: 0.05890950560569763\n",
      "Training batch 30 with loss 0.26815\n",
      "------------------------------------------\n",
      "L1 loss: 0.06655754148960114\n",
      "Training batch 31 with loss 0.30648\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937079131603241\n",
      "Training batch 32 with loss 0.28627\n",
      "------------------------------------------\n",
      "L1 loss: 0.06278329342603683\n",
      "Training batch 33 with loss 0.27193\n",
      "------------------------------------------\n",
      "L1 loss: 0.07513973861932755\n",
      "Training batch 34 with loss 0.31222\n",
      "------------------------------------------\n",
      "L1 loss: 0.09358397871255875\n",
      "Training batch 35 with loss 0.34252\n",
      "------------------------------------------\n",
      "L1 loss: 0.08476927876472473\n",
      "Training batch 36 with loss 0.27122\n",
      "------------------------------------------\n",
      "L1 loss: 0.06670399010181427\n",
      "Training batch 37 with loss 0.26518\n",
      "------------------------------------------\n",
      "L1 loss: 0.06383366137742996\n",
      "Training batch 38 with loss 0.21960\n",
      "------------------------------------------\n",
      "L1 loss: 0.0677388459444046\n",
      "Training batch 39 with loss 0.30506\n",
      "------------------------------------------\n",
      "L1 loss: 0.07465825229883194\n",
      "Training batch 40 with loss 0.26947\n",
      "------------------------------------------\n",
      "L1 loss: 0.06068279221653938\n",
      "Training batch 41 with loss 0.28071\n",
      "------------------------------------------\n",
      "L1 loss: 0.07995317131280899\n",
      "Training batch 42 with loss 0.28491\n",
      "------------------------------------------\n",
      "L1 loss: 0.07284868508577347\n",
      "Training batch 43 with loss 0.29920\n",
      "------------------------------------------\n",
      "L1 loss: 0.05569104850292206\n",
      "Training batch 44 with loss 0.28053\n",
      "------------------------------------------\n",
      "L1 loss: 0.0931282788515091\n",
      "Training batch 45 with loss 0.35730\n",
      "------------------------------------------\n",
      "L1 loss: 0.07427104562520981\n",
      "Training batch 46 with loss 0.27015\n",
      "------------------------------------------\n",
      "L1 loss: 0.11260200291872025\n",
      "Training batch 47 with loss 0.39002\n",
      "------------------------------------------\n",
      "L1 loss: 0.04452943801879883\n",
      "Training batch 48 with loss 0.24156\n",
      "------------------------------------------\n",
      "L1 loss: 0.08285684138536453\n",
      "Training batch 49 with loss 0.36417\n",
      "------------------------------------------\n",
      "L1 loss: 0.07511042058467865\n",
      "Training batch 50 with loss 0.31213\n",
      "------------------------------------------\n",
      "L1 loss: 0.07849396765232086\n",
      "Training batch 51 with loss 0.33095\n",
      "------------------------------------------\n",
      "L1 loss: 0.09094976633787155\n",
      "Training batch 52 with loss 0.29735\n",
      "------------------------------------------\n",
      "L1 loss: 0.07882451266050339\n",
      "Training batch 53 with loss 0.28629\n",
      "------------------------------------------\n",
      "L1 loss: 0.07721220701932907\n",
      "Training batch 54 with loss 0.27390\n",
      "------------------------------------------\n",
      "L1 loss: 0.06390415877103806\n",
      "Training batch 55 with loss 0.31354\n",
      "------------------------------------------\n",
      "L1 loss: 0.06066848710179329\n",
      "Training batch 56 with loss 0.27160\n",
      "------------------------------------------\n",
      "L1 loss: 0.05600985884666443\n",
      "Training batch 57 with loss 0.33688\n",
      "------------------------------------------\n",
      "L1 loss: 0.06653568148612976\n",
      "Training batch 58 with loss 0.27208\n",
      "------------------------------------------\n",
      "L1 loss: 0.08250108361244202\n",
      "Training batch 59 with loss 0.29422\n",
      "------------------------------------------\n",
      "L1 loss: 0.06479455530643463\n",
      "Training batch 60 with loss 0.28065\n",
      "------------------------------------------\n",
      "L1 loss: 0.09628590941429138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 61 with loss 0.34073\n",
      "------------------------------------------\n",
      "L1 loss: 0.05505077913403511\n",
      "Training batch 62 with loss 0.25608\n",
      "------------------------------------------\n",
      "L1 loss: 0.06275282055139542\n",
      "Training batch 63 with loss 0.27002\n",
      "------------------------------------------\n",
      "L1 loss: 0.06902532279491425\n",
      "Training batch 64 with loss 0.28413\n",
      "------------------------------------------\n",
      "L1 loss: 0.06163932755589485\n",
      "Training batch 65 with loss 0.22536\n",
      "------------------------------------------\n",
      "L1 loss: 0.06885982304811478\n",
      "Training batch 66 with loss 0.28147\n",
      "------------------------------------------\n",
      "L1 loss: 0.06471345573663712\n",
      "Training batch 67 with loss 0.26785\n",
      "------------------------------------------\n",
      "L1 loss: 0.07281521707773209\n",
      "Training batch 68 with loss 0.30533\n",
      "------------------------------------------\n",
      "L1 loss: 0.05085796117782593\n",
      "Training batch 69 with loss 0.25319\n",
      "------------------------------------------\n",
      "L1 loss: 0.07638705521821976\n",
      "Training batch 70 with loss 0.30934\n",
      "------------------------------------------\n",
      "L1 loss: 0.07166343927383423\n",
      "Training batch 71 with loss 0.31383\n",
      "------------------------------------------\n",
      "L1 loss: 0.08584711700677872\n",
      "Training batch 72 with loss 0.29928\n",
      "------------------------------------------\n",
      "L1 loss: 0.07203169167041779\n",
      "Training batch 73 with loss 0.28933\n",
      "------------------------------------------\n",
      "L1 loss: 0.09721457958221436\n",
      "Training batch 74 with loss 0.30655\n",
      "------------------------------------------\n",
      "L1 loss: 0.07448720932006836\n",
      "Training batch 75 with loss 0.27598\n",
      "------------------------------------------\n",
      "L1 loss: 0.08662135899066925\n",
      "Training batch 76 with loss 0.27995\n",
      "------------------------------------------\n",
      "L1 loss: 0.07687247544527054\n",
      "Training batch 77 with loss 0.25568\n",
      "------------------------------------------\n",
      "L1 loss: 0.08387849479913712\n",
      "Training batch 78 with loss 0.31389\n",
      "------------------------------------------\n",
      "L1 loss: 0.10595874488353729\n",
      "Training batch 79 with loss 0.38526\n",
      "------------------------------------------\n",
      "L1 loss: 0.07647734135389328\n",
      "Training batch 80 with loss 0.28859\n",
      "------------------------------------------\n",
      "L1 loss: 0.07852979004383087\n",
      "Training batch 81 with loss 0.29114\n",
      "------------------------------------------\n",
      "L1 loss: 0.08677946776151657\n",
      "Training batch 82 with loss 0.35706\n",
      "------------------------------------------\n",
      "L1 loss: 0.07414868474006653\n",
      "Training batch 83 with loss 0.28014\n",
      "------------------------------------------\n",
      "L1 loss: 0.06930877268314362\n",
      "Training batch 84 with loss 0.29912\n",
      "------------------------------------------\n",
      "L1 loss: 0.06660749018192291\n",
      "Training batch 85 with loss 0.29031\n",
      "------------------------------------------\n",
      "L1 loss: 0.08225341886281967\n",
      "Training batch 86 with loss 0.29464\n",
      "------------------------------------------\n",
      "L1 loss: 0.10084565728902817\n",
      "Training batch 87 with loss 0.39904\n",
      "------------------------------------------\n",
      "L1 loss: 0.0797862559556961\n",
      "Training batch 88 with loss 0.32648\n",
      "------------------------------------------\n",
      "L1 loss: 0.06980720907449722\n",
      "Training batch 89 with loss 0.25971\n",
      "------------------------------------------\n",
      "L1 loss: 0.0765305906534195\n",
      "Training batch 90 with loss 0.27408\n",
      "------------------------------------------\n",
      "L1 loss: 0.06597635895013809\n",
      "Training batch 91 with loss 0.27171\n",
      "------------------------------------------\n",
      "L1 loss: 0.09305191040039062\n",
      "Training batch 92 with loss 0.29752\n",
      "------------------------------------------\n",
      "L1 loss: 0.06186468154191971\n",
      "Training batch 93 with loss 0.23933\n",
      "------------------------------------------\n",
      "L1 loss: 0.08671072125434875\n",
      "Training batch 94 with loss 0.32142\n",
      "------------------------------------------\n",
      "L1 loss: 0.07279148697853088\n",
      "Training batch 95 with loss 0.26204\n",
      "------------------------------------------\n",
      "L1 loss: 0.06621144711971283\n",
      "Training batch 96 with loss 0.29306\n",
      "------------------------------------------\n",
      "L1 loss: 0.0852886214852333\n",
      "Training batch 97 with loss 0.35506\n",
      "------------------------------------------\n",
      "L1 loss: 0.05838419869542122\n",
      "Training batch 98 with loss 0.24366\n",
      "------------------------------------------\n",
      "L1 loss: 0.06589187681674957\n",
      "Training batch 99 with loss 0.26440\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29521374151110646\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4845\n",
      "------------------------------------------\n",
      "L1 loss: 0.06215961277484894\n",
      "Training batch 0 with loss 0.24743\n",
      "------------------------------------------\n",
      "L1 loss: 0.0870567113161087\n",
      "Training batch 1 with loss 0.29661\n",
      "------------------------------------------\n",
      "L1 loss: 0.07114778459072113\n",
      "Training batch 2 with loss 0.31726\n",
      "------------------------------------------\n",
      "L1 loss: 0.07521966099739075\n",
      "Training batch 3 with loss 0.26621\n",
      "------------------------------------------\n",
      "L1 loss: 0.08468542993068695\n",
      "Training batch 4 with loss 0.32250\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937702745199203\n",
      "Training batch 5 with loss 0.32462\n",
      "------------------------------------------\n",
      "L1 loss: 0.07809571921825409\n",
      "Training batch 6 with loss 0.27763\n",
      "------------------------------------------\n",
      "L1 loss: 0.08404747396707535\n",
      "Training batch 7 with loss 0.34311\n",
      "------------------------------------------\n",
      "L1 loss: 0.0777289941906929\n",
      "Training batch 8 with loss 0.30514\n",
      "------------------------------------------\n",
      "L1 loss: 0.06134984269738197\n",
      "Training batch 9 with loss 0.24165\n",
      "------------------------------------------\n",
      "L1 loss: 0.08485264331102371\n",
      "Training batch 10 with loss 0.28579\n",
      "------------------------------------------\n",
      "L1 loss: 0.0641360655426979\n",
      "Training batch 11 with loss 0.30215\n",
      "------------------------------------------\n",
      "L1 loss: 0.071657694876194\n",
      "Training batch 12 with loss 0.31357\n",
      "------------------------------------------\n",
      "L1 loss: 0.06999311596155167\n",
      "Training batch 13 with loss 0.24881\n",
      "------------------------------------------\n",
      "L1 loss: 0.05241493135690689\n",
      "Training batch 14 with loss 0.25713\n",
      "------------------------------------------\n",
      "L1 loss: 0.08571550995111465\n",
      "Training batch 15 with loss 0.27467\n",
      "------------------------------------------\n",
      "L1 loss: 0.06313475221395493\n",
      "Training batch 16 with loss 0.26542\n",
      "------------------------------------------\n",
      "L1 loss: 0.08807262778282166\n",
      "Training batch 17 with loss 0.33396\n",
      "------------------------------------------\n",
      "L1 loss: 0.0815667137503624\n",
      "Training batch 18 with loss 0.30629\n",
      "------------------------------------------\n",
      "L1 loss: 0.062023092061281204\n",
      "Training batch 19 with loss 0.23147\n",
      "------------------------------------------\n",
      "L1 loss: 0.07262712717056274\n",
      "Training batch 20 with loss 0.32219\n",
      "------------------------------------------\n",
      "L1 loss: 0.07259820401668549\n",
      "Training batch 21 with loss 0.27786\n",
      "------------------------------------------\n",
      "L1 loss: 0.06820299476385117\n",
      "Training batch 22 with loss 0.28258\n",
      "------------------------------------------\n",
      "L1 loss: 0.09229414910078049\n",
      "Training batch 23 with loss 0.36684\n",
      "------------------------------------------\n",
      "L1 loss: 0.06856391578912735\n",
      "Training batch 24 with loss 0.30306\n",
      "------------------------------------------\n",
      "L1 loss: 0.07566265016794205\n",
      "Training batch 25 with loss 0.30701\n",
      "------------------------------------------\n",
      "L1 loss: 0.08002156764268875\n",
      "Training batch 26 with loss 0.28752\n",
      "------------------------------------------\n",
      "L1 loss: 0.056497786194086075\n",
      "Training batch 27 with loss 0.30528\n",
      "------------------------------------------\n",
      "L1 loss: 0.056797780096530914\n",
      "Training batch 28 with loss 0.30395\n",
      "------------------------------------------\n",
      "L1 loss: 0.05887691304087639\n",
      "Training batch 29 with loss 0.25187\n",
      "------------------------------------------\n",
      "L1 loss: 0.06207738444209099\n",
      "Training batch 30 with loss 0.26311\n",
      "------------------------------------------\n",
      "L1 loss: 0.06482316553592682\n",
      "Training batch 31 with loss 0.30720\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937369704246521\n",
      "Training batch 32 with loss 0.28771\n",
      "------------------------------------------\n",
      "L1 loss: 0.06090014427900314\n",
      "Training batch 33 with loss 0.26750\n",
      "------------------------------------------\n",
      "L1 loss: 0.07456960529088974\n",
      "Training batch 34 with loss 0.30338\n",
      "------------------------------------------\n",
      "L1 loss: 0.09450836479663849\n",
      "Training batch 35 with loss 0.32795\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0882483571767807\n",
      "Training batch 36 with loss 0.28011\n",
      "------------------------------------------\n",
      "L1 loss: 0.06850823760032654\n",
      "Training batch 37 with loss 0.25661\n",
      "------------------------------------------\n",
      "L1 loss: 0.0637926533818245\n",
      "Training batch 38 with loss 0.22370\n",
      "------------------------------------------\n",
      "L1 loss: 0.06537748128175735\n",
      "Training batch 39 with loss 0.28597\n",
      "------------------------------------------\n",
      "L1 loss: 0.06799526512622833\n",
      "Training batch 40 with loss 0.24858\n",
      "------------------------------------------\n",
      "L1 loss: 0.060924187302589417\n",
      "Training batch 41 with loss 0.29053\n",
      "------------------------------------------\n",
      "L1 loss: 0.07621029019355774\n",
      "Training batch 42 with loss 0.26759\n",
      "------------------------------------------\n",
      "L1 loss: 0.0659521222114563\n",
      "Training batch 43 with loss 0.27354\n",
      "------------------------------------------\n",
      "L1 loss: 0.05533492565155029\n",
      "Training batch 44 with loss 0.26921\n",
      "------------------------------------------\n",
      "L1 loss: 0.09257875382900238\n",
      "Training batch 45 with loss 0.35683\n",
      "------------------------------------------\n",
      "L1 loss: 0.07282591611146927\n",
      "Training batch 46 with loss 0.26892\n",
      "------------------------------------------\n",
      "L1 loss: 0.11104562878608704\n",
      "Training batch 47 with loss 0.37109\n",
      "------------------------------------------\n",
      "L1 loss: 0.04916984960436821\n",
      "Training batch 48 with loss 0.26049\n",
      "------------------------------------------\n",
      "L1 loss: 0.0825144350528717\n",
      "Training batch 49 with loss 0.35586\n",
      "------------------------------------------\n",
      "L1 loss: 0.07196763157844543\n",
      "Training batch 50 with loss 0.27358\n",
      "------------------------------------------\n",
      "L1 loss: 0.08187202364206314\n",
      "Training batch 51 with loss 0.33567\n",
      "------------------------------------------\n",
      "L1 loss: 0.08958927541971207\n",
      "Training batch 52 with loss 0.28424\n",
      "------------------------------------------\n",
      "L1 loss: 0.0766596570611\n",
      "Training batch 53 with loss 0.28543\n",
      "------------------------------------------\n",
      "L1 loss: 0.07139338552951813\n",
      "Training batch 54 with loss 0.25610\n",
      "------------------------------------------\n",
      "L1 loss: 0.06264179944992065\n",
      "Training batch 55 with loss 0.31170\n",
      "------------------------------------------\n",
      "L1 loss: 0.05105246603488922\n",
      "Training batch 56 with loss 0.22925\n",
      "------------------------------------------\n",
      "L1 loss: 0.06993155926465988\n",
      "Training batch 57 with loss 0.27748\n",
      "------------------------------------------\n",
      "L1 loss: 0.06382234394550323\n",
      "Training batch 58 with loss 0.24771\n",
      "------------------------------------------\n",
      "L1 loss: 0.07816864550113678\n",
      "Training batch 59 with loss 0.29717\n",
      "------------------------------------------\n",
      "L1 loss: 0.06509380042552948\n",
      "Training batch 60 with loss 0.28582\n",
      "------------------------------------------\n",
      "L1 loss: 0.09936205297708511\n",
      "Training batch 61 with loss 0.36360\n",
      "------------------------------------------\n",
      "L1 loss: 0.04156975448131561\n",
      "Training batch 62 with loss 0.33848\n",
      "------------------------------------------\n",
      "L1 loss: 0.06162824481725693\n",
      "Training batch 63 with loss 0.26281\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683838501572609\n",
      "Training batch 64 with loss 0.26840\n",
      "------------------------------------------\n",
      "L1 loss: 0.06297548860311508\n",
      "Training batch 65 with loss 0.24742\n",
      "------------------------------------------\n",
      "L1 loss: 0.061937469989061356\n",
      "Training batch 66 with loss 0.51439\n",
      "------------------------------------------\n",
      "L1 loss: 0.06565149128437042\n",
      "Training batch 67 with loss 0.26514\n",
      "------------------------------------------\n",
      "L1 loss: 0.07349399477243423\n",
      "Training batch 68 with loss 0.30167\n",
      "------------------------------------------\n",
      "L1 loss: 0.050528835505247116\n",
      "Training batch 69 with loss 0.26267\n",
      "------------------------------------------\n",
      "L1 loss: 0.081911101937294\n",
      "Training batch 70 with loss 0.31731\n",
      "------------------------------------------\n",
      "L1 loss: 0.07198771089315414\n",
      "Training batch 71 with loss 0.30574\n",
      "------------------------------------------\n",
      "L1 loss: 0.0727706253528595\n",
      "Training batch 72 with loss 0.55713\n",
      "------------------------------------------\n",
      "L1 loss: 0.07007376104593277\n",
      "Training batch 73 with loss 0.27685\n",
      "------------------------------------------\n",
      "L1 loss: 0.09828805923461914\n",
      "Training batch 74 with loss 0.32525\n",
      "------------------------------------------\n",
      "L1 loss: 0.07545363157987595\n",
      "Training batch 75 with loss 0.28189\n",
      "------------------------------------------\n",
      "L1 loss: 0.08600595593452454\n",
      "Training batch 76 with loss 0.29057\n",
      "------------------------------------------\n",
      "L1 loss: 0.07798805832862854\n",
      "Training batch 77 with loss 0.26946\n",
      "------------------------------------------\n",
      "L1 loss: 0.07773452252149582\n",
      "Training batch 78 with loss 0.29198\n",
      "------------------------------------------\n",
      "L1 loss: 0.10268745571374893\n",
      "Training batch 79 with loss 0.38008\n",
      "------------------------------------------\n",
      "L1 loss: 0.0768999382853508\n",
      "Training batch 80 with loss 0.28160\n",
      "------------------------------------------\n",
      "L1 loss: 0.07680419832468033\n",
      "Training batch 81 with loss 0.29139\n",
      "------------------------------------------\n",
      "L1 loss: 0.0864810049533844\n",
      "Training batch 82 with loss 0.34832\n",
      "------------------------------------------\n",
      "L1 loss: 0.08076311647891998\n",
      "Training batch 83 with loss 0.28114\n",
      "------------------------------------------\n",
      "L1 loss: 0.07058146595954895\n",
      "Training batch 84 with loss 0.30537\n",
      "------------------------------------------\n",
      "L1 loss: 0.060616303235292435\n",
      "Training batch 85 with loss 0.26576\n",
      "------------------------------------------\n",
      "L1 loss: 0.08343740552663803\n",
      "Training batch 86 with loss 0.29513\n",
      "------------------------------------------\n",
      "L1 loss: 0.0998024120926857\n",
      "Training batch 87 with loss 0.39308\n",
      "------------------------------------------\n",
      "L1 loss: 0.07811398804187775\n",
      "Training batch 88 with loss 0.32344\n",
      "------------------------------------------\n",
      "L1 loss: 0.06860963255167007\n",
      "Training batch 89 with loss 0.25741\n",
      "------------------------------------------\n",
      "L1 loss: 0.07381068170070648\n",
      "Training batch 90 with loss 0.29541\n",
      "------------------------------------------\n",
      "L1 loss: 0.05892781913280487\n",
      "Training batch 91 with loss 0.25620\n",
      "------------------------------------------\n",
      "L1 loss: 0.09221917390823364\n",
      "Training batch 92 with loss 0.28888\n",
      "------------------------------------------\n",
      "L1 loss: 0.06452206522226334\n",
      "Training batch 93 with loss 0.24321\n",
      "------------------------------------------\n",
      "L1 loss: 0.08130200207233429\n",
      "Training batch 94 with loss 0.29461\n",
      "------------------------------------------\n",
      "L1 loss: 0.07068263739347458\n",
      "Training batch 95 with loss 0.25854\n",
      "------------------------------------------\n",
      "L1 loss: 0.060105517506599426\n",
      "Training batch 96 with loss 0.58761\n",
      "------------------------------------------\n",
      "L1 loss: 0.08741021156311035\n",
      "Training batch 97 with loss 0.35493\n",
      "------------------------------------------\n",
      "L1 loss: 0.05635960027575493\n",
      "Training batch 98 with loss 0.23193\n",
      "------------------------------------------\n",
      "L1 loss: 0.06285963207483292\n",
      "Training batch 99 with loss 0.26093\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29855347886681555\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4846\n",
      "------------------------------------------\n",
      "L1 loss: 0.06451652199029922\n",
      "Training batch 0 with loss 0.26298\n",
      "------------------------------------------\n",
      "L1 loss: 0.08412180840969086\n",
      "Training batch 1 with loss 0.29271\n",
      "------------------------------------------\n",
      "L1 loss: 0.0937623605132103\n",
      "Training batch 2 with loss 0.33434\n",
      "------------------------------------------\n",
      "L1 loss: 0.07713329046964645\n",
      "Training batch 3 with loss 0.27550\n",
      "------------------------------------------\n",
      "L1 loss: 0.08631246536970139\n",
      "Training batch 4 with loss 0.32577\n",
      "------------------------------------------\n",
      "L1 loss: 0.06675480306148529\n",
      "Training batch 5 with loss 0.32087\n",
      "------------------------------------------\n",
      "L1 loss: 0.0784551203250885\n",
      "Training batch 6 with loss 0.26002\n",
      "------------------------------------------\n",
      "L1 loss: 0.08220137655735016\n",
      "Training batch 7 with loss 0.31556\n",
      "------------------------------------------\n",
      "L1 loss: 0.0799887552857399\n",
      "Training batch 8 with loss 0.31860\n",
      "------------------------------------------\n",
      "L1 loss: 0.057848066091537476\n",
      "Training batch 9 with loss 0.24934\n",
      "------------------------------------------\n",
      "L1 loss: 0.08272160589694977\n",
      "Training batch 10 with loss 0.29694\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0575847327709198\n",
      "Training batch 11 with loss 0.31540\n",
      "------------------------------------------\n",
      "L1 loss: 0.07570386677980423\n",
      "Training batch 12 with loss 0.31495\n",
      "------------------------------------------\n",
      "L1 loss: 0.07082813233137131\n",
      "Training batch 13 with loss 0.24452\n",
      "------------------------------------------\n",
      "L1 loss: 0.05273282527923584\n",
      "Training batch 14 with loss 0.25437\n",
      "------------------------------------------\n",
      "L1 loss: 0.08554847538471222\n",
      "Training batch 15 with loss 0.27167\n",
      "------------------------------------------\n",
      "L1 loss: 0.06329936534166336\n",
      "Training batch 16 with loss 0.27216\n",
      "------------------------------------------\n",
      "L1 loss: 0.08317344635725021\n",
      "Training batch 17 with loss 0.32469\n",
      "------------------------------------------\n",
      "L1 loss: 0.08042648434638977\n",
      "Training batch 18 with loss 0.31264\n",
      "------------------------------------------\n",
      "L1 loss: 0.058158230036497116\n",
      "Training batch 19 with loss 0.22921\n",
      "------------------------------------------\n",
      "L1 loss: 0.07421860098838806\n",
      "Training batch 20 with loss 0.32742\n",
      "------------------------------------------\n",
      "L1 loss: 0.06934473663568497\n",
      "Training batch 21 with loss 0.27794\n",
      "------------------------------------------\n",
      "L1 loss: 0.06899211555719376\n",
      "Training batch 22 with loss 0.29112\n",
      "------------------------------------------\n",
      "L1 loss: 0.09331588447093964\n",
      "Training batch 23 with loss 0.36932\n",
      "------------------------------------------\n",
      "L1 loss: 0.06758265942335129\n",
      "Training batch 24 with loss 0.30856\n",
      "------------------------------------------\n",
      "L1 loss: 0.07665014266967773\n",
      "Training batch 25 with loss 0.32001\n",
      "------------------------------------------\n",
      "L1 loss: 0.07700641453266144\n",
      "Training batch 26 with loss 0.28318\n",
      "------------------------------------------\n",
      "L1 loss: 0.057135969400405884\n",
      "Training batch 27 with loss 0.30470\n",
      "------------------------------------------\n",
      "L1 loss: 0.058073971420526505\n",
      "Training batch 28 with loss 0.27572\n",
      "------------------------------------------\n",
      "L1 loss: 0.05931719020009041\n",
      "Training batch 29 with loss 0.24997\n",
      "------------------------------------------\n",
      "L1 loss: 0.06124702841043472\n",
      "Training batch 30 with loss 0.27356\n",
      "------------------------------------------\n",
      "L1 loss: 0.06423445045948029\n",
      "Training batch 31 with loss 0.29405\n",
      "------------------------------------------\n",
      "L1 loss: 0.07056159526109695\n",
      "Training batch 32 with loss 0.27539\n",
      "------------------------------------------\n",
      "L1 loss: 0.0643446296453476\n",
      "Training batch 33 with loss 0.26102\n",
      "------------------------------------------\n",
      "L1 loss: 0.07649306207895279\n",
      "Training batch 34 with loss 0.31624\n",
      "------------------------------------------\n",
      "L1 loss: 0.09300048649311066\n",
      "Training batch 35 with loss 0.33607\n",
      "------------------------------------------\n",
      "L1 loss: 0.08953465521335602\n",
      "Training batch 36 with loss 0.28164\n",
      "------------------------------------------\n",
      "L1 loss: 0.06988297402858734\n",
      "Training batch 37 with loss 0.27991\n",
      "------------------------------------------\n",
      "L1 loss: 0.0626581460237503\n",
      "Training batch 38 with loss 0.22483\n",
      "------------------------------------------\n",
      "L1 loss: 0.06855525076389313\n",
      "Training batch 39 with loss 0.29944\n",
      "------------------------------------------\n",
      "L1 loss: 0.07056303322315216\n",
      "Training batch 40 with loss 0.25630\n",
      "------------------------------------------\n",
      "L1 loss: 0.05185547098517418\n",
      "Training batch 41 with loss 0.33936\n",
      "------------------------------------------\n",
      "L1 loss: 0.07588518410921097\n",
      "Training batch 42 with loss 0.28263\n",
      "------------------------------------------\n",
      "L1 loss: 0.07102201879024506\n",
      "Training batch 43 with loss 0.29819\n",
      "------------------------------------------\n",
      "L1 loss: 0.05868757516145706\n",
      "Training batch 44 with loss 0.27101\n",
      "------------------------------------------\n",
      "L1 loss: 0.092566579580307\n",
      "Training batch 45 with loss 0.36430\n",
      "------------------------------------------\n",
      "L1 loss: 0.0761856883764267\n",
      "Training batch 46 with loss 0.27295\n",
      "------------------------------------------\n",
      "L1 loss: 0.1082274541258812\n",
      "Training batch 47 with loss 0.36716\n",
      "------------------------------------------\n",
      "L1 loss: 0.04801258444786072\n",
      "Training batch 48 with loss 0.25027\n",
      "------------------------------------------\n",
      "L1 loss: 0.08168929070234299\n",
      "Training batch 49 with loss 0.36307\n",
      "------------------------------------------\n",
      "L1 loss: 0.07486489415168762\n",
      "Training batch 50 with loss 0.29837\n",
      "------------------------------------------\n",
      "L1 loss: 0.07835284620523453\n",
      "Training batch 51 with loss 0.33758\n",
      "------------------------------------------\n",
      "L1 loss: 0.0875779539346695\n",
      "Training batch 52 with loss 0.29446\n",
      "------------------------------------------\n",
      "L1 loss: 0.07916060090065002\n",
      "Training batch 53 with loss 0.30028\n",
      "------------------------------------------\n",
      "L1 loss: 0.07665595412254333\n",
      "Training batch 54 with loss 0.27022\n",
      "------------------------------------------\n",
      "L1 loss: 0.060532163828611374\n",
      "Training batch 55 with loss 0.30228\n",
      "------------------------------------------\n",
      "L1 loss: 0.0533432774245739\n",
      "Training batch 56 with loss 0.24568\n",
      "------------------------------------------\n",
      "L1 loss: 0.06727191805839539\n",
      "Training batch 57 with loss 0.27549\n",
      "------------------------------------------\n",
      "L1 loss: 0.06661024689674377\n",
      "Training batch 58 with loss 0.26628\n",
      "------------------------------------------\n",
      "L1 loss: 0.08270321786403656\n",
      "Training batch 59 with loss 0.30039\n",
      "------------------------------------------\n",
      "L1 loss: 0.06571947783231735\n",
      "Training batch 60 with loss 0.27477\n",
      "------------------------------------------\n",
      "L1 loss: 0.10390114039182663\n",
      "Training batch 61 with loss 0.37463\n",
      "------------------------------------------\n",
      "L1 loss: 0.056250132620334625\n",
      "Training batch 62 with loss 0.25477\n",
      "------------------------------------------\n",
      "L1 loss: 0.05985294654965401\n",
      "Training batch 63 with loss 0.24552\n",
      "------------------------------------------\n",
      "L1 loss: 0.06979979574680328\n",
      "Training batch 64 with loss 0.27532\n",
      "------------------------------------------\n",
      "L1 loss: 0.05915525183081627\n",
      "Training batch 65 with loss 0.22876\n",
      "------------------------------------------\n",
      "L1 loss: 0.0655384436249733\n",
      "Training batch 66 with loss 0.27930\n",
      "------------------------------------------\n",
      "L1 loss: 0.058899153023958206\n",
      "Training batch 67 with loss 0.23790\n",
      "------------------------------------------\n",
      "L1 loss: 0.07279683649539948\n",
      "Training batch 68 with loss 0.30456\n",
      "------------------------------------------\n",
      "L1 loss: 0.05090297758579254\n",
      "Training batch 69 with loss 0.26300\n",
      "------------------------------------------\n",
      "L1 loss: 0.07181381434202194\n",
      "Training batch 70 with loss 0.46985\n",
      "------------------------------------------\n",
      "L1 loss: 0.07265463471412659\n",
      "Training batch 71 with loss 0.29974\n",
      "------------------------------------------\n",
      "L1 loss: 0.08571573346853256\n",
      "Training batch 72 with loss 0.30658\n",
      "------------------------------------------\n",
      "L1 loss: 0.07203751802444458\n",
      "Training batch 73 with loss 0.29660\n",
      "------------------------------------------\n",
      "L1 loss: 0.0965484157204628\n",
      "Training batch 74 with loss 0.31419\n",
      "------------------------------------------\n",
      "L1 loss: 0.07593714445829391\n",
      "Training batch 75 with loss 0.27981\n",
      "------------------------------------------\n",
      "L1 loss: 0.08705741167068481\n",
      "Training batch 76 with loss 0.29374\n",
      "------------------------------------------\n",
      "L1 loss: 0.0786098837852478\n",
      "Training batch 77 with loss 0.27195\n",
      "------------------------------------------\n",
      "L1 loss: 0.08129948377609253\n",
      "Training batch 78 with loss 0.30931\n",
      "------------------------------------------\n",
      "L1 loss: 0.10398725420236588\n",
      "Training batch 79 with loss 0.34724\n",
      "------------------------------------------\n",
      "L1 loss: 0.07702483236789703\n",
      "Training batch 80 with loss 0.27950\n",
      "------------------------------------------\n",
      "L1 loss: 0.06471458822488785\n",
      "Training batch 81 with loss 0.44160\n",
      "------------------------------------------\n",
      "L1 loss: 0.0869455337524414\n",
      "Training batch 82 with loss 0.35424\n",
      "------------------------------------------\n",
      "L1 loss: 0.07414623349905014\n",
      "Training batch 83 with loss 0.27471\n",
      "------------------------------------------\n",
      "L1 loss: 0.06639698147773743\n",
      "Training batch 84 with loss 0.30809\n",
      "------------------------------------------\n",
      "L1 loss: 0.06071004271507263\n",
      "Training batch 85 with loss 0.27734\n",
      "------------------------------------------\n",
      "L1 loss: 0.08271869271993637\n",
      "Training batch 86 with loss 0.28778\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.10213745385408401\n",
      "Training batch 87 with loss 0.40127\n",
      "------------------------------------------\n",
      "L1 loss: 0.07872944325208664\n",
      "Training batch 88 with loss 0.31473\n",
      "------------------------------------------\n",
      "L1 loss: 0.06902673095464706\n",
      "Training batch 89 with loss 0.25787\n",
      "------------------------------------------\n",
      "L1 loss: 0.07359360158443451\n",
      "Training batch 90 with loss 0.26665\n",
      "------------------------------------------\n",
      "L1 loss: 0.06447062641382217\n",
      "Training batch 91 with loss 0.26863\n",
      "------------------------------------------\n",
      "L1 loss: 0.09376782923936844\n",
      "Training batch 92 with loss 0.30032\n",
      "------------------------------------------\n",
      "L1 loss: 0.0552285835146904\n",
      "Training batch 93 with loss 0.24246\n",
      "------------------------------------------\n",
      "L1 loss: 0.08577478677034378\n",
      "Training batch 94 with loss 0.30990\n",
      "------------------------------------------\n",
      "L1 loss: 0.07206284254789352\n",
      "Training batch 95 with loss 0.25496\n",
      "------------------------------------------\n",
      "L1 loss: 0.05700574070215225\n",
      "Training batch 96 with loss 0.26023\n",
      "------------------------------------------\n",
      "L1 loss: 0.08691447973251343\n",
      "Training batch 97 with loss 0.36079\n",
      "------------------------------------------\n",
      "L1 loss: 0.05646548047661781\n",
      "Training batch 98 with loss 0.23342\n",
      "------------------------------------------\n",
      "L1 loss: 0.06054053455591202\n",
      "Training batch 99 with loss 0.26356\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29484191730618475\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4847\n",
      "------------------------------------------\n",
      "L1 loss: 0.06292027235031128\n",
      "Training batch 0 with loss 0.47603\n",
      "------------------------------------------\n",
      "L1 loss: 0.08199530839920044\n",
      "Training batch 1 with loss 0.28583\n",
      "------------------------------------------\n",
      "L1 loss: 0.09073685109615326\n",
      "Training batch 2 with loss 0.31664\n",
      "------------------------------------------\n",
      "L1 loss: 0.07502492517232895\n",
      "Training batch 3 with loss 0.25408\n",
      "------------------------------------------\n",
      "L1 loss: 0.0830797478556633\n",
      "Training batch 4 with loss 0.33516\n",
      "------------------------------------------\n",
      "L1 loss: 0.06767505407333374\n",
      "Training batch 5 with loss 0.33361\n",
      "------------------------------------------\n",
      "L1 loss: 0.07940305024385452\n",
      "Training batch 6 with loss 0.26967\n",
      "------------------------------------------\n",
      "L1 loss: 0.08469051867723465\n",
      "Training batch 7 with loss 0.33805\n",
      "------------------------------------------\n",
      "L1 loss: 0.08218341320753098\n",
      "Training batch 8 with loss 0.34350\n",
      "------------------------------------------\n",
      "L1 loss: 0.05893874168395996\n",
      "Training batch 9 with loss 0.24687\n",
      "------------------------------------------\n",
      "L1 loss: 0.08157134801149368\n",
      "Training batch 10 with loss 0.28995\n",
      "------------------------------------------\n",
      "L1 loss: 0.05877313017845154\n",
      "Training batch 11 with loss 0.28294\n",
      "------------------------------------------\n",
      "L1 loss: 0.07240845263004303\n",
      "Training batch 12 with loss 0.31021\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990130990743637\n",
      "Training batch 13 with loss 0.26071\n",
      "------------------------------------------\n",
      "L1 loss: 0.05323873087763786\n",
      "Training batch 14 with loss 0.24145\n",
      "------------------------------------------\n",
      "L1 loss: 0.083738312125206\n",
      "Training batch 15 with loss 0.26183\n",
      "------------------------------------------\n",
      "L1 loss: 0.06335931271314621\n",
      "Training batch 16 with loss 0.25649\n",
      "------------------------------------------\n",
      "L1 loss: 0.08633039891719818\n",
      "Training batch 17 with loss 0.33804\n",
      "------------------------------------------\n",
      "L1 loss: 0.08170685917139053\n",
      "Training batch 18 with loss 0.29392\n",
      "------------------------------------------\n",
      "L1 loss: 0.056617241352796555\n",
      "Training batch 19 with loss 0.24778\n",
      "------------------------------------------\n",
      "L1 loss: 0.07380890101194382\n",
      "Training batch 20 with loss 0.31458\n",
      "------------------------------------------\n",
      "L1 loss: 0.07306071370840073\n",
      "Training batch 21 with loss 0.28920\n",
      "------------------------------------------\n",
      "L1 loss: 0.06842001527547836\n",
      "Training batch 22 with loss 0.29194\n",
      "------------------------------------------\n",
      "L1 loss: 0.09285988658666611\n",
      "Training batch 23 with loss 0.36065\n",
      "------------------------------------------\n",
      "L1 loss: 0.06992192566394806\n",
      "Training batch 24 with loss 0.30447\n",
      "------------------------------------------\n",
      "L1 loss: 0.07276753336191177\n",
      "Training batch 25 with loss 0.30151\n",
      "------------------------------------------\n",
      "L1 loss: 0.077352374792099\n",
      "Training batch 26 with loss 0.28877\n",
      "------------------------------------------\n",
      "L1 loss: 0.05587802082300186\n",
      "Training batch 27 with loss 0.29571\n",
      "------------------------------------------\n",
      "L1 loss: 0.058925382792949677\n",
      "Training batch 28 with loss 0.29999\n",
      "------------------------------------------\n",
      "L1 loss: 0.05439997464418411\n",
      "Training batch 29 with loss 0.24537\n",
      "------------------------------------------\n",
      "L1 loss: 0.05813203006982803\n",
      "Training batch 30 with loss 0.26452\n",
      "------------------------------------------\n",
      "L1 loss: 0.06188838183879852\n",
      "Training batch 31 with loss 0.29281\n",
      "------------------------------------------\n",
      "L1 loss: 0.06970155984163284\n",
      "Training batch 32 with loss 0.27732\n",
      "------------------------------------------\n",
      "L1 loss: 0.06292441487312317\n",
      "Training batch 33 with loss 0.27765\n",
      "------------------------------------------\n",
      "L1 loss: 0.07557116448879242\n",
      "Training batch 34 with loss 0.33969\n",
      "------------------------------------------\n",
      "L1 loss: 0.09272211790084839\n",
      "Training batch 35 with loss 0.32846\n",
      "------------------------------------------\n",
      "L1 loss: 0.0870189443230629\n",
      "Training batch 36 with loss 0.26650\n",
      "------------------------------------------\n",
      "L1 loss: 0.06927545368671417\n",
      "Training batch 37 with loss 0.26546\n",
      "------------------------------------------\n",
      "L1 loss: 0.06128811091184616\n",
      "Training batch 38 with loss 0.21748\n",
      "------------------------------------------\n",
      "L1 loss: 0.06431851536035538\n",
      "Training batch 39 with loss 0.27977\n",
      "------------------------------------------\n",
      "L1 loss: 0.07128507643938065\n",
      "Training batch 40 with loss 0.26467\n",
      "------------------------------------------\n",
      "L1 loss: 0.05693674832582474\n",
      "Training batch 41 with loss 0.28934\n",
      "------------------------------------------\n",
      "L1 loss: 0.079615019261837\n",
      "Training batch 42 with loss 0.27749\n",
      "------------------------------------------\n",
      "L1 loss: 0.06161530315876007\n",
      "Training batch 43 with loss 0.45853\n",
      "------------------------------------------\n",
      "L1 loss: 0.04560643807053566\n",
      "Training batch 44 with loss 0.27206\n",
      "------------------------------------------\n",
      "L1 loss: 0.0918290764093399\n",
      "Training batch 45 with loss 0.36781\n",
      "------------------------------------------\n",
      "L1 loss: 0.07567227631807327\n",
      "Training batch 46 with loss 0.26262\n",
      "------------------------------------------\n",
      "L1 loss: 0.11299508064985275\n",
      "Training batch 47 with loss 0.38437\n",
      "------------------------------------------\n",
      "L1 loss: 0.045801397413015366\n",
      "Training batch 48 with loss 0.23750\n",
      "------------------------------------------\n",
      "L1 loss: 0.08097805082798004\n",
      "Training batch 49 with loss 0.34324\n",
      "------------------------------------------\n",
      "L1 loss: 0.07262367010116577\n",
      "Training batch 50 with loss 0.29642\n",
      "------------------------------------------\n",
      "L1 loss: 0.08444298803806305\n",
      "Training batch 51 with loss 0.33381\n",
      "------------------------------------------\n",
      "L1 loss: 0.09128626435995102\n",
      "Training batch 52 with loss 0.29041\n",
      "------------------------------------------\n",
      "L1 loss: 0.08115497976541519\n",
      "Training batch 53 with loss 0.28944\n",
      "------------------------------------------\n",
      "L1 loss: 0.07144714146852493\n",
      "Training batch 54 with loss 0.26580\n",
      "------------------------------------------\n",
      "L1 loss: 0.06111499294638634\n",
      "Training batch 55 with loss 0.29330\n",
      "------------------------------------------\n",
      "L1 loss: 0.05240615829825401\n",
      "Training batch 56 with loss 0.24650\n",
      "------------------------------------------\n",
      "L1 loss: 0.06430205702781677\n",
      "Training batch 57 with loss 0.26896\n",
      "------------------------------------------\n",
      "L1 loss: 0.07010743767023087\n",
      "Training batch 58 with loss 0.25593\n",
      "------------------------------------------\n",
      "L1 loss: 0.08320305496454239\n",
      "Training batch 59 with loss 0.29063\n",
      "------------------------------------------\n",
      "L1 loss: 0.06726813316345215\n",
      "Training batch 60 with loss 0.28690\n",
      "------------------------------------------\n",
      "L1 loss: 0.10480136424303055\n",
      "Training batch 61 with loss 0.35871\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.05738314986228943\n",
      "Training batch 62 with loss 0.26043\n",
      "------------------------------------------\n",
      "L1 loss: 0.06376983970403671\n",
      "Training batch 63 with loss 0.25995\n",
      "------------------------------------------\n",
      "L1 loss: 0.06968401372432709\n",
      "Training batch 64 with loss 0.27891\n",
      "------------------------------------------\n",
      "L1 loss: 0.06336191296577454\n",
      "Training batch 65 with loss 0.24197\n",
      "------------------------------------------\n",
      "L1 loss: 0.06691303849220276\n",
      "Training batch 66 with loss 0.26365\n",
      "------------------------------------------\n",
      "L1 loss: 0.06510289758443832\n",
      "Training batch 67 with loss 0.25645\n",
      "------------------------------------------\n",
      "L1 loss: 0.07112063467502594\n",
      "Training batch 68 with loss 0.30054\n",
      "------------------------------------------\n",
      "L1 loss: 0.05052154138684273\n",
      "Training batch 69 with loss 0.26909\n",
      "------------------------------------------\n",
      "L1 loss: 0.08018646389245987\n",
      "Training batch 70 with loss 0.31642\n",
      "------------------------------------------\n",
      "L1 loss: 0.07167188823223114\n",
      "Training batch 71 with loss 0.29562\n",
      "------------------------------------------\n",
      "L1 loss: 0.0876256600022316\n",
      "Training batch 72 with loss 0.33165\n",
      "------------------------------------------\n",
      "L1 loss: 0.07395090907812119\n",
      "Training batch 73 with loss 0.30582\n",
      "------------------------------------------\n",
      "L1 loss: 0.09777341037988663\n",
      "Training batch 74 with loss 0.31226\n",
      "------------------------------------------\n",
      "L1 loss: 0.0749967023730278\n",
      "Training batch 75 with loss 0.28698\n",
      "------------------------------------------\n",
      "L1 loss: 0.08557448536157608\n",
      "Training batch 76 with loss 0.30032\n",
      "------------------------------------------\n",
      "L1 loss: 0.07993292063474655\n",
      "Training batch 77 with loss 0.26423\n",
      "------------------------------------------\n",
      "L1 loss: 0.08159932494163513\n",
      "Training batch 78 with loss 0.30656\n",
      "------------------------------------------\n",
      "L1 loss: 0.10693149268627167\n",
      "Training batch 79 with loss 0.37415\n",
      "------------------------------------------\n",
      "L1 loss: 0.07692651450634003\n",
      "Training batch 80 with loss 0.28809\n",
      "------------------------------------------\n",
      "L1 loss: 0.0717134028673172\n",
      "Training batch 81 with loss 0.28264\n",
      "------------------------------------------\n",
      "L1 loss: 0.08345754444599152\n",
      "Training batch 82 with loss 0.35712\n",
      "------------------------------------------\n",
      "L1 loss: 0.07823585718870163\n",
      "Training batch 83 with loss 0.27343\n",
      "------------------------------------------\n",
      "L1 loss: 0.06636713445186615\n",
      "Training batch 84 with loss 0.28820\n",
      "------------------------------------------\n",
      "L1 loss: 0.06381688266992569\n",
      "Training batch 85 with loss 0.28477\n",
      "------------------------------------------\n",
      "L1 loss: 0.08250819146633148\n",
      "Training batch 86 with loss 0.28761\n",
      "------------------------------------------\n",
      "L1 loss: 0.10105885565280914\n",
      "Training batch 87 with loss 0.40713\n",
      "------------------------------------------\n",
      "L1 loss: 0.08027464151382446\n",
      "Training batch 88 with loss 0.32388\n",
      "------------------------------------------\n",
      "L1 loss: 0.07074248790740967\n",
      "Training batch 89 with loss 0.26169\n",
      "------------------------------------------\n",
      "L1 loss: 0.0727323666214943\n",
      "Training batch 90 with loss 0.27120\n",
      "------------------------------------------\n",
      "L1 loss: 0.06062624976038933\n",
      "Training batch 91 with loss 0.25918\n",
      "------------------------------------------\n",
      "L1 loss: 0.08592472970485687\n",
      "Training batch 92 with loss 0.56168\n",
      "------------------------------------------\n",
      "L1 loss: 0.06815770268440247\n",
      "Training batch 93 with loss 0.25903\n",
      "------------------------------------------\n",
      "L1 loss: 0.08489309996366501\n",
      "Training batch 94 with loss 0.29085\n",
      "------------------------------------------\n",
      "L1 loss: 0.07085680216550827\n",
      "Training batch 95 with loss 0.25607\n",
      "------------------------------------------\n",
      "L1 loss: 0.07217147946357727\n",
      "Training batch 96 with loss 0.29878\n",
      "------------------------------------------\n",
      "L1 loss: 0.08500514179468155\n",
      "Training batch 97 with loss 0.35425\n",
      "------------------------------------------\n",
      "L1 loss: 0.05535294488072395\n",
      "Training batch 98 with loss 0.24051\n",
      "------------------------------------------\n",
      "L1 loss: 0.06116455793380737\n",
      "Training batch 99 with loss 0.26300\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2975315360724926\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4848\n",
      "------------------------------------------\n",
      "L1 loss: 0.05914543941617012\n",
      "Training batch 0 with loss 0.25581\n",
      "------------------------------------------\n",
      "L1 loss: 0.0819547101855278\n",
      "Training batch 1 with loss 0.28657\n",
      "------------------------------------------\n",
      "L1 loss: 0.09303684532642365\n",
      "Training batch 2 with loss 0.32155\n",
      "------------------------------------------\n",
      "L1 loss: 0.0815916433930397\n",
      "Training batch 3 with loss 0.28460\n",
      "------------------------------------------\n",
      "L1 loss: 0.08409177511930466\n",
      "Training batch 4 with loss 0.33940\n",
      "------------------------------------------\n",
      "L1 loss: 0.06791772693395615\n",
      "Training batch 5 with loss 0.33566\n",
      "------------------------------------------\n",
      "L1 loss: 0.07667539268732071\n",
      "Training batch 6 with loss 0.26897\n",
      "------------------------------------------\n",
      "L1 loss: 0.08213400095701218\n",
      "Training batch 7 with loss 0.31825\n",
      "------------------------------------------\n",
      "L1 loss: 0.08170896768569946\n",
      "Training batch 8 with loss 0.33402\n",
      "------------------------------------------\n",
      "L1 loss: 0.056009434163570404\n",
      "Training batch 9 with loss 0.25097\n",
      "------------------------------------------\n",
      "L1 loss: 0.08377815783023834\n",
      "Training batch 10 with loss 0.28504\n",
      "------------------------------------------\n",
      "L1 loss: 0.061794500797986984\n",
      "Training batch 11 with loss 0.30947\n",
      "------------------------------------------\n",
      "L1 loss: 0.07588649541139603\n",
      "Training batch 12 with loss 0.31734\n",
      "------------------------------------------\n",
      "L1 loss: 0.06816843152046204\n",
      "Training batch 13 with loss 0.24070\n",
      "------------------------------------------\n",
      "L1 loss: 0.05174423009157181\n",
      "Training batch 14 with loss 0.23995\n",
      "------------------------------------------\n",
      "L1 loss: 0.08426141738891602\n",
      "Training batch 15 with loss 0.27114\n",
      "------------------------------------------\n",
      "L1 loss: 0.06008867174386978\n",
      "Training batch 16 with loss 0.41697\n",
      "------------------------------------------\n",
      "L1 loss: 0.09042651951313019\n",
      "Training batch 17 with loss 0.35100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08047723770141602\n",
      "Training batch 18 with loss 0.30228\n",
      "------------------------------------------\n",
      "L1 loss: 0.06037590280175209\n",
      "Training batch 19 with loss 0.22637\n",
      "------------------------------------------\n",
      "L1 loss: 0.0729350596666336\n",
      "Training batch 20 with loss 0.31409\n",
      "------------------------------------------\n",
      "L1 loss: 0.06963101774454117\n",
      "Training batch 21 with loss 0.27818\n",
      "------------------------------------------\n",
      "L1 loss: 0.0700569748878479\n",
      "Training batch 22 with loss 0.28359\n",
      "------------------------------------------\n",
      "L1 loss: 0.08811694383621216\n",
      "Training batch 23 with loss 0.36799\n",
      "------------------------------------------\n",
      "L1 loss: 0.06763125211000443\n",
      "Training batch 24 with loss 0.30274\n",
      "------------------------------------------\n",
      "L1 loss: 0.07296110689640045\n",
      "Training batch 25 with loss 0.28786\n",
      "------------------------------------------\n",
      "L1 loss: 0.07229935377836227\n",
      "Training batch 26 with loss 0.26561\n",
      "------------------------------------------\n",
      "L1 loss: 0.0569879487156868\n",
      "Training batch 27 with loss 0.30555\n",
      "------------------------------------------\n",
      "L1 loss: 0.05709083378314972\n",
      "Training batch 28 with loss 0.30722\n",
      "------------------------------------------\n",
      "L1 loss: 0.06326714903116226\n",
      "Training batch 29 with loss 0.26537\n",
      "------------------------------------------\n",
      "L1 loss: 0.06143259257078171\n",
      "Training batch 30 with loss 0.26799\n",
      "------------------------------------------\n",
      "L1 loss: 0.06680372357368469\n",
      "Training batch 31 with loss 0.31173\n",
      "------------------------------------------\n",
      "L1 loss: 0.06941119581460953\n",
      "Training batch 32 with loss 0.28758\n",
      "------------------------------------------\n",
      "L1 loss: 0.06256480515003204\n",
      "Training batch 33 with loss 0.26184\n",
      "------------------------------------------\n",
      "L1 loss: 0.07381557673215866\n",
      "Training batch 34 with loss 0.30482\n",
      "------------------------------------------\n",
      "L1 loss: 0.09526883065700531\n",
      "Training batch 35 with loss 0.33979\n",
      "------------------------------------------\n",
      "L1 loss: 0.08441082388162613\n",
      "Training batch 36 with loss 0.27954\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07069388777017593\n",
      "Training batch 37 with loss 0.27211\n",
      "------------------------------------------\n",
      "L1 loss: 0.06190888211131096\n",
      "Training batch 38 with loss 0.21739\n",
      "------------------------------------------\n",
      "L1 loss: 0.0687621608376503\n",
      "Training batch 39 with loss 0.31744\n",
      "------------------------------------------\n",
      "L1 loss: 0.06274023652076721\n",
      "Training batch 40 with loss 0.34777\n",
      "------------------------------------------\n",
      "L1 loss: 0.05812475457787514\n",
      "Training batch 41 with loss 0.27681\n",
      "------------------------------------------\n",
      "L1 loss: 0.08026833087205887\n",
      "Training batch 42 with loss 0.27446\n",
      "------------------------------------------\n",
      "L1 loss: 0.06899861246347427\n",
      "Training batch 43 with loss 0.26892\n",
      "------------------------------------------\n",
      "L1 loss: 0.05664931610226631\n",
      "Training batch 44 with loss 0.26865\n",
      "------------------------------------------\n",
      "L1 loss: 0.09225230664014816\n",
      "Training batch 45 with loss 0.36387\n",
      "------------------------------------------\n",
      "L1 loss: 0.07514900714159012\n",
      "Training batch 46 with loss 0.27161\n",
      "------------------------------------------\n",
      "L1 loss: 0.11050546169281006\n",
      "Training batch 47 with loss 0.36285\n",
      "------------------------------------------\n",
      "L1 loss: 0.0337531603872776\n",
      "Training batch 48 with loss 0.18585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07941232621669769\n",
      "Training batch 49 with loss 0.33462\n",
      "------------------------------------------\n",
      "L1 loss: 0.07058445364236832\n",
      "Training batch 50 with loss 0.28466\n",
      "------------------------------------------\n",
      "L1 loss: 0.07976862043142319\n",
      "Training batch 51 with loss 0.33419\n",
      "------------------------------------------\n",
      "L1 loss: 0.08837439119815826\n",
      "Training batch 52 with loss 0.28326\n",
      "------------------------------------------\n",
      "L1 loss: 0.07693774998188019\n",
      "Training batch 53 with loss 0.27677\n",
      "------------------------------------------\n",
      "L1 loss: 0.07292690128087997\n",
      "Training batch 54 with loss 0.26096\n",
      "------------------------------------------\n",
      "L1 loss: 0.058153603225946426\n",
      "Training batch 55 with loss 0.29998\n",
      "------------------------------------------\n",
      "L1 loss: 0.05587082356214523\n",
      "Training batch 56 with loss 0.24978\n",
      "------------------------------------------\n",
      "L1 loss: 0.06553323566913605\n",
      "Training batch 57 with loss 0.26948\n",
      "------------------------------------------\n",
      "L1 loss: 0.06594595313072205\n",
      "Training batch 58 with loss 0.26647\n",
      "------------------------------------------\n",
      "L1 loss: 0.08139359205961227\n",
      "Training batch 59 with loss 0.29233\n",
      "------------------------------------------\n",
      "L1 loss: 0.0664612203836441\n",
      "Training batch 60 with loss 0.27844\n",
      "------------------------------------------\n",
      "L1 loss: 0.09227479994297028\n",
      "Training batch 61 with loss 0.32924\n",
      "------------------------------------------\n",
      "L1 loss: 0.056238651275634766\n",
      "Training batch 62 with loss 0.26120\n",
      "------------------------------------------\n",
      "L1 loss: 0.06327851116657257\n",
      "Training batch 63 with loss 0.25453\n",
      "------------------------------------------\n",
      "L1 loss: 0.07127083837985992\n",
      "Training batch 64 with loss 0.27451\n",
      "------------------------------------------\n",
      "L1 loss: 0.0636538714170456\n",
      "Training batch 65 with loss 0.23747\n",
      "------------------------------------------\n",
      "L1 loss: 0.06391142308712006\n",
      "Training batch 66 with loss 0.26336\n",
      "------------------------------------------\n",
      "L1 loss: 0.06459338963031769\n",
      "Training batch 67 with loss 0.26343\n",
      "------------------------------------------\n",
      "L1 loss: 0.072412870824337\n",
      "Training batch 68 with loss 0.30805\n",
      "------------------------------------------\n",
      "L1 loss: 0.05058345943689346\n",
      "Training batch 69 with loss 0.25299\n",
      "------------------------------------------\n",
      "L1 loss: 0.07229860126972198\n",
      "Training batch 70 with loss 0.41248\n",
      "------------------------------------------\n",
      "L1 loss: 0.07186485826969147\n",
      "Training batch 71 with loss 0.30596\n",
      "------------------------------------------\n",
      "L1 loss: 0.08747421205043793\n",
      "Training batch 72 with loss 0.30529\n",
      "------------------------------------------\n",
      "L1 loss: 0.0625116229057312\n",
      "Training batch 73 with loss 0.30614\n",
      "------------------------------------------\n",
      "L1 loss: 0.0957934707403183\n",
      "Training batch 74 with loss 0.31901\n",
      "------------------------------------------\n",
      "L1 loss: 0.07608777284622192\n",
      "Training batch 75 with loss 0.29485\n",
      "------------------------------------------\n",
      "L1 loss: 0.08700167387723923\n",
      "Training batch 76 with loss 0.29452\n",
      "------------------------------------------\n",
      "L1 loss: 0.07782341539859772\n",
      "Training batch 77 with loss 0.27530\n",
      "------------------------------------------\n",
      "L1 loss: 0.08627968281507492\n",
      "Training batch 78 with loss 0.30571\n",
      "------------------------------------------\n",
      "L1 loss: 0.1058524027466774\n",
      "Training batch 79 with loss 0.39288\n",
      "------------------------------------------\n",
      "L1 loss: 0.07670489698648453\n",
      "Training batch 80 with loss 0.28729\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683903768658638\n",
      "Training batch 81 with loss 0.27611\n",
      "------------------------------------------\n",
      "L1 loss: 0.08289378136396408\n",
      "Training batch 82 with loss 0.34231\n",
      "------------------------------------------\n",
      "L1 loss: 0.07602187246084213\n",
      "Training batch 83 with loss 0.28103\n",
      "------------------------------------------\n",
      "L1 loss: 0.07085765153169632\n",
      "Training batch 84 with loss 0.29404\n",
      "------------------------------------------\n",
      "L1 loss: 0.061852872371673584\n",
      "Training batch 85 with loss 0.25803\n",
      "------------------------------------------\n",
      "L1 loss: 0.08473468571901321\n",
      "Training batch 86 with loss 0.29215\n",
      "------------------------------------------\n",
      "L1 loss: 0.10043919831514359\n",
      "Training batch 87 with loss 0.39817\n",
      "------------------------------------------\n",
      "L1 loss: 0.07613290101289749\n",
      "Training batch 88 with loss 0.31363\n",
      "------------------------------------------\n",
      "L1 loss: 0.06807852536439896\n",
      "Training batch 89 with loss 0.24754\n",
      "------------------------------------------\n",
      "L1 loss: 0.07129189372062683\n",
      "Training batch 90 with loss 0.27806\n",
      "------------------------------------------\n",
      "L1 loss: 0.06332427263259888\n",
      "Training batch 91 with loss 0.25823\n",
      "------------------------------------------\n",
      "L1 loss: 0.08970966935157776\n",
      "Training batch 92 with loss 0.28431\n",
      "------------------------------------------\n",
      "L1 loss: 0.06154736876487732\n",
      "Training batch 93 with loss 0.24437\n",
      "------------------------------------------\n",
      "L1 loss: 0.08366802334785461\n",
      "Training batch 94 with loss 0.30500\n",
      "------------------------------------------\n",
      "L1 loss: 0.07166370749473572\n",
      "Training batch 95 with loss 0.25073\n",
      "------------------------------------------\n",
      "L1 loss: 0.06567614525556564\n",
      "Training batch 96 with loss 0.29631\n",
      "------------------------------------------\n",
      "L1 loss: 0.08531104773283005\n",
      "Training batch 97 with loss 0.35502\n",
      "------------------------------------------\n",
      "L1 loss: 0.05601465329527855\n",
      "Training batch 98 with loss 0.24573\n",
      "------------------------------------------\n",
      "L1 loss: 0.06694556772708893\n",
      "Training batch 99 with loss 0.28185\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2929304513335228\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4849\n",
      "------------------------------------------\n",
      "L1 loss: 0.058892205357551575\n",
      "Training batch 0 with loss 0.24060\n",
      "------------------------------------------\n",
      "L1 loss: 0.0853843167424202\n",
      "Training batch 1 with loss 0.29336\n",
      "------------------------------------------\n",
      "L1 loss: 0.09390269219875336\n",
      "Training batch 2 with loss 0.32221\n",
      "------------------------------------------\n",
      "L1 loss: 0.07582898437976837\n",
      "Training batch 3 with loss 0.27087\n",
      "------------------------------------------\n",
      "L1 loss: 0.08432484418153763\n",
      "Training batch 4 with loss 0.32660\n",
      "------------------------------------------\n",
      "L1 loss: 0.06352578103542328\n",
      "Training batch 5 with loss 0.33244\n",
      "------------------------------------------\n",
      "L1 loss: 0.0805690661072731\n",
      "Training batch 6 with loss 0.27805\n",
      "------------------------------------------\n",
      "L1 loss: 0.08317505568265915\n",
      "Training batch 7 with loss 0.34076\n",
      "------------------------------------------\n",
      "L1 loss: 0.08106113970279694\n",
      "Training batch 8 with loss 0.31886\n",
      "------------------------------------------\n",
      "L1 loss: 0.05966205149888992\n",
      "Training batch 9 with loss 0.25598\n",
      "------------------------------------------\n",
      "L1 loss: 0.0835656076669693\n",
      "Training batch 10 with loss 0.29575\n",
      "------------------------------------------\n",
      "L1 loss: 0.05988503247499466\n",
      "Training batch 11 with loss 0.29265\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07556667178869247\n",
      "Training batch 12 with loss 0.30591\n",
      "------------------------------------------\n",
      "L1 loss: 0.06992389261722565\n",
      "Training batch 13 with loss 0.23455\n",
      "------------------------------------------\n",
      "L1 loss: 0.052676040679216385\n",
      "Training batch 14 with loss 0.25464\n",
      "------------------------------------------\n",
      "L1 loss: 0.08409979939460754\n",
      "Training batch 15 with loss 0.28977\n",
      "------------------------------------------\n",
      "L1 loss: 0.06253589689731598\n",
      "Training batch 16 with loss 0.28693\n",
      "------------------------------------------\n",
      "L1 loss: 0.0863637924194336\n",
      "Training batch 17 with loss 0.33704\n",
      "------------------------------------------\n",
      "L1 loss: 0.0789056196808815\n",
      "Training batch 18 with loss 0.28827\n",
      "------------------------------------------\n",
      "L1 loss: 0.06347363442182541\n",
      "Training batch 19 with loss 0.24099\n",
      "------------------------------------------\n",
      "L1 loss: 0.05633820593357086\n",
      "Training batch 20 with loss 0.38779\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018007338047028\n",
      "Training batch 21 with loss 0.27269\n",
      "------------------------------------------\n",
      "L1 loss: 0.0667826309800148\n",
      "Training batch 22 with loss 0.29764\n",
      "------------------------------------------\n",
      "L1 loss: 0.08343882113695145\n",
      "Training batch 23 with loss 0.34331\n",
      "------------------------------------------\n",
      "L1 loss: 0.07013092190027237\n",
      "Training batch 24 with loss 0.30898\n",
      "------------------------------------------\n",
      "L1 loss: 0.07448094338178635\n",
      "Training batch 25 with loss 0.30764\n",
      "------------------------------------------\n",
      "L1 loss: 0.07193481177091599\n",
      "Training batch 26 with loss 0.26640\n",
      "------------------------------------------\n",
      "L1 loss: 0.054240088909864426\n",
      "Training batch 27 with loss 0.27560\n",
      "------------------------------------------\n",
      "L1 loss: 0.05948308855295181\n",
      "Training batch 28 with loss 0.31721\n",
      "------------------------------------------\n",
      "L1 loss: 0.05920514836907387\n",
      "Training batch 29 with loss 0.24157\n",
      "------------------------------------------\n",
      "L1 loss: 0.058766793459653854\n",
      "Training batch 30 with loss 0.26383\n",
      "------------------------------------------\n",
      "L1 loss: 0.06449910253286362\n",
      "Training batch 31 with loss 0.30630\n",
      "------------------------------------------\n",
      "L1 loss: 0.06964551657438278\n",
      "Training batch 32 with loss 0.28640\n",
      "------------------------------------------\n",
      "L1 loss: 0.06261591613292694\n",
      "Training batch 33 with loss 0.26592\n",
      "------------------------------------------\n",
      "L1 loss: 0.0733717754483223\n",
      "Training batch 34 with loss 0.32879\n",
      "------------------------------------------\n",
      "L1 loss: 0.09433691948652267\n",
      "Training batch 35 with loss 0.33077\n",
      "------------------------------------------\n",
      "L1 loss: 0.08164485543966293\n",
      "Training batch 36 with loss 0.28229\n",
      "------------------------------------------\n",
      "L1 loss: 0.06843970715999603\n",
      "Training batch 37 with loss 0.26855\n",
      "------------------------------------------\n",
      "L1 loss: 0.06218409538269043\n",
      "Training batch 38 with loss 0.22060\n",
      "------------------------------------------\n",
      "L1 loss: 0.0670655369758606\n",
      "Training batch 39 with loss 0.30037\n",
      "------------------------------------------\n",
      "L1 loss: 0.06092528626322746\n",
      "Training batch 40 with loss 0.31728\n",
      "------------------------------------------\n",
      "L1 loss: 0.056243639439344406\n",
      "Training batch 41 with loss 0.28074\n",
      "------------------------------------------\n",
      "L1 loss: 0.07972593605518341\n",
      "Training batch 42 with loss 0.26343\n",
      "------------------------------------------\n",
      "L1 loss: 0.06985697150230408\n",
      "Training batch 43 with loss 0.28911\n",
      "------------------------------------------\n",
      "L1 loss: 0.058406487107276917\n",
      "Training batch 44 with loss 0.26963\n",
      "------------------------------------------\n",
      "L1 loss: 0.09281624853610992\n",
      "Training batch 45 with loss 0.35283\n",
      "------------------------------------------\n",
      "L1 loss: 0.07329316437244415\n",
      "Training batch 46 with loss 0.26744\n",
      "------------------------------------------\n",
      "L1 loss: 0.1089840680360794\n",
      "Training batch 47 with loss 0.35057\n",
      "------------------------------------------\n",
      "L1 loss: 0.047432493418455124\n",
      "Training batch 48 with loss 0.25326\n",
      "------------------------------------------\n",
      "L1 loss: 0.08135097473859787\n",
      "Training batch 49 with loss 0.34704\n",
      "------------------------------------------\n",
      "L1 loss: 0.07382478564977646\n",
      "Training batch 50 with loss 0.28361\n",
      "------------------------------------------\n",
      "L1 loss: 0.08092848211526871\n",
      "Training batch 51 with loss 0.32746\n",
      "------------------------------------------\n",
      "L1 loss: 0.08914465457201004\n",
      "Training batch 52 with loss 0.30201\n",
      "------------------------------------------\n",
      "L1 loss: 0.0782165601849556\n",
      "Training batch 53 with loss 0.28505\n",
      "------------------------------------------\n",
      "L1 loss: 0.07166814059019089\n",
      "Training batch 54 with loss 0.26517\n",
      "------------------------------------------\n",
      "L1 loss: 0.062135808169841766\n",
      "Training batch 55 with loss 0.30808\n",
      "------------------------------------------\n",
      "L1 loss: 0.055988967418670654\n",
      "Training batch 56 with loss 0.25677\n",
      "------------------------------------------\n",
      "L1 loss: 0.06750530004501343\n",
      "Training batch 57 with loss 0.28025\n",
      "------------------------------------------\n",
      "L1 loss: 0.06535685807466507\n",
      "Training batch 58 with loss 0.24705\n",
      "------------------------------------------\n",
      "L1 loss: 0.08021076023578644\n",
      "Training batch 59 with loss 0.29697\n",
      "------------------------------------------\n",
      "L1 loss: 0.06263399124145508\n",
      "Training batch 60 with loss 0.26923\n",
      "------------------------------------------\n",
      "L1 loss: 0.08813992142677307\n",
      "Training batch 61 with loss 0.34022\n",
      "------------------------------------------\n",
      "L1 loss: 0.055460065603256226\n",
      "Training batch 62 with loss 0.26236\n",
      "------------------------------------------\n",
      "L1 loss: 0.06337501108646393\n",
      "Training batch 63 with loss 0.26562\n",
      "------------------------------------------\n",
      "L1 loss: 0.06865306943655014\n",
      "Training batch 64 with loss 0.27783\n",
      "------------------------------------------\n",
      "L1 loss: 0.05665893107652664\n",
      "Training batch 65 with loss 0.22550\n",
      "------------------------------------------\n",
      "L1 loss: 0.06723993271589279\n",
      "Training batch 66 with loss 0.27181\n",
      "------------------------------------------\n",
      "L1 loss: 0.06506892293691635\n",
      "Training batch 67 with loss 0.26411\n",
      "------------------------------------------\n",
      "L1 loss: 0.07577718049287796\n",
      "Training batch 68 with loss 0.30124\n",
      "------------------------------------------\n",
      "L1 loss: 0.050830427557229996\n",
      "Training batch 69 with loss 0.26197\n",
      "------------------------------------------\n",
      "L1 loss: 0.08180204778909683\n",
      "Training batch 70 with loss 0.31947\n",
      "------------------------------------------\n",
      "L1 loss: 0.07254370301961899\n",
      "Training batch 71 with loss 0.31079\n",
      "------------------------------------------\n",
      "L1 loss: 0.08387551456689835\n",
      "Training batch 72 with loss 0.28949\n",
      "------------------------------------------\n",
      "L1 loss: 0.07080627977848053\n",
      "Training batch 73 with loss 0.28448\n",
      "------------------------------------------\n",
      "L1 loss: 0.09611771255731583\n",
      "Training batch 74 with loss 0.32186\n",
      "------------------------------------------\n",
      "L1 loss: 0.07435138523578644\n",
      "Training batch 75 with loss 0.27705\n",
      "------------------------------------------\n",
      "L1 loss: 0.08724452555179596\n",
      "Training batch 76 with loss 0.28699\n",
      "------------------------------------------\n",
      "L1 loss: 0.07825694233179092\n",
      "Training batch 77 with loss 0.26699\n",
      "------------------------------------------\n",
      "L1 loss: 0.0808158814907074\n",
      "Training batch 78 with loss 0.30475\n",
      "------------------------------------------\n",
      "L1 loss: 0.10605638474225998\n",
      "Training batch 79 with loss 0.38253\n",
      "------------------------------------------\n",
      "L1 loss: 0.07457543909549713\n",
      "Training batch 80 with loss 0.27574\n",
      "------------------------------------------\n",
      "L1 loss: 0.06702622771263123\n",
      "Training batch 81 with loss 0.26965\n",
      "------------------------------------------\n",
      "L1 loss: 0.08521836251020432\n",
      "Training batch 82 with loss 0.36268\n",
      "------------------------------------------\n",
      "L1 loss: 0.07488226890563965\n",
      "Training batch 83 with loss 0.28630\n",
      "------------------------------------------\n",
      "L1 loss: 0.06659217923879623\n",
      "Training batch 84 with loss 0.27869\n",
      "------------------------------------------\n",
      "L1 loss: 0.05876181647181511\n",
      "Training batch 85 with loss 0.26719\n",
      "------------------------------------------\n",
      "L1 loss: 0.07940007746219635\n",
      "Training batch 86 with loss 0.31132\n",
      "------------------------------------------\n",
      "L1 loss: 0.10180191695690155\n",
      "Training batch 87 with loss 0.39352\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07792244851589203\n",
      "Training batch 88 with loss 0.32211\n",
      "------------------------------------------\n",
      "L1 loss: 0.06859757006168365\n",
      "Training batch 89 with loss 0.27460\n",
      "------------------------------------------\n",
      "L1 loss: 0.07520951330661774\n",
      "Training batch 90 with loss 0.28380\n",
      "------------------------------------------\n",
      "L1 loss: 0.06507059186697006\n",
      "Training batch 91 with loss 0.27479\n",
      "------------------------------------------\n",
      "L1 loss: 0.09093831479549408\n",
      "Training batch 92 with loss 0.29220\n",
      "------------------------------------------\n",
      "L1 loss: 0.06647149473428726\n",
      "Training batch 93 with loss 0.25702\n",
      "------------------------------------------\n",
      "L1 loss: 0.08464931696653366\n",
      "Training batch 94 with loss 0.29997\n",
      "------------------------------------------\n",
      "L1 loss: 0.07207800447940826\n",
      "Training batch 95 with loss 0.25467\n",
      "------------------------------------------\n",
      "L1 loss: 0.06533335149288177\n",
      "Training batch 96 with loss 0.29622\n",
      "------------------------------------------\n",
      "L1 loss: 0.09009575098752975\n",
      "Training batch 97 with loss 0.37057\n",
      "------------------------------------------\n",
      "L1 loss: 0.05822265148162842\n",
      "Training batch 98 with loss 0.23954\n",
      "------------------------------------------\n",
      "L1 loss: 0.06646370142698288\n",
      "Training batch 99 with loss 0.27252\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2918996851146221\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4850\n",
      "------------------------------------------\n",
      "L1 loss: 0.06172178313136101\n",
      "Training batch 0 with loss 0.24802\n",
      "------------------------------------------\n",
      "L1 loss: 0.08495110273361206\n",
      "Training batch 1 with loss 0.28282\n",
      "------------------------------------------\n",
      "L1 loss: 0.09302964061498642\n",
      "Training batch 2 with loss 0.31141\n",
      "------------------------------------------\n",
      "L1 loss: 0.07388328015804291\n",
      "Training batch 3 with loss 0.27030\n",
      "------------------------------------------\n",
      "L1 loss: 0.08508221060037613\n",
      "Training batch 4 with loss 0.32286\n",
      "------------------------------------------\n",
      "L1 loss: 0.06585300713777542\n",
      "Training batch 5 with loss 0.31834\n",
      "------------------------------------------\n",
      "L1 loss: 0.07915963232517242\n",
      "Training batch 6 with loss 0.27003\n",
      "------------------------------------------\n",
      "L1 loss: 0.08321992307901382\n",
      "Training batch 7 with loss 0.33509\n",
      "------------------------------------------\n",
      "L1 loss: 0.08167844265699387\n",
      "Training batch 8 with loss 0.32653\n",
      "------------------------------------------\n",
      "L1 loss: 0.061940595507621765\n",
      "Training batch 9 with loss 0.25919\n",
      "------------------------------------------\n",
      "L1 loss: 0.08084486424922943\n",
      "Training batch 10 with loss 0.29286\n",
      "------------------------------------------\n",
      "L1 loss: 0.06079495698213577\n",
      "Training batch 11 with loss 0.29779\n",
      "------------------------------------------\n",
      "L1 loss: 0.07643043249845505\n",
      "Training batch 12 with loss 0.30955\n",
      "------------------------------------------\n",
      "L1 loss: 0.07104102522134781\n",
      "Training batch 13 with loss 0.26500\n",
      "------------------------------------------\n",
      "L1 loss: 0.05395089462399483\n",
      "Training batch 14 with loss 0.25518\n",
      "------------------------------------------\n",
      "L1 loss: 0.08392442017793655\n",
      "Training batch 15 with loss 0.27615\n",
      "------------------------------------------\n",
      "L1 loss: 0.06488308310508728\n",
      "Training batch 16 with loss 0.26913\n",
      "------------------------------------------\n",
      "L1 loss: 0.08616717159748077\n",
      "Training batch 17 with loss 0.33110\n",
      "------------------------------------------\n",
      "L1 loss: 0.07966866344213486\n",
      "Training batch 18 with loss 0.31499\n",
      "------------------------------------------\n",
      "L1 loss: 0.06111226603388786\n",
      "Training batch 19 with loss 0.23212\n",
      "------------------------------------------\n",
      "L1 loss: 0.07381356507539749\n",
      "Training batch 20 with loss 0.33406\n",
      "------------------------------------------\n",
      "L1 loss: 0.072287417948246\n",
      "Training batch 21 with loss 0.28883\n",
      "------------------------------------------\n",
      "L1 loss: 0.06788278371095657\n",
      "Training batch 22 with loss 0.28961\n",
      "------------------------------------------\n",
      "L1 loss: 0.08880886435508728\n",
      "Training batch 23 with loss 0.36109\n",
      "------------------------------------------\n",
      "L1 loss: 0.06948880106210709\n",
      "Training batch 24 with loss 0.29202\n",
      "------------------------------------------\n",
      "L1 loss: 0.07716041803359985\n",
      "Training batch 25 with loss 0.30422\n",
      "------------------------------------------\n",
      "L1 loss: 0.07170432806015015\n",
      "Training batch 26 with loss 0.26560\n",
      "------------------------------------------\n",
      "L1 loss: 0.05477936938405037\n",
      "Training batch 27 with loss 0.29174\n",
      "------------------------------------------\n",
      "L1 loss: 0.05678094923496246\n",
      "Training batch 28 with loss 0.29204\n",
      "------------------------------------------\n",
      "L1 loss: 0.05488431081175804\n",
      "Training batch 29 with loss 0.23496\n",
      "------------------------------------------\n",
      "L1 loss: 0.060011886060237885\n",
      "Training batch 30 with loss 0.25651\n",
      "------------------------------------------\n",
      "L1 loss: 0.06490825116634369\n",
      "Training batch 31 with loss 0.30439\n",
      "------------------------------------------\n",
      "L1 loss: 0.06996926665306091\n",
      "Training batch 32 with loss 0.26879\n",
      "------------------------------------------\n",
      "L1 loss: 0.06039320304989815\n",
      "Training batch 33 with loss 0.25772\n",
      "------------------------------------------\n",
      "L1 loss: 0.07387925684452057\n",
      "Training batch 34 with loss 0.30438\n",
      "------------------------------------------\n",
      "L1 loss: 0.09497109055519104\n",
      "Training batch 35 with loss 0.31812\n",
      "------------------------------------------\n",
      "L1 loss: 0.08703804761171341\n",
      "Training batch 36 with loss 0.28827\n",
      "------------------------------------------\n",
      "L1 loss: 0.06644545495510101\n",
      "Training batch 37 with loss 0.26095\n",
      "------------------------------------------\n",
      "L1 loss: 0.06198212876915932\n",
      "Training batch 38 with loss 0.21085\n",
      "------------------------------------------\n",
      "L1 loss: 0.06351958215236664\n",
      "Training batch 39 with loss 0.27958\n",
      "------------------------------------------\n",
      "L1 loss: 0.0710432380437851\n",
      "Training batch 40 with loss 0.25699\n",
      "------------------------------------------\n",
      "L1 loss: 0.059497226029634476\n",
      "Training batch 41 with loss 0.29539\n",
      "------------------------------------------\n",
      "L1 loss: 0.08071297407150269\n",
      "Training batch 42 with loss 0.27984\n",
      "------------------------------------------\n",
      "L1 loss: 0.07077465206384659\n",
      "Training batch 43 with loss 0.27754\n",
      "------------------------------------------\n",
      "L1 loss: 0.05555348098278046\n",
      "Training batch 44 with loss 0.27417\n",
      "------------------------------------------\n",
      "L1 loss: 0.09262700378894806\n",
      "Training batch 45 with loss 0.34748\n",
      "------------------------------------------\n",
      "L1 loss: 0.07102242857217789\n",
      "Training batch 46 with loss 0.25417\n",
      "------------------------------------------\n",
      "L1 loss: 0.11149267107248306\n",
      "Training batch 47 with loss 0.37960\n",
      "------------------------------------------\n",
      "L1 loss: 0.043765634298324585\n",
      "Training batch 48 with loss 0.24148\n",
      "------------------------------------------\n",
      "L1 loss: 0.07852602005004883\n",
      "Training batch 49 with loss 0.35898\n",
      "------------------------------------------\n",
      "L1 loss: 0.07160244882106781\n",
      "Training batch 50 with loss 0.28691\n",
      "------------------------------------------\n",
      "L1 loss: 0.08388379961252213\n",
      "Training batch 51 with loss 0.32735\n",
      "------------------------------------------\n",
      "L1 loss: 0.08924498409032822\n",
      "Training batch 52 with loss 0.29060\n",
      "------------------------------------------\n",
      "L1 loss: 0.07938370108604431\n",
      "Training batch 53 with loss 0.28177\n",
      "------------------------------------------\n",
      "L1 loss: 0.07496137917041779\n",
      "Training batch 54 with loss 0.26592\n",
      "------------------------------------------\n",
      "L1 loss: 0.06017094850540161\n",
      "Training batch 55 with loss 0.30366\n",
      "------------------------------------------\n",
      "L1 loss: 0.05662968382239342\n",
      "Training batch 56 with loss 0.26285\n",
      "------------------------------------------\n",
      "L1 loss: 0.07069854438304901\n",
      "Training batch 57 with loss 0.28197\n",
      "------------------------------------------\n",
      "L1 loss: 0.06606276333332062\n",
      "Training batch 58 with loss 0.24742\n",
      "------------------------------------------\n",
      "L1 loss: 0.08047197759151459\n",
      "Training batch 59 with loss 0.29298\n",
      "------------------------------------------\n",
      "L1 loss: 0.06615058332681656\n",
      "Training batch 60 with loss 0.27835\n",
      "------------------------------------------\n",
      "L1 loss: 0.0964418351650238\n",
      "Training batch 61 with loss 0.36827\n",
      "------------------------------------------\n",
      "L1 loss: 0.05661020427942276\n",
      "Training batch 62 with loss 0.25562\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.05625541880726814\n",
      "Training batch 63 with loss 0.25897\n",
      "------------------------------------------\n",
      "L1 loss: 0.06981002539396286\n",
      "Training batch 64 with loss 0.27503\n",
      "------------------------------------------\n",
      "L1 loss: 0.06252872198820114\n",
      "Training batch 65 with loss 0.22433\n",
      "------------------------------------------\n",
      "L1 loss: 0.06442122906446457\n",
      "Training batch 66 with loss 0.26423\n",
      "------------------------------------------\n",
      "L1 loss: 0.06600106507539749\n",
      "Training batch 67 with loss 0.25491\n",
      "------------------------------------------\n",
      "L1 loss: 0.07272584736347198\n",
      "Training batch 68 with loss 0.30545\n",
      "------------------------------------------\n",
      "L1 loss: 0.05236296355724335\n",
      "Training batch 69 with loss 0.27722\n",
      "------------------------------------------\n",
      "L1 loss: 0.0796932727098465\n",
      "Training batch 70 with loss 0.31334\n",
      "------------------------------------------\n",
      "L1 loss: 0.071260966360569\n",
      "Training batch 71 with loss 0.30788\n",
      "------------------------------------------\n",
      "L1 loss: 0.08477573096752167\n",
      "Training batch 72 with loss 0.32545\n",
      "------------------------------------------\n",
      "L1 loss: 0.07138553261756897\n",
      "Training batch 73 with loss 0.29293\n",
      "------------------------------------------\n",
      "L1 loss: 0.08552177250385284\n",
      "Training batch 74 with loss 0.40093\n",
      "------------------------------------------\n",
      "L1 loss: 0.07189007848501205\n",
      "Training batch 75 with loss 0.32851\n",
      "------------------------------------------\n",
      "L1 loss: 0.08657646179199219\n",
      "Training batch 76 with loss 0.29056\n",
      "------------------------------------------\n",
      "L1 loss: 0.078617624938488\n",
      "Training batch 77 with loss 0.27309\n",
      "------------------------------------------\n",
      "L1 loss: 0.07939716428518295\n",
      "Training batch 78 with loss 0.32458\n",
      "------------------------------------------\n",
      "L1 loss: 0.10812371224164963\n",
      "Training batch 79 with loss 0.37775\n",
      "------------------------------------------\n",
      "L1 loss: 0.07484234869480133\n",
      "Training batch 80 with loss 0.27692\n",
      "------------------------------------------\n",
      "L1 loss: 0.08195937424898148\n",
      "Training batch 81 with loss 0.30659\n",
      "------------------------------------------\n",
      "L1 loss: 0.08298849314451218\n",
      "Training batch 82 with loss 0.35874\n",
      "------------------------------------------\n",
      "L1 loss: 0.07911037653684616\n",
      "Training batch 83 with loss 0.28719\n",
      "------------------------------------------\n",
      "L1 loss: 0.07160907238721848\n",
      "Training batch 84 with loss 0.30618\n",
      "------------------------------------------\n",
      "L1 loss: 0.06563802808523178\n",
      "Training batch 85 with loss 0.28446\n",
      "------------------------------------------\n",
      "L1 loss: 0.07043899595737457\n",
      "Training batch 86 with loss 0.26076\n",
      "------------------------------------------\n",
      "L1 loss: 0.10097453743219376\n",
      "Training batch 87 with loss 0.40744\n",
      "------------------------------------------\n",
      "L1 loss: 0.07775204628705978\n",
      "Training batch 88 with loss 0.31514\n",
      "------------------------------------------\n",
      "L1 loss: 0.06963299959897995\n",
      "Training batch 89 with loss 0.26302\n",
      "------------------------------------------\n",
      "L1 loss: 0.07481885701417923\n",
      "Training batch 90 with loss 0.27789\n",
      "------------------------------------------\n",
      "L1 loss: 0.06242572143673897\n",
      "Training batch 91 with loss 0.26479\n",
      "------------------------------------------\n",
      "L1 loss: 0.09649678319692612\n",
      "Training batch 92 with loss 0.31544\n",
      "------------------------------------------\n",
      "L1 loss: 0.06068392097949982\n",
      "Training batch 93 with loss 0.23967\n",
      "------------------------------------------\n",
      "L1 loss: 0.08596037328243256\n",
      "Training batch 94 with loss 0.30947\n",
      "------------------------------------------\n",
      "L1 loss: 0.06980009377002716\n",
      "Training batch 95 with loss 0.25652\n",
      "------------------------------------------\n",
      "L1 loss: 0.06984280794858932\n",
      "Training batch 96 with loss 0.65187\n",
      "------------------------------------------\n",
      "L1 loss: 0.08670992404222488\n",
      "Training batch 97 with loss 0.35312\n",
      "------------------------------------------\n",
      "L1 loss: 0.055490829050540924\n",
      "Training batch 98 with loss 0.23839\n",
      "------------------------------------------\n",
      "L1 loss: 0.06251457333564758\n",
      "Training batch 99 with loss 0.24341\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29513666123151777\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4851\n",
      "------------------------------------------\n",
      "L1 loss: 0.06449015438556671\n",
      "Training batch 0 with loss 0.25107\n",
      "------------------------------------------\n",
      "L1 loss: 0.07034236937761307\n",
      "Training batch 1 with loss 0.26615\n",
      "------------------------------------------\n",
      "L1 loss: 0.09573341906070709\n",
      "Training batch 2 with loss 0.34356\n",
      "------------------------------------------\n",
      "L1 loss: 0.07214783877134323\n",
      "Training batch 3 with loss 0.26396\n",
      "------------------------------------------\n",
      "L1 loss: 0.08416177332401276\n",
      "Training batch 4 with loss 0.33615\n",
      "------------------------------------------\n",
      "L1 loss: 0.06789911538362503\n",
      "Training batch 5 with loss 0.35131\n",
      "------------------------------------------\n",
      "L1 loss: 0.07711928337812424\n",
      "Training batch 6 with loss 0.26269\n",
      "------------------------------------------\n",
      "L1 loss: 0.08451984822750092\n",
      "Training batch 7 with loss 0.33144\n",
      "------------------------------------------\n",
      "L1 loss: 0.0762568786740303\n",
      "Training batch 8 with loss 0.30921\n",
      "------------------------------------------\n",
      "L1 loss: 0.05777358636260033\n",
      "Training batch 9 with loss 0.27097\n",
      "------------------------------------------\n",
      "L1 loss: 0.08270802348852158\n",
      "Training batch 10 with loss 0.30408\n",
      "------------------------------------------\n",
      "L1 loss: 0.0608825609087944\n",
      "Training batch 11 with loss 0.29408\n",
      "------------------------------------------\n",
      "L1 loss: 0.07475048303604126\n",
      "Training batch 12 with loss 0.32991\n",
      "------------------------------------------\n",
      "L1 loss: 0.06750272959470749\n",
      "Training batch 13 with loss 0.24533\n",
      "------------------------------------------\n",
      "L1 loss: 0.05222143605351448\n",
      "Training batch 14 with loss 0.23508\n",
      "------------------------------------------\n",
      "L1 loss: 0.08341925591230392\n",
      "Training batch 15 with loss 0.27428\n",
      "------------------------------------------\n",
      "L1 loss: 0.06283114105463028\n",
      "Training batch 16 with loss 0.26020\n",
      "------------------------------------------\n",
      "L1 loss: 0.0839131698012352\n",
      "Training batch 17 with loss 0.32340\n",
      "------------------------------------------\n",
      "L1 loss: 0.08001051843166351\n",
      "Training batch 18 with loss 0.31256\n",
      "------------------------------------------\n",
      "L1 loss: 0.06267102807760239\n",
      "Training batch 19 with loss 0.23839\n",
      "------------------------------------------\n",
      "L1 loss: 0.0736241489648819\n",
      "Training batch 20 with loss 0.32495\n",
      "------------------------------------------\n",
      "L1 loss: 0.06948432326316833\n",
      "Training batch 21 with loss 0.27151\n",
      "------------------------------------------\n",
      "L1 loss: 0.06909468024969101\n",
      "Training batch 22 with loss 0.28618\n",
      "------------------------------------------\n",
      "L1 loss: 0.08583178371191025\n",
      "Training batch 23 with loss 0.36573\n",
      "------------------------------------------\n",
      "L1 loss: 0.06830000877380371\n",
      "Training batch 24 with loss 0.29821\n",
      "------------------------------------------\n",
      "L1 loss: 0.07642684131860733\n",
      "Training batch 25 with loss 0.30496\n",
      "------------------------------------------\n",
      "L1 loss: 0.07854312658309937\n",
      "Training batch 26 with loss 0.28385\n",
      "------------------------------------------\n",
      "L1 loss: 0.055277492851018906\n",
      "Training batch 27 with loss 0.28737\n",
      "------------------------------------------\n",
      "L1 loss: 0.05870101973414421\n",
      "Training batch 28 with loss 0.30570\n",
      "------------------------------------------\n",
      "L1 loss: 0.05705263093113899\n",
      "Training batch 29 with loss 0.23678\n",
      "------------------------------------------\n",
      "L1 loss: 0.06073729693889618\n",
      "Training batch 30 with loss 0.26273\n",
      "------------------------------------------\n",
      "L1 loss: 0.06146243214607239\n",
      "Training batch 31 with loss 0.30183\n",
      "------------------------------------------\n",
      "L1 loss: 0.06938275694847107\n",
      "Training batch 32 with loss 0.27303\n",
      "------------------------------------------\n",
      "L1 loss: 0.06272360682487488\n",
      "Training batch 33 with loss 0.26200\n",
      "------------------------------------------\n",
      "L1 loss: 0.07365741580724716\n",
      "Training batch 34 with loss 0.32251\n",
      "------------------------------------------\n",
      "L1 loss: 0.09511984139680862\n",
      "Training batch 35 with loss 0.34203\n",
      "------------------------------------------\n",
      "L1 loss: 0.07922471314668655\n",
      "Training batch 36 with loss 0.28458\n",
      "------------------------------------------\n",
      "L1 loss: 0.06548209488391876\n",
      "Training batch 37 with loss 0.25546\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06280302256345749\n",
      "Training batch 38 with loss 0.21714\n",
      "------------------------------------------\n",
      "L1 loss: 0.06723406165838242\n",
      "Training batch 39 with loss 0.29779\n",
      "------------------------------------------\n",
      "L1 loss: 0.06967843323945999\n",
      "Training batch 40 with loss 0.25769\n",
      "------------------------------------------\n",
      "L1 loss: 0.06088193133473396\n",
      "Training batch 41 with loss 0.28722\n",
      "------------------------------------------\n",
      "L1 loss: 0.07699144631624222\n",
      "Training batch 42 with loss 0.28146\n",
      "------------------------------------------\n",
      "L1 loss: 0.07279375195503235\n",
      "Training batch 43 with loss 0.27422\n",
      "------------------------------------------\n",
      "L1 loss: 0.053802818059921265\n",
      "Training batch 44 with loss 0.25447\n",
      "------------------------------------------\n",
      "L1 loss: 0.09032115340232849\n",
      "Training batch 45 with loss 0.36843\n",
      "------------------------------------------\n",
      "L1 loss: 0.07551936060190201\n",
      "Training batch 46 with loss 0.26686\n",
      "------------------------------------------\n",
      "L1 loss: 0.10959752649068832\n",
      "Training batch 47 with loss 0.36071\n",
      "------------------------------------------\n",
      "L1 loss: 0.045029621571302414\n",
      "Training batch 48 with loss 0.24393\n",
      "------------------------------------------\n",
      "L1 loss: 0.07824248820543289\n",
      "Training batch 49 with loss 0.34862\n",
      "------------------------------------------\n",
      "L1 loss: 0.07317660748958588\n",
      "Training batch 50 with loss 0.29983\n",
      "------------------------------------------\n",
      "L1 loss: 0.0814945325255394\n",
      "Training batch 51 with loss 0.32603\n",
      "------------------------------------------\n",
      "L1 loss: 0.0911044254899025\n",
      "Training batch 52 with loss 0.30130\n",
      "------------------------------------------\n",
      "L1 loss: 0.07585826516151428\n",
      "Training batch 53 with loss 0.28673\n",
      "------------------------------------------\n",
      "L1 loss: 0.07721906155347824\n",
      "Training batch 54 with loss 0.27006\n",
      "------------------------------------------\n",
      "L1 loss: 0.0632036030292511\n",
      "Training batch 55 with loss 0.32199\n",
      "------------------------------------------\n",
      "L1 loss: 0.0582418218255043\n",
      "Training batch 56 with loss 0.27076\n",
      "------------------------------------------\n",
      "L1 loss: 0.07174308598041534\n",
      "Training batch 57 with loss 0.28131\n",
      "------------------------------------------\n",
      "L1 loss: 0.06404700130224228\n",
      "Training batch 58 with loss 0.25182\n",
      "------------------------------------------\n",
      "L1 loss: 0.05969444662332535\n",
      "Training batch 59 with loss 0.35031\n",
      "------------------------------------------\n",
      "L1 loss: 0.06315646320581436\n",
      "Training batch 60 with loss 0.26784\n",
      "------------------------------------------\n",
      "L1 loss: 0.09962871670722961\n",
      "Training batch 61 with loss 0.34904\n",
      "------------------------------------------\n",
      "L1 loss: 0.05619340389966965\n",
      "Training batch 62 with loss 0.25982\n",
      "------------------------------------------\n",
      "L1 loss: 0.06153317168354988\n",
      "Training batch 63 with loss 0.25134\n",
      "------------------------------------------\n",
      "L1 loss: 0.0708378478884697\n",
      "Training batch 64 with loss 0.27744\n",
      "------------------------------------------\n",
      "L1 loss: 0.061281729489564896\n",
      "Training batch 65 with loss 0.21943\n",
      "------------------------------------------\n",
      "L1 loss: 0.06622520834207535\n",
      "Training batch 66 with loss 0.26234\n",
      "------------------------------------------\n",
      "L1 loss: 0.06512370705604553\n",
      "Training batch 67 with loss 0.26677\n",
      "------------------------------------------\n",
      "L1 loss: 0.07294405996799469\n",
      "Training batch 68 with loss 0.29903\n",
      "------------------------------------------\n",
      "L1 loss: 0.05233217030763626\n",
      "Training batch 69 with loss 0.26373\n",
      "------------------------------------------\n",
      "L1 loss: 0.07946768403053284\n",
      "Training batch 70 with loss 0.31884\n",
      "------------------------------------------\n",
      "L1 loss: 0.07318972051143646\n",
      "Training batch 71 with loss 0.31787\n",
      "------------------------------------------\n",
      "L1 loss: 0.08025135099887848\n",
      "Training batch 72 with loss 0.27514\n",
      "------------------------------------------\n",
      "L1 loss: 0.07155581563711166\n",
      "Training batch 73 with loss 0.31158\n",
      "------------------------------------------\n",
      "L1 loss: 0.0987723171710968\n",
      "Training batch 74 with loss 0.30420\n",
      "------------------------------------------\n",
      "L1 loss: 0.0736391618847847\n",
      "Training batch 75 with loss 0.28214\n",
      "------------------------------------------\n",
      "L1 loss: 0.08693914115428925\n",
      "Training batch 76 with loss 0.28997\n",
      "------------------------------------------\n",
      "L1 loss: 0.08006509393453598\n",
      "Training batch 77 with loss 0.26827\n",
      "------------------------------------------\n",
      "L1 loss: 0.08262496441602707\n",
      "Training batch 78 with loss 0.31704\n",
      "------------------------------------------\n",
      "L1 loss: 0.10460657626390457\n",
      "Training batch 79 with loss 0.35253\n",
      "------------------------------------------\n",
      "L1 loss: 0.07520082592964172\n",
      "Training batch 80 with loss 0.28009\n",
      "------------------------------------------\n",
      "L1 loss: 0.07734932750463486\n",
      "Training batch 81 with loss 0.29742\n",
      "------------------------------------------\n",
      "L1 loss: 0.08455681055784225\n",
      "Training batch 82 with loss 0.35421\n",
      "------------------------------------------\n",
      "L1 loss: 0.0822606086730957\n",
      "Training batch 83 with loss 0.28927\n",
      "------------------------------------------\n",
      "L1 loss: 0.06880291551351547\n",
      "Training batch 84 with loss 0.30913\n",
      "------------------------------------------\n",
      "L1 loss: 0.05782029405236244\n",
      "Training batch 85 with loss 0.26446\n",
      "------------------------------------------\n",
      "L1 loss: 0.08085712045431137\n",
      "Training batch 86 with loss 0.30364\n",
      "------------------------------------------\n",
      "L1 loss: 0.09889478236436844\n",
      "Training batch 87 with loss 0.43349\n",
      "------------------------------------------\n",
      "L1 loss: 0.07734762877225876\n",
      "Training batch 88 with loss 0.30507\n",
      "------------------------------------------\n",
      "L1 loss: 0.07030947506427765\n",
      "Training batch 89 with loss 0.26910\n",
      "------------------------------------------\n",
      "L1 loss: 0.0737357959151268\n",
      "Training batch 90 with loss 0.27585\n",
      "------------------------------------------\n",
      "L1 loss: 0.06016244739294052\n",
      "Training batch 91 with loss 0.25124\n",
      "------------------------------------------\n",
      "L1 loss: 0.09063072502613068\n",
      "Training batch 92 with loss 0.29166\n",
      "------------------------------------------\n",
      "L1 loss: 0.07349827140569687\n",
      "Training batch 93 with loss 0.27782\n",
      "------------------------------------------\n",
      "L1 loss: 0.08370179682970047\n",
      "Training batch 94 with loss 0.30294\n",
      "------------------------------------------\n",
      "L1 loss: 0.07173532247543335\n",
      "Training batch 95 with loss 0.25619\n",
      "------------------------------------------\n",
      "L1 loss: 0.06620707362890244\n",
      "Training batch 96 with loss 0.29400\n",
      "------------------------------------------\n",
      "L1 loss: 0.08929792791604996\n",
      "Training batch 97 with loss 0.36922\n",
      "------------------------------------------\n",
      "L1 loss: 0.057882316410541534\n",
      "Training batch 98 with loss 0.25218\n",
      "------------------------------------------\n",
      "L1 loss: 0.05741341784596443\n",
      "Training batch 99 with loss 0.23904\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2916231442987919\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4852\n",
      "------------------------------------------\n",
      "L1 loss: 0.06183737888932228\n",
      "Training batch 0 with loss 0.25237\n",
      "------------------------------------------\n",
      "L1 loss: 0.08578118681907654\n",
      "Training batch 1 with loss 0.28092\n",
      "------------------------------------------\n",
      "L1 loss: 0.0906645730137825\n",
      "Training batch 2 with loss 0.31296\n",
      "------------------------------------------\n",
      "L1 loss: 0.07491722702980042\n",
      "Training batch 3 with loss 0.26405\n",
      "------------------------------------------\n",
      "L1 loss: 0.08302398771047592\n",
      "Training batch 4 with loss 0.31710\n",
      "------------------------------------------\n",
      "L1 loss: 0.06608938425779343\n",
      "Training batch 5 with loss 0.33108\n",
      "------------------------------------------\n",
      "L1 loss: 0.07714374363422394\n",
      "Training batch 6 with loss 0.26246\n",
      "------------------------------------------\n",
      "L1 loss: 0.08359723538160324\n",
      "Training batch 7 with loss 0.32643\n",
      "------------------------------------------\n",
      "L1 loss: 0.08396834135055542\n",
      "Training batch 8 with loss 0.35310\n",
      "------------------------------------------\n",
      "L1 loss: 0.05802655220031738\n",
      "Training batch 9 with loss 0.25557\n",
      "------------------------------------------\n",
      "L1 loss: 0.08274416625499725\n",
      "Training batch 10 with loss 0.28974\n",
      "------------------------------------------\n",
      "L1 loss: 0.06302083283662796\n",
      "Training batch 11 with loss 0.30839\n",
      "------------------------------------------\n",
      "L1 loss: 0.07735685259103775\n",
      "Training batch 12 with loss 0.31676\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06971808522939682\n",
      "Training batch 13 with loss 0.24701\n",
      "------------------------------------------\n",
      "L1 loss: 0.05423395708203316\n",
      "Training batch 14 with loss 0.27764\n",
      "------------------------------------------\n",
      "L1 loss: 0.08319572359323502\n",
      "Training batch 15 with loss 0.27524\n",
      "------------------------------------------\n",
      "L1 loss: 0.06425044685602188\n",
      "Training batch 16 with loss 0.26344\n",
      "------------------------------------------\n",
      "L1 loss: 0.08932715654373169\n",
      "Training batch 17 with loss 0.34701\n",
      "------------------------------------------\n",
      "L1 loss: 0.08078078925609589\n",
      "Training batch 18 with loss 0.29614\n",
      "------------------------------------------\n",
      "L1 loss: 0.06029810756444931\n",
      "Training batch 19 with loss 0.23027\n",
      "------------------------------------------\n",
      "L1 loss: 0.07384413480758667\n",
      "Training batch 20 with loss 0.31094\n",
      "------------------------------------------\n",
      "L1 loss: 0.07195460796356201\n",
      "Training batch 21 with loss 0.27930\n",
      "------------------------------------------\n",
      "L1 loss: 0.06753797829151154\n",
      "Training batch 22 with loss 0.29785\n",
      "------------------------------------------\n",
      "L1 loss: 0.08983755856752396\n",
      "Training batch 23 with loss 0.35437\n",
      "------------------------------------------\n",
      "L1 loss: 0.06723695248365402\n",
      "Training batch 24 with loss 0.28803\n",
      "------------------------------------------\n",
      "L1 loss: 0.07638763636350632\n",
      "Training batch 25 with loss 0.30870\n",
      "------------------------------------------\n",
      "L1 loss: 0.07120244204998016\n",
      "Training batch 26 with loss 0.27001\n",
      "------------------------------------------\n",
      "L1 loss: 0.05657505989074707\n",
      "Training batch 27 with loss 0.30158\n",
      "------------------------------------------\n",
      "L1 loss: 0.056991495192050934\n",
      "Training batch 28 with loss 0.30519\n",
      "------------------------------------------\n",
      "L1 loss: 0.05976114794611931\n",
      "Training batch 29 with loss 0.25068\n",
      "------------------------------------------\n",
      "L1 loss: 0.06132969632744789\n",
      "Training batch 30 with loss 0.26384\n",
      "------------------------------------------\n",
      "L1 loss: 0.06361325085163116\n",
      "Training batch 31 with loss 0.30162\n",
      "------------------------------------------\n",
      "L1 loss: 0.06892157346010208\n",
      "Training batch 32 with loss 0.28574\n",
      "------------------------------------------\n",
      "L1 loss: 0.061871498823165894\n",
      "Training batch 33 with loss 0.26486\n",
      "------------------------------------------\n",
      "L1 loss: 0.07574381679296494\n",
      "Training batch 34 with loss 0.28677\n",
      "------------------------------------------\n",
      "L1 loss: 0.09435243904590607\n",
      "Training batch 35 with loss 0.33657\n",
      "------------------------------------------\n",
      "L1 loss: 0.08438485115766525\n",
      "Training batch 36 with loss 0.27318\n",
      "------------------------------------------\n",
      "L1 loss: 0.0696231871843338\n",
      "Training batch 37 with loss 0.27876\n",
      "------------------------------------------\n",
      "L1 loss: 0.06291381269693375\n",
      "Training batch 38 with loss 0.22897\n",
      "------------------------------------------\n",
      "L1 loss: 0.06748490035533905\n",
      "Training batch 39 with loss 0.29996\n",
      "------------------------------------------\n",
      "L1 loss: 0.06629408895969391\n",
      "Training batch 40 with loss 0.25549\n",
      "------------------------------------------\n",
      "L1 loss: 0.061645932495594025\n",
      "Training batch 41 with loss 0.28818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07664632052183151\n",
      "Training batch 42 with loss 0.28106\n",
      "------------------------------------------\n",
      "L1 loss: 0.07104776799678802\n",
      "Training batch 43 with loss 0.26593\n",
      "------------------------------------------\n",
      "L1 loss: 0.05795205011963844\n",
      "Training batch 44 with loss 0.27102\n",
      "------------------------------------------\n",
      "L1 loss: 0.09273146837949753\n",
      "Training batch 45 with loss 0.36639\n",
      "------------------------------------------\n",
      "L1 loss: 0.06931860744953156\n",
      "Training batch 46 with loss 0.26683\n",
      "------------------------------------------\n",
      "L1 loss: 0.09376926720142365\n",
      "Training batch 47 with loss 0.41275\n",
      "------------------------------------------\n",
      "L1 loss: 0.05020969361066818\n",
      "Training batch 48 with loss 0.27032\n",
      "------------------------------------------\n",
      "L1 loss: 0.08189481496810913\n",
      "Training batch 49 with loss 0.35663\n",
      "------------------------------------------\n",
      "L1 loss: 0.07201841473579407\n",
      "Training batch 50 with loss 0.28964\n",
      "------------------------------------------\n",
      "L1 loss: 0.07869206368923187\n",
      "Training batch 51 with loss 0.34025\n",
      "------------------------------------------\n",
      "L1 loss: 0.09013749659061432\n",
      "Training batch 52 with loss 0.28653\n",
      "------------------------------------------\n",
      "L1 loss: 0.07865787297487259\n",
      "Training batch 53 with loss 0.28502\n",
      "------------------------------------------\n",
      "L1 loss: 0.07802751660346985\n",
      "Training batch 54 with loss 0.27499\n",
      "------------------------------------------\n",
      "L1 loss: 0.06261230260133743\n",
      "Training batch 55 with loss 0.30915\n",
      "------------------------------------------\n",
      "L1 loss: 0.057794831693172455\n",
      "Training batch 56 with loss 0.24627\n",
      "------------------------------------------\n",
      "L1 loss: 0.06812755763530731\n",
      "Training batch 57 with loss 0.28333\n",
      "------------------------------------------\n",
      "L1 loss: 0.06767598539590836\n",
      "Training batch 58 with loss 0.26553\n",
      "------------------------------------------\n",
      "L1 loss: 0.07889293879270554\n",
      "Training batch 59 with loss 0.28971\n",
      "------------------------------------------\n",
      "L1 loss: 0.06532488763332367\n",
      "Training batch 60 with loss 0.27718\n",
      "------------------------------------------\n",
      "L1 loss: 0.09910363703966141\n",
      "Training batch 61 with loss 0.37140\n",
      "------------------------------------------\n",
      "L1 loss: 0.05723366141319275\n",
      "Training batch 62 with loss 0.24554\n",
      "------------------------------------------\n",
      "L1 loss: 0.05931059271097183\n",
      "Training batch 63 with loss 0.26525\n",
      "------------------------------------------\n",
      "L1 loss: 0.06989534199237823\n",
      "Training batch 64 with loss 0.28221\n",
      "------------------------------------------\n",
      "L1 loss: 0.06311480700969696\n",
      "Training batch 65 with loss 0.24081\n",
      "------------------------------------------\n",
      "L1 loss: 0.06415878981351852\n",
      "Training batch 66 with loss 0.28215\n",
      "------------------------------------------\n",
      "L1 loss: 0.0515151284635067\n",
      "Training batch 67 with loss 0.29781\n",
      "------------------------------------------\n",
      "L1 loss: 0.07403045892715454\n",
      "Training batch 68 with loss 0.30199\n",
      "------------------------------------------\n",
      "L1 loss: 0.05077588930726051\n",
      "Training batch 69 with loss 0.25124\n",
      "------------------------------------------\n",
      "L1 loss: 0.07665891200304031\n",
      "Training batch 70 with loss 0.30749\n",
      "------------------------------------------\n",
      "L1 loss: 0.07039310038089752\n",
      "Training batch 71 with loss 0.31326\n",
      "------------------------------------------\n",
      "L1 loss: 0.08218742161989212\n",
      "Training batch 72 with loss 0.30137\n",
      "------------------------------------------\n",
      "L1 loss: 0.07088905572891235\n",
      "Training batch 73 with loss 0.29670\n",
      "------------------------------------------\n",
      "L1 loss: 0.0940423309803009\n",
      "Training batch 74 with loss 0.30606\n",
      "------------------------------------------\n",
      "L1 loss: 0.07440990954637527\n",
      "Training batch 75 with loss 0.27735\n",
      "------------------------------------------\n",
      "L1 loss: 0.08659568428993225\n",
      "Training batch 76 with loss 0.28519\n",
      "------------------------------------------\n",
      "L1 loss: 0.07822635769844055\n",
      "Training batch 77 with loss 0.26127\n",
      "------------------------------------------\n",
      "L1 loss: 0.08297791332006454\n",
      "Training batch 78 with loss 0.30580\n",
      "------------------------------------------\n",
      "L1 loss: 0.09406358003616333\n",
      "Training batch 79 with loss 0.34710\n",
      "------------------------------------------\n",
      "L1 loss: 0.07583999633789062\n",
      "Training batch 80 with loss 0.26498\n",
      "------------------------------------------\n",
      "L1 loss: 0.06922810524702072\n",
      "Training batch 81 with loss 0.27643\n",
      "------------------------------------------\n",
      "L1 loss: 0.0873924270272255\n",
      "Training batch 82 with loss 0.35667\n",
      "------------------------------------------\n",
      "L1 loss: 0.0776694193482399\n",
      "Training batch 83 with loss 0.28791\n",
      "------------------------------------------\n",
      "L1 loss: 0.07157275825738907\n",
      "Training batch 84 with loss 0.29377\n",
      "------------------------------------------\n",
      "L1 loss: 0.06031308323144913\n",
      "Training batch 85 with loss 0.26936\n",
      "------------------------------------------\n",
      "L1 loss: 0.09566554427146912\n",
      "Training batch 86 with loss 0.45270\n",
      "------------------------------------------\n",
      "L1 loss: 0.10154969990253448\n",
      "Training batch 87 with loss 0.39009\n",
      "------------------------------------------\n",
      "L1 loss: 0.07431849837303162\n",
      "Training batch 88 with loss 0.30419\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06976667791604996\n",
      "Training batch 89 with loss 0.26999\n",
      "------------------------------------------\n",
      "L1 loss: 0.07130063325166702\n",
      "Training batch 90 with loss 0.28085\n",
      "------------------------------------------\n",
      "L1 loss: 0.06579496711492538\n",
      "Training batch 91 with loss 0.26902\n",
      "------------------------------------------\n",
      "L1 loss: 0.0873100683093071\n",
      "Training batch 92 with loss 0.27386\n",
      "------------------------------------------\n",
      "L1 loss: 0.06772422790527344\n",
      "Training batch 93 with loss 0.24370\n",
      "------------------------------------------\n",
      "L1 loss: 0.08560960739850998\n",
      "Training batch 94 with loss 0.29542\n",
      "------------------------------------------\n",
      "L1 loss: 0.07048909366130829\n",
      "Training batch 95 with loss 0.25957\n",
      "------------------------------------------\n",
      "L1 loss: 0.059498004615306854\n",
      "Training batch 96 with loss 0.28408\n",
      "------------------------------------------\n",
      "L1 loss: 0.08694716542959213\n",
      "Training batch 97 with loss 0.36267\n",
      "------------------------------------------\n",
      "L1 loss: 0.055732812732458115\n",
      "Training batch 98 with loss 0.23862\n",
      "------------------------------------------\n",
      "L1 loss: 0.06211487948894501\n",
      "Training batch 99 with loss 0.25775\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29248411357402804\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4853\n",
      "------------------------------------------\n",
      "L1 loss: 0.061868395656347275\n",
      "Training batch 0 with loss 0.25130\n",
      "------------------------------------------\n",
      "L1 loss: 0.08472020924091339\n",
      "Training batch 1 with loss 0.29807\n",
      "------------------------------------------\n",
      "L1 loss: 0.09278570115566254\n",
      "Training batch 2 with loss 0.32616\n",
      "------------------------------------------\n",
      "L1 loss: 0.07065223157405853\n",
      "Training batch 3 with loss 0.26773\n",
      "------------------------------------------\n",
      "L1 loss: 0.08436756581068039\n",
      "Training batch 4 with loss 0.32181\n",
      "------------------------------------------\n",
      "L1 loss: 0.05285005271434784\n",
      "Training batch 5 with loss 0.43392\n",
      "------------------------------------------\n",
      "L1 loss: 0.07926278561353683\n",
      "Training batch 6 with loss 0.25784\n",
      "------------------------------------------\n",
      "L1 loss: 0.08358605206012726\n",
      "Training batch 7 with loss 0.33734\n",
      "------------------------------------------\n",
      "L1 loss: 0.08018901944160461\n",
      "Training batch 8 with loss 0.32973\n",
      "------------------------------------------\n",
      "L1 loss: 0.05848407745361328\n",
      "Training batch 9 with loss 0.25649\n",
      "------------------------------------------\n",
      "L1 loss: 0.06229583919048309\n",
      "Training batch 10 with loss 0.30685\n",
      "------------------------------------------\n",
      "L1 loss: 0.060475174337625504\n",
      "Training batch 11 with loss 0.30143\n",
      "------------------------------------------\n",
      "L1 loss: 0.07558801770210266\n",
      "Training batch 12 with loss 0.30178\n",
      "------------------------------------------\n",
      "L1 loss: 0.06842219084501266\n",
      "Training batch 13 with loss 0.23738\n",
      "------------------------------------------\n",
      "L1 loss: 0.05255032330751419\n",
      "Training batch 14 with loss 0.23916\n",
      "------------------------------------------\n",
      "L1 loss: 0.08374034613370895\n",
      "Training batch 15 with loss 0.26424\n",
      "------------------------------------------\n",
      "L1 loss: 0.06386493891477585\n",
      "Training batch 16 with loss 0.25944\n",
      "------------------------------------------\n",
      "L1 loss: 0.08870406448841095\n",
      "Training batch 17 with loss 0.33617\n",
      "------------------------------------------\n",
      "L1 loss: 0.08025537431240082\n",
      "Training batch 18 with loss 0.30589\n",
      "------------------------------------------\n",
      "L1 loss: 0.061268407851457596\n",
      "Training batch 19 with loss 0.24227\n",
      "------------------------------------------\n",
      "L1 loss: 0.07288148999214172\n",
      "Training batch 20 with loss 0.29901\n",
      "------------------------------------------\n",
      "L1 loss: 0.06839706003665924\n",
      "Training batch 21 with loss 0.27702\n",
      "------------------------------------------\n",
      "L1 loss: 0.06952239573001862\n",
      "Training batch 22 with loss 0.28273\n",
      "------------------------------------------\n",
      "L1 loss: 0.09283187985420227\n",
      "Training batch 23 with loss 0.37543\n",
      "------------------------------------------\n",
      "L1 loss: 0.06752023100852966\n",
      "Training batch 24 with loss 0.29665\n",
      "------------------------------------------\n",
      "L1 loss: 0.07555125653743744\n",
      "Training batch 25 with loss 0.28917\n",
      "------------------------------------------\n",
      "L1 loss: 0.0763193890452385\n",
      "Training batch 26 with loss 0.29426\n",
      "------------------------------------------\n",
      "L1 loss: 0.05530056357383728\n",
      "Training batch 27 with loss 0.28145\n",
      "------------------------------------------\n",
      "L1 loss: 0.056539103388786316\n",
      "Training batch 28 with loss 0.29979\n",
      "------------------------------------------\n",
      "L1 loss: 0.05707775428891182\n",
      "Training batch 29 with loss 0.24942\n",
      "------------------------------------------\n",
      "L1 loss: 0.06115717440843582\n",
      "Training batch 30 with loss 0.27444\n",
      "------------------------------------------\n",
      "L1 loss: 0.06334271281957626\n",
      "Training batch 31 with loss 0.28243\n",
      "------------------------------------------\n",
      "L1 loss: 0.07013913989067078\n",
      "Training batch 32 with loss 0.27718\n",
      "------------------------------------------\n",
      "L1 loss: 0.0625516027212143\n",
      "Training batch 33 with loss 0.25340\n",
      "------------------------------------------\n",
      "L1 loss: 0.06377451121807098\n",
      "Training batch 34 with loss 0.59950\n",
      "------------------------------------------\n",
      "L1 loss: 0.09358807653188705\n",
      "Training batch 35 with loss 0.31820\n",
      "------------------------------------------\n",
      "L1 loss: 0.08580692857503891\n",
      "Training batch 36 with loss 0.28351\n",
      "------------------------------------------\n",
      "L1 loss: 0.06920798122882843\n",
      "Training batch 37 with loss 0.26363\n",
      "------------------------------------------\n",
      "L1 loss: 0.061153750866651535\n",
      "Training batch 38 with loss 0.22545\n",
      "------------------------------------------\n",
      "L1 loss: 0.06853721290826797\n",
      "Training batch 39 with loss 0.30804\n",
      "------------------------------------------\n",
      "L1 loss: 0.07043799012899399\n",
      "Training batch 40 with loss 0.25451\n",
      "------------------------------------------\n",
      "L1 loss: 0.06039809435606003\n",
      "Training batch 41 with loss 0.29669\n",
      "------------------------------------------\n",
      "L1 loss: 0.07628978788852692\n",
      "Training batch 42 with loss 0.28677\n",
      "------------------------------------------\n",
      "L1 loss: 0.06762775778770447\n",
      "Training batch 43 with loss 0.26797\n",
      "------------------------------------------\n",
      "L1 loss: 0.05508888140320778\n",
      "Training batch 44 with loss 0.25823\n",
      "------------------------------------------\n",
      "L1 loss: 0.08505672216415405\n",
      "Training batch 45 with loss 0.52680\n",
      "------------------------------------------\n",
      "L1 loss: 0.07152014970779419\n",
      "Training batch 46 with loss 0.26932\n",
      "------------------------------------------\n",
      "L1 loss: 0.10867136716842651\n",
      "Training batch 47 with loss 0.34738\n",
      "------------------------------------------\n",
      "L1 loss: 0.049382809549570084\n",
      "Training batch 48 with loss 0.26392\n",
      "------------------------------------------\n",
      "L1 loss: 0.0794735923409462\n",
      "Training batch 49 with loss 0.35103\n",
      "------------------------------------------\n",
      "L1 loss: 0.07295654714107513\n",
      "Training batch 50 with loss 0.27590\n",
      "------------------------------------------\n",
      "L1 loss: 0.08317747712135315\n",
      "Training batch 51 with loss 0.34742\n",
      "------------------------------------------\n",
      "L1 loss: 0.09136701375246048\n",
      "Training batch 52 with loss 0.29561\n",
      "------------------------------------------\n",
      "L1 loss: 0.08041523396968842\n",
      "Training batch 53 with loss 0.29887\n",
      "------------------------------------------\n",
      "L1 loss: 0.07654517143964767\n",
      "Training batch 54 with loss 0.28051\n",
      "------------------------------------------\n",
      "L1 loss: 0.060395386070013046\n",
      "Training batch 55 with loss 0.31807\n",
      "------------------------------------------\n",
      "L1 loss: 0.05568498373031616\n",
      "Training batch 56 with loss 0.24733\n",
      "------------------------------------------\n",
      "L1 loss: 0.06672008335590363\n",
      "Training batch 57 with loss 0.26928\n",
      "------------------------------------------\n",
      "L1 loss: 0.06294086575508118\n",
      "Training batch 58 with loss 0.27646\n",
      "------------------------------------------\n",
      "L1 loss: 0.07881560921669006\n",
      "Training batch 59 with loss 0.30338\n",
      "------------------------------------------\n",
      "L1 loss: 0.06571264564990997\n",
      "Training batch 60 with loss 0.27017\n",
      "------------------------------------------\n",
      "L1 loss: 0.09211713075637817\n",
      "Training batch 61 with loss 0.32515\n",
      "------------------------------------------\n",
      "L1 loss: 0.05646951124072075\n",
      "Training batch 62 with loss 0.25425\n",
      "------------------------------------------\n",
      "L1 loss: 0.05964471399784088\n",
      "Training batch 63 with loss 0.24408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.06796836107969284\n",
      "Training batch 64 with loss 0.27796\n",
      "------------------------------------------\n",
      "L1 loss: 0.06269518285989761\n",
      "Training batch 65 with loss 0.23166\n",
      "------------------------------------------\n",
      "L1 loss: 0.06733708083629608\n",
      "Training batch 66 with loss 0.28420\n",
      "------------------------------------------\n",
      "L1 loss: 0.06618347764015198\n",
      "Training batch 67 with loss 0.25962\n",
      "------------------------------------------\n",
      "L1 loss: 0.07142775505781174\n",
      "Training batch 68 with loss 0.28920\n",
      "------------------------------------------\n",
      "L1 loss: 0.04921683669090271\n",
      "Training batch 69 with loss 0.27007\n",
      "------------------------------------------\n",
      "L1 loss: 0.08251804858446121\n",
      "Training batch 70 with loss 0.30514\n",
      "------------------------------------------\n",
      "L1 loss: 0.07111039757728577\n",
      "Training batch 71 with loss 0.29298\n",
      "------------------------------------------\n",
      "L1 loss: 0.08676975965499878\n",
      "Training batch 72 with loss 0.31701\n",
      "------------------------------------------\n",
      "L1 loss: 0.0724736750125885\n",
      "Training batch 73 with loss 0.30964\n",
      "------------------------------------------\n",
      "L1 loss: 0.0952012687921524\n",
      "Training batch 74 with loss 0.29967\n",
      "------------------------------------------\n",
      "L1 loss: 0.07609635591506958\n",
      "Training batch 75 with loss 0.27424\n",
      "------------------------------------------\n",
      "L1 loss: 0.08628647774457932\n",
      "Training batch 76 with loss 0.30574\n",
      "------------------------------------------\n",
      "L1 loss: 0.07703288644552231\n",
      "Training batch 77 with loss 0.25534\n",
      "------------------------------------------\n",
      "L1 loss: 0.08112891763448715\n",
      "Training batch 78 with loss 0.32159\n",
      "------------------------------------------\n",
      "L1 loss: 0.10473791509866714\n",
      "Training batch 79 with loss 0.36600\n",
      "------------------------------------------\n",
      "L1 loss: 0.07629071176052094\n",
      "Training batch 80 with loss 0.28358\n",
      "------------------------------------------\n",
      "L1 loss: 0.07955286651849747\n",
      "Training batch 81 with loss 0.30864\n",
      "------------------------------------------\n",
      "L1 loss: 0.08310261368751526\n",
      "Training batch 82 with loss 0.36671\n",
      "------------------------------------------\n",
      "L1 loss: 0.08028107136487961\n",
      "Training batch 83 with loss 0.28106\n",
      "------------------------------------------\n",
      "L1 loss: 0.0707288384437561\n",
      "Training batch 84 with loss 0.28401\n",
      "------------------------------------------\n",
      "L1 loss: 0.06131279096007347\n",
      "Training batch 85 with loss 0.27333\n",
      "------------------------------------------\n",
      "L1 loss: 0.08347392082214355\n",
      "Training batch 86 with loss 0.29416\n",
      "------------------------------------------\n",
      "L1 loss: 0.10149994492530823\n",
      "Training batch 87 with loss 0.37305\n",
      "------------------------------------------\n",
      "L1 loss: 0.07803259789943695\n",
      "Training batch 88 with loss 0.30309\n",
      "------------------------------------------\n",
      "L1 loss: 0.06728328764438629\n",
      "Training batch 89 with loss 0.26863\n",
      "------------------------------------------\n",
      "L1 loss: 0.07454867660999298\n",
      "Training batch 90 with loss 0.26463\n",
      "------------------------------------------\n",
      "L1 loss: 0.06411820650100708\n",
      "Training batch 91 with loss 0.26924\n",
      "------------------------------------------\n",
      "L1 loss: 0.09501737356185913\n",
      "Training batch 92 with loss 0.30934\n",
      "------------------------------------------\n",
      "L1 loss: 0.055985838174819946\n",
      "Training batch 93 with loss 0.23555\n",
      "------------------------------------------\n",
      "L1 loss: 0.08198612928390503\n",
      "Training batch 94 with loss 0.29157\n",
      "------------------------------------------\n",
      "L1 loss: 0.06978374719619751\n",
      "Training batch 95 with loss 0.25060\n",
      "------------------------------------------\n",
      "L1 loss: 0.07084892690181732\n",
      "Training batch 96 with loss 0.30898\n",
      "------------------------------------------\n",
      "L1 loss: 0.08644719421863556\n",
      "Training batch 97 with loss 0.33712\n",
      "------------------------------------------\n",
      "L1 loss: 0.05468684807419777\n",
      "Training batch 98 with loss 0.23841\n",
      "------------------------------------------\n",
      "L1 loss: 0.065741628408432\n",
      "Training batch 99 with loss 0.25795\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2949449248611927\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4854\n",
      "------------------------------------------\n",
      "L1 loss: 0.06481357663869858\n",
      "Training batch 0 with loss 0.26962\n",
      "------------------------------------------\n",
      "L1 loss: 0.08549924939870834\n",
      "Training batch 1 with loss 0.28829\n",
      "------------------------------------------\n",
      "L1 loss: 0.09521746635437012\n",
      "Training batch 2 with loss 0.31933\n",
      "------------------------------------------\n",
      "L1 loss: 0.07622555643320084\n",
      "Training batch 3 with loss 0.25927\n",
      "------------------------------------------\n",
      "L1 loss: 0.08318274468183517\n",
      "Training batch 4 with loss 0.33534\n",
      "------------------------------------------\n",
      "L1 loss: 0.0664893239736557\n",
      "Training batch 5 with loss 0.31486\n",
      "------------------------------------------\n",
      "L1 loss: 0.06887908279895782\n",
      "Training batch 6 with loss 0.27669\n",
      "------------------------------------------\n",
      "L1 loss: 0.08395563811063766\n",
      "Training batch 7 with loss 0.32919\n",
      "------------------------------------------\n",
      "L1 loss: 0.08169788122177124\n",
      "Training batch 8 with loss 0.33970\n",
      "------------------------------------------\n",
      "L1 loss: 0.05954046547412872\n",
      "Training batch 9 with loss 0.25374\n",
      "------------------------------------------\n",
      "L1 loss: 0.08041583001613617\n",
      "Training batch 10 with loss 0.28823\n",
      "------------------------------------------\n",
      "L1 loss: 0.05765173211693764\n",
      "Training batch 11 with loss 0.29386\n",
      "------------------------------------------\n",
      "L1 loss: 0.07708293199539185\n",
      "Training batch 12 with loss 0.31190\n",
      "------------------------------------------\n",
      "L1 loss: 0.07107213139533997\n",
      "Training batch 13 with loss 0.24679\n",
      "------------------------------------------\n",
      "L1 loss: 0.054387450218200684\n",
      "Training batch 14 with loss 0.26968\n",
      "------------------------------------------\n",
      "L1 loss: 0.0837700217962265\n",
      "Training batch 15 with loss 0.27551\n",
      "------------------------------------------\n",
      "L1 loss: 0.06415336579084396\n",
      "Training batch 16 with loss 0.26702\n",
      "------------------------------------------\n",
      "L1 loss: 0.08654731512069702\n",
      "Training batch 17 with loss 0.33416\n",
      "------------------------------------------\n",
      "L1 loss: 0.08076701313257217\n",
      "Training batch 18 with loss 0.30512\n",
      "------------------------------------------\n",
      "L1 loss: 0.06317615509033203\n",
      "Training batch 19 with loss 0.23481\n",
      "------------------------------------------\n",
      "L1 loss: 0.07149087637662888\n",
      "Training batch 20 with loss 0.31296\n",
      "------------------------------------------\n",
      "L1 loss: 0.07312258332967758\n",
      "Training batch 21 with loss 0.27859\n",
      "------------------------------------------\n",
      "L1 loss: 0.06920658051967621\n",
      "Training batch 22 with loss 0.27933\n",
      "------------------------------------------\n",
      "L1 loss: 0.088380828499794\n",
      "Training batch 23 with loss 0.36283\n",
      "------------------------------------------\n",
      "L1 loss: 0.06716416776180267\n",
      "Training batch 24 with loss 0.30983\n",
      "------------------------------------------\n",
      "L1 loss: 0.07484743744134903\n",
      "Training batch 25 with loss 0.30060\n",
      "------------------------------------------\n",
      "L1 loss: 0.07431814819574356\n",
      "Training batch 26 with loss 0.27955\n",
      "------------------------------------------\n",
      "L1 loss: 0.05841941758990288\n",
      "Training batch 27 with loss 0.30024\n",
      "------------------------------------------\n",
      "L1 loss: 0.05842217803001404\n",
      "Training batch 28 with loss 0.31237\n",
      "------------------------------------------\n",
      "L1 loss: 0.06414274871349335\n",
      "Training batch 29 with loss 0.26600\n",
      "------------------------------------------\n",
      "L1 loss: 0.06010795384645462\n",
      "Training batch 30 with loss 0.25952\n",
      "------------------------------------------\n",
      "L1 loss: 0.06550218164920807\n",
      "Training batch 31 with loss 0.30948\n",
      "------------------------------------------\n",
      "L1 loss: 0.06920177489519119\n",
      "Training batch 32 with loss 0.27806\n",
      "------------------------------------------\n",
      "L1 loss: 0.06036350876092911\n",
      "Training batch 33 with loss 0.24351\n",
      "------------------------------------------\n",
      "L1 loss: 0.07688861340284348\n",
      "Training batch 34 with loss 0.30076\n",
      "------------------------------------------\n",
      "L1 loss: 0.09592579305171967\n",
      "Training batch 35 with loss 0.33442\n",
      "------------------------------------------\n",
      "L1 loss: 0.082682304084301\n",
      "Training batch 36 with loss 0.29378\n",
      "------------------------------------------\n",
      "L1 loss: 0.06724490225315094\n",
      "Training batch 37 with loss 0.26807\n",
      "------------------------------------------\n",
      "L1 loss: 0.06028549000620842\n",
      "Training batch 38 with loss 0.21242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.06703000515699387\n",
      "Training batch 39 with loss 0.29937\n",
      "------------------------------------------\n",
      "L1 loss: 0.06921038031578064\n",
      "Training batch 40 with loss 0.25542\n",
      "------------------------------------------\n",
      "L1 loss: 0.05747615545988083\n",
      "Training batch 41 with loss 0.31769\n",
      "------------------------------------------\n",
      "L1 loss: 0.07652202993631363\n",
      "Training batch 42 with loss 0.26574\n",
      "------------------------------------------\n",
      "L1 loss: 0.06978517770767212\n",
      "Training batch 43 with loss 0.28218\n",
      "------------------------------------------\n",
      "L1 loss: 0.05719878152012825\n",
      "Training batch 44 with loss 0.28006\n",
      "------------------------------------------\n",
      "L1 loss: 0.0899750217795372\n",
      "Training batch 45 with loss 0.36855\n",
      "------------------------------------------\n",
      "L1 loss: 0.07212097942829132\n",
      "Training batch 46 with loss 0.26226\n",
      "------------------------------------------\n",
      "L1 loss: 0.10830581188201904\n",
      "Training batch 47 with loss 0.36280\n",
      "------------------------------------------\n",
      "L1 loss: 0.04690215364098549\n",
      "Training batch 48 with loss 0.23673\n",
      "------------------------------------------\n",
      "L1 loss: 0.07702945917844772\n",
      "Training batch 49 with loss 0.34122\n",
      "------------------------------------------\n",
      "L1 loss: 0.0751248151063919\n",
      "Training batch 50 with loss 0.32124\n",
      "------------------------------------------\n",
      "L1 loss: 0.0835992693901062\n",
      "Training batch 51 with loss 0.34665\n",
      "------------------------------------------\n",
      "L1 loss: 0.08922497183084488\n",
      "Training batch 52 with loss 0.28594\n",
      "------------------------------------------\n",
      "L1 loss: 0.07599910348653793\n",
      "Training batch 53 with loss 0.28369\n",
      "------------------------------------------\n",
      "L1 loss: 0.07427100837230682\n",
      "Training batch 54 with loss 0.26301\n",
      "------------------------------------------\n",
      "L1 loss: 0.06043011322617531\n",
      "Training batch 55 with loss 0.28331\n",
      "------------------------------------------\n",
      "L1 loss: 0.055442292243242264\n",
      "Training batch 56 with loss 0.24614\n",
      "------------------------------------------\n",
      "L1 loss: 0.06718719005584717\n",
      "Training batch 57 with loss 0.26931\n",
      "------------------------------------------\n",
      "L1 loss: 0.06822734326124191\n",
      "Training batch 58 with loss 0.26916\n",
      "------------------------------------------\n",
      "L1 loss: 0.06334119290113449\n",
      "Training batch 59 with loss 0.34624\n",
      "------------------------------------------\n",
      "L1 loss: 0.06427767872810364\n",
      "Training batch 60 with loss 0.27186\n",
      "------------------------------------------\n",
      "L1 loss: 0.098385751247406\n",
      "Training batch 61 with loss 0.34717\n",
      "------------------------------------------\n",
      "L1 loss: 0.05694133788347244\n",
      "Training batch 62 with loss 0.25442\n",
      "------------------------------------------\n",
      "L1 loss: 0.06244127079844475\n",
      "Training batch 63 with loss 0.24974\n",
      "------------------------------------------\n",
      "L1 loss: 0.06957193464040756\n",
      "Training batch 64 with loss 0.27520\n",
      "------------------------------------------\n",
      "L1 loss: 0.06016607582569122\n",
      "Training batch 65 with loss 0.23851\n",
      "------------------------------------------\n",
      "L1 loss: 0.06883718073368073\n",
      "Training batch 66 with loss 0.28393\n",
      "------------------------------------------\n",
      "L1 loss: 0.06601997464895248\n",
      "Training batch 67 with loss 0.25827\n",
      "------------------------------------------\n",
      "L1 loss: 0.07393796741962433\n",
      "Training batch 68 with loss 0.31265\n",
      "------------------------------------------\n",
      "L1 loss: 0.05136782303452492\n",
      "Training batch 69 with loss 0.26204\n",
      "------------------------------------------\n",
      "L1 loss: 0.07763977348804474\n",
      "Training batch 70 with loss 0.32251\n",
      "------------------------------------------\n",
      "L1 loss: 0.0713426023721695\n",
      "Training batch 71 with loss 0.29823\n",
      "------------------------------------------\n",
      "L1 loss: 0.08394256979227066\n",
      "Training batch 72 with loss 0.29839\n",
      "------------------------------------------\n",
      "L1 loss: 0.07341933250427246\n",
      "Training batch 73 with loss 0.31707\n",
      "------------------------------------------\n",
      "L1 loss: 0.09654662758111954\n",
      "Training batch 74 with loss 0.32144\n",
      "------------------------------------------\n",
      "L1 loss: 0.07302138954401016\n",
      "Training batch 75 with loss 0.29179\n",
      "------------------------------------------\n",
      "L1 loss: 0.08683718740940094\n",
      "Training batch 76 with loss 0.28685\n",
      "------------------------------------------\n",
      "L1 loss: 0.07964823395013809\n",
      "Training batch 77 with loss 0.27278\n",
      "------------------------------------------\n",
      "L1 loss: 0.08059258759021759\n",
      "Training batch 78 with loss 0.29299\n",
      "------------------------------------------\n",
      "L1 loss: 0.10684875398874283\n",
      "Training batch 79 with loss 0.38492\n",
      "------------------------------------------\n",
      "L1 loss: 0.07805825024843216\n",
      "Training batch 80 with loss 0.27776\n",
      "------------------------------------------\n",
      "L1 loss: 0.07032845169305801\n",
      "Training batch 81 with loss 0.27068\n",
      "------------------------------------------\n",
      "L1 loss: 0.08525394648313522\n",
      "Training batch 82 with loss 0.35668\n",
      "------------------------------------------\n",
      "L1 loss: 0.07523536682128906\n",
      "Training batch 83 with loss 0.27102\n",
      "------------------------------------------\n",
      "L1 loss: 0.07130040228366852\n",
      "Training batch 84 with loss 0.33473\n",
      "------------------------------------------\n",
      "L1 loss: 0.061700016260147095\n",
      "Training batch 85 with loss 0.26223\n",
      "------------------------------------------\n",
      "L1 loss: 0.08035597205162048\n",
      "Training batch 86 with loss 0.29208\n",
      "------------------------------------------\n",
      "L1 loss: 0.10039646923542023\n",
      "Training batch 87 with loss 0.41448\n",
      "------------------------------------------\n",
      "L1 loss: 0.06318723410367966\n",
      "Training batch 88 with loss 0.44811\n",
      "------------------------------------------\n",
      "L1 loss: 0.05684330314397812\n",
      "Training batch 89 with loss 0.32978\n",
      "------------------------------------------\n",
      "L1 loss: 0.07576892524957657\n",
      "Training batch 90 with loss 0.28264\n",
      "------------------------------------------\n",
      "L1 loss: 0.06095929443836212\n",
      "Training batch 91 with loss 0.24789\n",
      "------------------------------------------\n",
      "L1 loss: 0.09437374025583267\n",
      "Training batch 92 with loss 0.31084\n",
      "------------------------------------------\n",
      "L1 loss: 0.0626949816942215\n",
      "Training batch 93 with loss 0.23782\n",
      "------------------------------------------\n",
      "L1 loss: 0.08444277197122574\n",
      "Training batch 94 with loss 0.29605\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702957734465599\n",
      "Training batch 95 with loss 0.25976\n",
      "------------------------------------------\n",
      "L1 loss: 0.06643843650817871\n",
      "Training batch 96 with loss 0.27749\n",
      "------------------------------------------\n",
      "L1 loss: 0.08478786051273346\n",
      "Training batch 97 with loss 0.36688\n",
      "------------------------------------------\n",
      "L1 loss: 0.05607122182846069\n",
      "Training batch 98 with loss 0.22866\n",
      "------------------------------------------\n",
      "L1 loss: 0.06531369686126709\n",
      "Training batch 99 with loss 0.26939\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2937784066796303\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4855\n",
      "------------------------------------------\n",
      "L1 loss: 0.061113569885492325\n",
      "Training batch 0 with loss 0.24446\n",
      "------------------------------------------\n",
      "L1 loss: 0.08142977207899094\n",
      "Training batch 1 with loss 0.29078\n",
      "------------------------------------------\n",
      "L1 loss: 0.0793677419424057\n",
      "Training batch 2 with loss 0.79820\n",
      "------------------------------------------\n",
      "L1 loss: 0.0772923156619072\n",
      "Training batch 3 with loss 0.26540\n",
      "------------------------------------------\n",
      "L1 loss: 0.08410844951868057\n",
      "Training batch 4 with loss 0.32600\n",
      "------------------------------------------\n",
      "L1 loss: 0.06546300649642944\n",
      "Training batch 5 with loss 0.31643\n",
      "------------------------------------------\n",
      "L1 loss: 0.0783478319644928\n",
      "Training batch 6 with loss 0.27018\n",
      "------------------------------------------\n",
      "L1 loss: 0.08357325941324234\n",
      "Training batch 7 with loss 0.33803\n",
      "------------------------------------------\n",
      "L1 loss: 0.08186619728803635\n",
      "Training batch 8 with loss 0.33339\n",
      "------------------------------------------\n",
      "L1 loss: 0.06185505911707878\n",
      "Training batch 9 with loss 0.25341\n",
      "------------------------------------------\n",
      "L1 loss: 0.08249058574438095\n",
      "Training batch 10 with loss 0.29790\n",
      "------------------------------------------\n",
      "L1 loss: 0.06056961044669151\n",
      "Training batch 11 with loss 0.29807\n",
      "------------------------------------------\n",
      "L1 loss: 0.07723688334226608\n",
      "Training batch 12 with loss 0.31702\n",
      "------------------------------------------\n",
      "L1 loss: 0.06839345395565033\n",
      "Training batch 13 with loss 0.23974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.05244884267449379\n",
      "Training batch 14 with loss 0.24054\n",
      "------------------------------------------\n",
      "L1 loss: 0.08352793753147125\n",
      "Training batch 15 with loss 0.27518\n",
      "------------------------------------------\n",
      "L1 loss: 0.06502733379602432\n",
      "Training batch 16 with loss 0.27199\n",
      "------------------------------------------\n",
      "L1 loss: 0.08524526655673981\n",
      "Training batch 17 with loss 0.33000\n",
      "------------------------------------------\n",
      "L1 loss: 0.0798119455575943\n",
      "Training batch 18 with loss 0.30271\n",
      "------------------------------------------\n",
      "L1 loss: 0.06123560667037964\n",
      "Training batch 19 with loss 0.22439\n",
      "------------------------------------------\n",
      "L1 loss: 0.0638260617852211\n",
      "Training batch 20 with loss 0.61784\n",
      "------------------------------------------\n",
      "L1 loss: 0.0682850033044815\n",
      "Training batch 21 with loss 0.26741\n",
      "------------------------------------------\n",
      "L1 loss: 0.06940609216690063\n",
      "Training batch 22 with loss 0.31689\n",
      "------------------------------------------\n",
      "L1 loss: 0.08804653584957123\n",
      "Training batch 23 with loss 0.35521\n",
      "------------------------------------------\n",
      "L1 loss: 0.06812659651041031\n",
      "Training batch 24 with loss 0.29182\n",
      "------------------------------------------\n",
      "L1 loss: 0.07609102874994278\n",
      "Training batch 25 with loss 0.29726\n",
      "------------------------------------------\n",
      "L1 loss: 0.07835016399621964\n",
      "Training batch 26 with loss 0.27974\n",
      "------------------------------------------\n",
      "L1 loss: 0.05547310411930084\n",
      "Training batch 27 with loss 0.27968\n",
      "------------------------------------------\n",
      "L1 loss: 0.057619307190179825\n",
      "Training batch 28 with loss 0.28670\n",
      "------------------------------------------\n",
      "L1 loss: 0.0411943681538105\n",
      "Training batch 29 with loss 0.25114\n",
      "------------------------------------------\n",
      "L1 loss: 0.06031529977917671\n",
      "Training batch 30 with loss 0.27693\n",
      "------------------------------------------\n",
      "L1 loss: 0.0648646429181099\n",
      "Training batch 31 with loss 0.29291\n",
      "------------------------------------------\n",
      "L1 loss: 0.06795471906661987\n",
      "Training batch 32 with loss 0.30377\n",
      "------------------------------------------\n",
      "L1 loss: 0.06145966798067093\n",
      "Training batch 33 with loss 0.27081\n",
      "------------------------------------------\n",
      "L1 loss: 0.07539519667625427\n",
      "Training batch 34 with loss 0.33702\n",
      "------------------------------------------\n",
      "L1 loss: 0.09404341131448746\n",
      "Training batch 35 with loss 0.33718\n",
      "------------------------------------------\n",
      "L1 loss: 0.08173483610153198\n",
      "Training batch 36 with loss 0.29287\n",
      "------------------------------------------\n",
      "L1 loss: 0.06581984460353851\n",
      "Training batch 37 with loss 0.25519\n",
      "------------------------------------------\n",
      "L1 loss: 0.06473417580127716\n",
      "Training batch 38 with loss 0.21766\n",
      "------------------------------------------\n",
      "L1 loss: 0.06440719962120056\n",
      "Training batch 39 with loss 0.28399\n",
      "------------------------------------------\n",
      "L1 loss: 0.07154317200183868\n",
      "Training batch 40 with loss 0.26141\n",
      "------------------------------------------\n",
      "L1 loss: 0.059248149394989014\n",
      "Training batch 41 with loss 0.28582\n",
      "------------------------------------------\n",
      "L1 loss: 0.078375443816185\n",
      "Training batch 42 with loss 0.27525\n",
      "------------------------------------------\n",
      "L1 loss: 0.07020211219787598\n",
      "Training batch 43 with loss 0.28304\n",
      "------------------------------------------\n",
      "L1 loss: 0.058405883610248566\n",
      "Training batch 44 with loss 0.26645\n",
      "------------------------------------------\n",
      "L1 loss: 0.09218727797269821\n",
      "Training batch 45 with loss 0.38357\n",
      "------------------------------------------\n",
      "L1 loss: 0.07206510007381439\n",
      "Training batch 46 with loss 0.27797\n",
      "------------------------------------------\n",
      "L1 loss: 0.11003780364990234\n",
      "Training batch 47 with loss 0.35986\n",
      "------------------------------------------\n",
      "L1 loss: 0.04435550794005394\n",
      "Training batch 48 with loss 0.24046\n",
      "------------------------------------------\n",
      "L1 loss: 0.08009705692529678\n",
      "Training batch 49 with loss 0.34165\n",
      "------------------------------------------\n",
      "L1 loss: 0.07395868748426437\n",
      "Training batch 50 with loss 0.30140\n",
      "------------------------------------------\n",
      "L1 loss: 0.07875455170869827\n",
      "Training batch 51 with loss 0.33418\n",
      "------------------------------------------\n",
      "L1 loss: 0.09099103510379791\n",
      "Training batch 52 with loss 0.28606\n",
      "------------------------------------------\n",
      "L1 loss: 0.07884373515844345\n",
      "Training batch 53 with loss 0.28489\n",
      "------------------------------------------\n",
      "L1 loss: 0.07107941806316376\n",
      "Training batch 54 with loss 0.26616\n",
      "------------------------------------------\n",
      "L1 loss: 0.06072969734668732\n",
      "Training batch 55 with loss 0.30780\n",
      "------------------------------------------\n",
      "L1 loss: 0.058357998728752136\n",
      "Training batch 56 with loss 0.26269\n",
      "------------------------------------------\n",
      "L1 loss: 0.06866911798715591\n",
      "Training batch 57 with loss 0.28091\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683288723230362\n",
      "Training batch 58 with loss 0.27279\n",
      "------------------------------------------\n",
      "L1 loss: 0.08192263543605804\n",
      "Training batch 59 with loss 0.29391\n",
      "------------------------------------------\n",
      "L1 loss: 0.060581136494874954\n",
      "Training batch 60 with loss 0.27558\n",
      "------------------------------------------\n",
      "L1 loss: 0.09544867277145386\n",
      "Training batch 61 with loss 0.34795\n",
      "------------------------------------------\n",
      "L1 loss: 0.057275738567113876\n",
      "Training batch 62 with loss 0.25626\n",
      "------------------------------------------\n",
      "L1 loss: 0.06272250413894653\n",
      "Training batch 63 with loss 0.25200\n",
      "------------------------------------------\n",
      "L1 loss: 0.07037153095006943\n",
      "Training batch 64 with loss 0.28950\n",
      "------------------------------------------\n",
      "L1 loss: 0.056700367480516434\n",
      "Training batch 65 with loss 0.22269\n",
      "------------------------------------------\n",
      "L1 loss: 0.070777527987957\n",
      "Training batch 66 with loss 0.55198\n",
      "------------------------------------------\n",
      "L1 loss: 0.06463569402694702\n",
      "Training batch 67 with loss 0.27052\n",
      "------------------------------------------\n",
      "L1 loss: 0.06622441858053207\n",
      "Training batch 68 with loss 0.25473\n",
      "------------------------------------------\n",
      "L1 loss: 0.050825800746679306\n",
      "Training batch 69 with loss 0.27324\n",
      "------------------------------------------\n",
      "L1 loss: 0.0795426219701767\n",
      "Training batch 70 with loss 0.31239\n",
      "------------------------------------------\n",
      "L1 loss: 0.07214727252721786\n",
      "Training batch 71 with loss 0.29813\n",
      "------------------------------------------\n",
      "L1 loss: 0.08531998842954636\n",
      "Training batch 72 with loss 0.31215\n",
      "------------------------------------------\n",
      "L1 loss: 0.07318542897701263\n",
      "Training batch 73 with loss 0.29319\n",
      "------------------------------------------\n",
      "L1 loss: 0.09813328087329865\n",
      "Training batch 74 with loss 0.31623\n",
      "------------------------------------------\n",
      "L1 loss: 0.07647666335105896\n",
      "Training batch 75 with loss 0.29194\n",
      "------------------------------------------\n",
      "L1 loss: 0.08625101298093796\n",
      "Training batch 76 with loss 0.29806\n",
      "------------------------------------------\n",
      "L1 loss: 0.0793827548623085\n",
      "Training batch 77 with loss 0.27378\n",
      "------------------------------------------\n",
      "L1 loss: 0.07953013479709625\n",
      "Training batch 78 with loss 0.30695\n",
      "------------------------------------------\n",
      "L1 loss: 0.10418067127466202\n",
      "Training batch 79 with loss 0.37852\n",
      "------------------------------------------\n",
      "L1 loss: 0.07673901319503784\n",
      "Training batch 80 with loss 0.28363\n",
      "------------------------------------------\n",
      "L1 loss: 0.0807250514626503\n",
      "Training batch 81 with loss 0.30156\n",
      "------------------------------------------\n",
      "L1 loss: 0.08577551692724228\n",
      "Training batch 82 with loss 0.35190\n",
      "------------------------------------------\n",
      "L1 loss: 0.07421168684959412\n",
      "Training batch 83 with loss 0.28128\n",
      "------------------------------------------\n",
      "L1 loss: 0.07208902388811111\n",
      "Training batch 84 with loss 0.29198\n",
      "------------------------------------------\n",
      "L1 loss: 0.06626038998365402\n",
      "Training batch 85 with loss 0.26178\n",
      "------------------------------------------\n",
      "L1 loss: 0.08257371187210083\n",
      "Training batch 86 with loss 0.31009\n",
      "------------------------------------------\n",
      "L1 loss: 0.10054264962673187\n",
      "Training batch 87 with loss 0.38410\n",
      "------------------------------------------\n",
      "L1 loss: 0.07588764280080795\n",
      "Training batch 88 with loss 0.31439\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018952071666718\n",
      "Training batch 89 with loss 0.25941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "L1 loss: 0.07465153932571411\n",
      "Training batch 90 with loss 0.27334\n",
      "------------------------------------------\n",
      "L1 loss: 0.06204383447766304\n",
      "Training batch 91 with loss 0.25283\n",
      "------------------------------------------\n",
      "L1 loss: 0.08796916902065277\n",
      "Training batch 92 with loss 0.28931\n",
      "------------------------------------------\n",
      "L1 loss: 0.05463837832212448\n",
      "Training batch 93 with loss 0.23127\n",
      "------------------------------------------\n",
      "L1 loss: 0.08559528738260269\n",
      "Training batch 94 with loss 0.29693\n",
      "------------------------------------------\n",
      "L1 loss: 0.07009394466876984\n",
      "Training batch 95 with loss 0.24756\n",
      "------------------------------------------\n",
      "L1 loss: 0.06116890907287598\n",
      "Training batch 96 with loss 0.28496\n",
      "------------------------------------------\n",
      "L1 loss: 0.0822373479604721\n",
      "Training batch 97 with loss 0.37143\n",
      "------------------------------------------\n",
      "L1 loss: 0.05646814405918121\n",
      "Training batch 98 with loss 0.24109\n",
      "------------------------------------------\n",
      "L1 loss: 0.06091146171092987\n",
      "Training batch 99 with loss 0.26255\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.3007929874956608\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4856\n",
      "------------------------------------------\n",
      "L1 loss: 0.05995078757405281\n",
      "Training batch 0 with loss 0.26393\n",
      "------------------------------------------\n",
      "L1 loss: 0.08638013154268265\n",
      "Training batch 1 with loss 0.28156\n",
      "------------------------------------------\n",
      "L1 loss: 0.09347384423017502\n",
      "Training batch 2 with loss 0.32428\n",
      "------------------------------------------\n",
      "L1 loss: 0.05928003042936325\n",
      "Training batch 3 with loss 0.24567\n",
      "------------------------------------------\n",
      "L1 loss: 0.08451613038778305\n",
      "Training batch 4 with loss 0.33518\n",
      "------------------------------------------\n",
      "L1 loss: 0.06749194115400314\n",
      "Training batch 5 with loss 0.33849\n",
      "------------------------------------------\n",
      "L1 loss: 0.07956366240978241\n",
      "Training batch 6 with loss 0.27679\n",
      "------------------------------------------\n",
      "L1 loss: 0.08411198854446411\n",
      "Training batch 7 with loss 0.33612\n",
      "------------------------------------------\n",
      "L1 loss: 0.08105797320604324\n",
      "Training batch 8 with loss 0.32435\n",
      "------------------------------------------\n",
      "L1 loss: 0.05907130986452103\n",
      "Training batch 9 with loss 0.25137\n",
      "------------------------------------------\n",
      "L1 loss: 0.08281645178794861\n",
      "Training batch 10 with loss 0.30600\n",
      "------------------------------------------\n",
      "L1 loss: 0.057480089366436005\n",
      "Training batch 11 with loss 0.29015\n",
      "------------------------------------------\n",
      "L1 loss: 0.07523589581251144\n",
      "Training batch 12 with loss 0.30607\n",
      "------------------------------------------\n",
      "L1 loss: 0.06854652613401413\n",
      "Training batch 13 with loss 0.25331\n",
      "------------------------------------------\n",
      "L1 loss: 0.05210435017943382\n",
      "Training batch 14 with loss 0.24877\n",
      "------------------------------------------\n",
      "L1 loss: 0.08540287613868713\n",
      "Training batch 15 with loss 0.26794\n",
      "------------------------------------------\n",
      "L1 loss: 0.06271906942129135\n",
      "Training batch 16 with loss 0.26994\n",
      "------------------------------------------\n",
      "L1 loss: 0.08806400746107101\n",
      "Training batch 17 with loss 0.33988\n",
      "------------------------------------------\n",
      "L1 loss: 0.08137068897485733\n",
      "Training batch 18 with loss 0.29433\n",
      "------------------------------------------\n",
      "L1 loss: 0.05969061702489853\n",
      "Training batch 19 with loss 0.23918\n",
      "------------------------------------------\n",
      "L1 loss: 0.07294603437185287\n",
      "Training batch 20 with loss 0.32560\n",
      "------------------------------------------\n",
      "L1 loss: 0.07249458134174347\n",
      "Training batch 21 with loss 0.26847\n",
      "------------------------------------------\n",
      "L1 loss: 0.06993888318538666\n",
      "Training batch 22 with loss 0.28709\n",
      "------------------------------------------\n",
      "L1 loss: 0.08486161381006241\n",
      "Training batch 23 with loss 0.34643\n",
      "------------------------------------------\n",
      "L1 loss: 0.06710413098335266\n",
      "Training batch 24 with loss 0.29840\n",
      "------------------------------------------\n",
      "L1 loss: 0.07311812043190002\n",
      "Training batch 25 with loss 0.28363\n",
      "------------------------------------------\n",
      "L1 loss: 0.07037816196680069\n",
      "Training batch 26 with loss 0.27218\n",
      "------------------------------------------\n",
      "L1 loss: 0.05529284477233887\n",
      "Training batch 27 with loss 0.28233\n",
      "------------------------------------------\n",
      "L1 loss: 0.05781307816505432\n",
      "Training batch 28 with loss 0.29846\n",
      "------------------------------------------\n",
      "L1 loss: 0.06281992048025131\n",
      "Training batch 29 with loss 0.26624\n",
      "------------------------------------------\n",
      "L1 loss: 0.060708798468112946\n",
      "Training batch 30 with loss 0.27387\n",
      "------------------------------------------\n",
      "L1 loss: 0.06652920693159103\n",
      "Training batch 31 with loss 0.30570\n",
      "------------------------------------------\n",
      "L1 loss: 0.07059282064437866\n",
      "Training batch 32 with loss 0.29170\n",
      "------------------------------------------\n",
      "L1 loss: 0.06550832837820053\n",
      "Training batch 33 with loss 0.25915\n",
      "------------------------------------------\n",
      "L1 loss: 0.07520552724599838\n",
      "Training batch 34 with loss 0.31613\n",
      "------------------------------------------\n",
      "L1 loss: 0.09325068444013596\n",
      "Training batch 35 with loss 0.32491\n",
      "------------------------------------------\n",
      "L1 loss: 0.08929499238729477\n",
      "Training batch 36 with loss 0.28512\n",
      "------------------------------------------\n",
      "L1 loss: 0.0694708600640297\n",
      "Training batch 37 with loss 0.26587\n",
      "------------------------------------------\n",
      "L1 loss: 0.06290341168642044\n",
      "Training batch 38 with loss 0.22168\n",
      "------------------------------------------\n",
      "L1 loss: 0.05361460894346237\n",
      "Training batch 39 with loss 0.54938\n",
      "------------------------------------------\n",
      "L1 loss: 0.0756620392203331\n",
      "Training batch 40 with loss 0.27590\n",
      "------------------------------------------\n",
      "L1 loss: 0.05711529776453972\n",
      "Training batch 41 with loss 0.30387\n",
      "------------------------------------------\n",
      "L1 loss: 0.07840686291456223\n",
      "Training batch 42 with loss 0.26333\n",
      "------------------------------------------\n",
      "L1 loss: 0.07059349119663239\n",
      "Training batch 43 with loss 0.29258\n",
      "------------------------------------------\n",
      "L1 loss: 0.05536826327443123\n",
      "Training batch 44 with loss 0.25297\n",
      "------------------------------------------\n",
      "L1 loss: 0.0935140922665596\n",
      "Training batch 45 with loss 0.36447\n",
      "------------------------------------------\n",
      "L1 loss: 0.07167082279920578\n",
      "Training batch 46 with loss 0.28267\n",
      "------------------------------------------\n",
      "L1 loss: 0.11015861481428146\n",
      "Training batch 47 with loss 0.35766\n",
      "------------------------------------------\n",
      "L1 loss: 0.041071031242609024\n",
      "Training batch 48 with loss 0.22979\n",
      "------------------------------------------\n",
      "L1 loss: 0.08173473179340363\n",
      "Training batch 49 with loss 0.35788\n",
      "------------------------------------------\n",
      "L1 loss: 0.07247733324766159\n",
      "Training batch 50 with loss 0.28585\n",
      "------------------------------------------\n",
      "L1 loss: 0.08412360399961472\n",
      "Training batch 51 with loss 0.36779\n",
      "------------------------------------------\n",
      "L1 loss: 0.09042295068502426\n",
      "Training batch 52 with loss 0.28043\n",
      "------------------------------------------\n",
      "L1 loss: 0.08033241331577301\n",
      "Training batch 53 with loss 0.28384\n",
      "------------------------------------------\n",
      "L1 loss: 0.0779326781630516\n",
      "Training batch 54 with loss 0.26377\n",
      "------------------------------------------\n",
      "L1 loss: 0.061292242258787155\n",
      "Training batch 55 with loss 0.29187\n",
      "------------------------------------------\n",
      "L1 loss: 0.057467736303806305\n",
      "Training batch 56 with loss 0.26018\n",
      "------------------------------------------\n",
      "L1 loss: 0.06683530658483505\n",
      "Training batch 57 with loss 0.27990\n",
      "------------------------------------------\n",
      "L1 loss: 0.05520832911133766\n",
      "Training batch 58 with loss 0.44604\n",
      "------------------------------------------\n",
      "L1 loss: 0.07768435776233673\n",
      "Training batch 59 with loss 0.28696\n",
      "------------------------------------------\n",
      "L1 loss: 0.0654379203915596\n",
      "Training batch 60 with loss 0.27562\n",
      "------------------------------------------\n",
      "L1 loss: 0.09923549741506577\n",
      "Training batch 61 with loss 0.34709\n",
      "------------------------------------------\n",
      "L1 loss: 0.05555155873298645\n",
      "Training batch 62 with loss 0.25013\n",
      "------------------------------------------\n",
      "L1 loss: 0.04762804135680199\n",
      "Training batch 63 with loss 0.18818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07137221097946167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 64 with loss 0.27011\n",
      "------------------------------------------\n",
      "L1 loss: 0.06184285879135132\n",
      "Training batch 65 with loss 0.23863\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683482363820076\n",
      "Training batch 66 with loss 0.28008\n",
      "------------------------------------------\n",
      "L1 loss: 0.065586619079113\n",
      "Training batch 67 with loss 0.27372\n",
      "------------------------------------------\n",
      "L1 loss: 0.07651756703853607\n",
      "Training batch 68 with loss 0.30793\n",
      "------------------------------------------\n",
      "L1 loss: 0.04823683947324753\n",
      "Training batch 69 with loss 0.25762\n",
      "------------------------------------------\n",
      "L1 loss: 0.08140911161899567\n",
      "Training batch 70 with loss 0.31172\n",
      "------------------------------------------\n",
      "L1 loss: 0.0712960734963417\n",
      "Training batch 71 with loss 0.34746\n",
      "------------------------------------------\n",
      "L1 loss: 0.08623209595680237\n",
      "Training batch 72 with loss 0.29373\n",
      "------------------------------------------\n",
      "L1 loss: 0.07113829255104065\n",
      "Training batch 73 with loss 0.29030\n",
      "------------------------------------------\n",
      "L1 loss: 0.09679398685693741\n",
      "Training batch 74 with loss 0.31785\n",
      "------------------------------------------\n",
      "L1 loss: 0.07230304181575775\n",
      "Training batch 75 with loss 0.28195\n",
      "------------------------------------------\n",
      "L1 loss: 0.08643286675214767\n",
      "Training batch 76 with loss 0.30062\n",
      "------------------------------------------\n",
      "L1 loss: 0.08057450503110886\n",
      "Training batch 77 with loss 0.27794\n",
      "------------------------------------------\n",
      "L1 loss: 0.07845717668533325\n",
      "Training batch 78 with loss 0.29011\n",
      "------------------------------------------\n",
      "L1 loss: 0.10720366984605789\n",
      "Training batch 79 with loss 0.38009\n",
      "------------------------------------------\n",
      "L1 loss: 0.07559848576784134\n",
      "Training batch 80 with loss 0.28833\n",
      "------------------------------------------\n",
      "L1 loss: 0.07852931320667267\n",
      "Training batch 81 with loss 0.28754\n",
      "------------------------------------------\n",
      "L1 loss: 0.07178938388824463\n",
      "Training batch 82 with loss 0.38432\n",
      "------------------------------------------\n",
      "L1 loss: 0.07530111074447632\n",
      "Training batch 83 with loss 0.28633\n",
      "------------------------------------------\n",
      "L1 loss: 0.06752396374940872\n",
      "Training batch 84 with loss 0.30192\n",
      "------------------------------------------\n",
      "L1 loss: 0.06230863928794861\n",
      "Training batch 85 with loss 0.28286\n",
      "------------------------------------------\n",
      "L1 loss: 0.0790480300784111\n",
      "Training batch 86 with loss 0.28050\n",
      "------------------------------------------\n",
      "L1 loss: 0.10321366786956787\n",
      "Training batch 87 with loss 0.38715\n",
      "------------------------------------------\n",
      "L1 loss: 0.07862471044063568\n",
      "Training batch 88 with loss 0.33827\n",
      "------------------------------------------\n",
      "L1 loss: 0.06762100756168365\n",
      "Training batch 89 with loss 0.27324\n",
      "------------------------------------------\n",
      "L1 loss: 0.07207249104976654\n",
      "Training batch 90 with loss 0.27114\n",
      "------------------------------------------\n",
      "L1 loss: 0.06471406668424606\n",
      "Training batch 91 with loss 0.25257\n",
      "------------------------------------------\n",
      "L1 loss: 0.09435014426708221\n",
      "Training batch 92 with loss 0.29990\n",
      "------------------------------------------\n",
      "L1 loss: 0.060390740633010864\n",
      "Training batch 93 with loss 0.23988\n",
      "------------------------------------------\n",
      "L1 loss: 0.08633702248334885\n",
      "Training batch 94 with loss 0.30466\n",
      "------------------------------------------\n",
      "L1 loss: 0.06924037635326385\n",
      "Training batch 95 with loss 0.24828\n",
      "------------------------------------------\n",
      "L1 loss: 0.07389924675226212\n",
      "Training batch 96 with loss 0.31331\n",
      "------------------------------------------\n",
      "L1 loss: 0.08867020159959793\n",
      "Training batch 97 with loss 0.35784\n",
      "------------------------------------------\n",
      "L1 loss: 0.05702237784862518\n",
      "Training batch 98 with loss 0.24981\n",
      "------------------------------------------\n",
      "L1 loss: 0.06335145235061646\n",
      "Training batch 99 with loss 0.27309\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2954524154961109\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4857\n",
      "------------------------------------------\n",
      "L1 loss: 0.06027141213417053\n",
      "Training batch 0 with loss 0.25160\n",
      "------------------------------------------\n",
      "L1 loss: 0.08323092013597488\n",
      "Training batch 1 with loss 0.27455\n",
      "------------------------------------------\n",
      "L1 loss: 0.09265002608299255\n",
      "Training batch 2 with loss 0.32644\n",
      "------------------------------------------\n",
      "L1 loss: 0.07206568866968155\n",
      "Training batch 3 with loss 0.25657\n",
      "------------------------------------------\n",
      "L1 loss: 0.08313684165477753\n",
      "Training batch 4 with loss 0.32086\n",
      "------------------------------------------\n",
      "L1 loss: 0.06916852295398712\n",
      "Training batch 5 with loss 0.33294\n",
      "------------------------------------------\n",
      "L1 loss: 0.08246901631355286\n",
      "Training batch 6 with loss 0.27729\n",
      "------------------------------------------\n",
      "L1 loss: 0.0825151577591896\n",
      "Training batch 7 with loss 0.32265\n",
      "------------------------------------------\n",
      "L1 loss: 0.08078259229660034\n",
      "Training batch 8 with loss 0.32029\n",
      "------------------------------------------\n",
      "L1 loss: 0.05930851027369499\n",
      "Training batch 9 with loss 0.25738\n",
      "------------------------------------------\n",
      "L1 loss: 0.08437942713499069\n",
      "Training batch 10 with loss 0.30437\n",
      "------------------------------------------\n",
      "L1 loss: 0.0622476302087307\n",
      "Training batch 11 with loss 0.28547\n",
      "------------------------------------------\n",
      "L1 loss: 0.0728100836277008\n",
      "Training batch 12 with loss 0.30703\n",
      "------------------------------------------\n",
      "L1 loss: 0.06935902684926987\n",
      "Training batch 13 with loss 0.26069\n",
      "------------------------------------------\n",
      "L1 loss: 0.05274282023310661\n",
      "Training batch 14 with loss 0.24584\n",
      "------------------------------------------\n",
      "L1 loss: 0.0828915685415268\n",
      "Training batch 15 with loss 0.27073\n",
      "------------------------------------------\n",
      "L1 loss: 0.06284017860889435\n",
      "Training batch 16 with loss 0.25902\n",
      "------------------------------------------\n",
      "L1 loss: 0.08452821522951126\n",
      "Training batch 17 with loss 0.33418\n",
      "------------------------------------------\n",
      "L1 loss: 0.08017052710056305\n",
      "Training batch 18 with loss 0.30957\n",
      "------------------------------------------\n",
      "L1 loss: 0.06369318068027496\n",
      "Training batch 19 with loss 0.23800\n",
      "------------------------------------------\n",
      "L1 loss: 0.07256434857845306\n",
      "Training batch 20 with loss 0.31701\n",
      "------------------------------------------\n",
      "L1 loss: 0.06998714804649353\n",
      "Training batch 21 with loss 0.26543\n",
      "------------------------------------------\n",
      "L1 loss: 0.06829801201820374\n",
      "Training batch 22 with loss 0.28441\n",
      "------------------------------------------\n",
      "L1 loss: 0.0836753398180008\n",
      "Training batch 23 with loss 0.35083\n",
      "------------------------------------------\n",
      "L1 loss: 0.06933516263961792\n",
      "Training batch 24 with loss 0.31361\n",
      "------------------------------------------\n",
      "L1 loss: 0.0769188180565834\n",
      "Training batch 25 with loss 0.31206\n",
      "------------------------------------------\n",
      "L1 loss: 0.07244790345430374\n",
      "Training batch 26 with loss 0.27346\n",
      "------------------------------------------\n",
      "L1 loss: 0.05474979802966118\n",
      "Training batch 27 with loss 0.28267\n",
      "------------------------------------------\n",
      "L1 loss: 0.05814516544342041\n",
      "Training batch 28 with loss 0.28671\n",
      "------------------------------------------\n",
      "L1 loss: 0.05605020746588707\n",
      "Training batch 29 with loss 0.24584\n",
      "------------------------------------------\n",
      "L1 loss: 0.05982368066906929\n",
      "Training batch 30 with loss 0.26614\n",
      "------------------------------------------\n",
      "L1 loss: 0.06314602494239807\n",
      "Training batch 31 with loss 0.29300\n",
      "------------------------------------------\n",
      "L1 loss: 0.06983422487974167\n",
      "Training batch 32 with loss 0.28119\n",
      "------------------------------------------\n",
      "L1 loss: 0.060126036405563354\n",
      "Training batch 33 with loss 0.25701\n",
      "------------------------------------------\n",
      "L1 loss: 0.07247734814882278\n",
      "Training batch 34 with loss 0.31957\n",
      "------------------------------------------\n",
      "L1 loss: 0.0946163684129715\n",
      "Training batch 35 with loss 0.33254\n",
      "------------------------------------------\n",
      "L1 loss: 0.08216439932584763\n",
      "Training batch 36 with loss 0.27266\n",
      "------------------------------------------\n",
      "L1 loss: 0.0711456835269928\n",
      "Training batch 37 with loss 0.28120\n",
      "------------------------------------------\n",
      "L1 loss: 0.06378456950187683\n",
      "Training batch 38 with loss 0.22658\n",
      "------------------------------------------\n",
      "L1 loss: 0.06683007627725601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 39 with loss 0.29493\n",
      "------------------------------------------\n",
      "L1 loss: 0.07361409813165665\n",
      "Training batch 40 with loss 0.26491\n",
      "------------------------------------------\n",
      "L1 loss: 0.05983472242951393\n",
      "Training batch 41 with loss 0.29443\n",
      "------------------------------------------\n",
      "L1 loss: 0.07958053797483444\n",
      "Training batch 42 with loss 0.26933\n",
      "------------------------------------------\n",
      "L1 loss: 0.07273711264133453\n",
      "Training batch 43 with loss 0.29987\n",
      "------------------------------------------\n",
      "L1 loss: 0.05855569988489151\n",
      "Training batch 44 with loss 0.26772\n",
      "------------------------------------------\n",
      "L1 loss: 0.08381686359643936\n",
      "Training batch 45 with loss 0.48343\n",
      "------------------------------------------\n",
      "L1 loss: 0.07391832023859024\n",
      "Training batch 46 with loss 0.27467\n",
      "------------------------------------------\n",
      "L1 loss: 0.10972780734300613\n",
      "Training batch 47 with loss 0.35267\n",
      "------------------------------------------\n",
      "L1 loss: 0.04941688850522041\n",
      "Training batch 48 with loss 0.23415\n",
      "------------------------------------------\n",
      "L1 loss: 0.0826243981719017\n",
      "Training batch 49 with loss 0.36300\n",
      "------------------------------------------\n",
      "L1 loss: 0.0735127255320549\n",
      "Training batch 50 with loss 0.28868\n",
      "------------------------------------------\n",
      "L1 loss: 0.08079248666763306\n",
      "Training batch 51 with loss 0.33796\n",
      "------------------------------------------\n",
      "L1 loss: 0.08819179236888885\n",
      "Training batch 52 with loss 0.28109\n",
      "------------------------------------------\n",
      "L1 loss: 0.0782778412103653\n",
      "Training batch 53 with loss 0.28222\n",
      "------------------------------------------\n",
      "L1 loss: 0.0717238262295723\n",
      "Training batch 54 with loss 0.25685\n",
      "------------------------------------------\n",
      "L1 loss: 0.05950421094894409\n",
      "Training batch 55 with loss 0.30752\n",
      "------------------------------------------\n",
      "L1 loss: 0.05537481978535652\n",
      "Training batch 56 with loss 0.24264\n",
      "------------------------------------------\n",
      "L1 loss: 0.06857671588659286\n",
      "Training batch 57 with loss 0.27125\n",
      "------------------------------------------\n",
      "L1 loss: 0.06680557876825333\n",
      "Training batch 58 with loss 0.27091\n",
      "------------------------------------------\n",
      "L1 loss: 0.07934077829122543\n",
      "Training batch 59 with loss 0.28590\n",
      "------------------------------------------\n",
      "L1 loss: 0.06483285874128342\n",
      "Training batch 60 with loss 0.27025\n",
      "------------------------------------------\n",
      "L1 loss: 0.10382705181837082\n",
      "Training batch 61 with loss 0.37420\n",
      "------------------------------------------\n",
      "L1 loss: 0.05682267248630524\n",
      "Training batch 62 with loss 0.25838\n",
      "------------------------------------------\n",
      "L1 loss: 0.05856230854988098\n",
      "Training batch 63 with loss 0.24530\n",
      "------------------------------------------\n",
      "L1 loss: 0.06959803402423859\n",
      "Training batch 64 with loss 0.27563\n",
      "------------------------------------------\n",
      "L1 loss: 0.06012583151459694\n",
      "Training batch 65 with loss 0.23129\n",
      "------------------------------------------\n",
      "L1 loss: 0.06184414029121399\n",
      "Training batch 66 with loss 0.25997\n",
      "------------------------------------------\n",
      "L1 loss: 0.06440911442041397\n",
      "Training batch 67 with loss 0.26311\n",
      "------------------------------------------\n",
      "L1 loss: 0.07232747226953506\n",
      "Training batch 68 with loss 0.30326\n",
      "------------------------------------------\n",
      "L1 loss: 0.05267573148012161\n",
      "Training batch 69 with loss 0.26750\n",
      "------------------------------------------\n",
      "L1 loss: 0.08251593261957169\n",
      "Training batch 70 with loss 0.30496\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709969624876976\n",
      "Training batch 71 with loss 0.31125\n",
      "------------------------------------------\n",
      "L1 loss: 0.08378883451223373\n",
      "Training batch 72 with loss 0.29209\n",
      "------------------------------------------\n",
      "L1 loss: 0.07632286846637726\n",
      "Training batch 73 with loss 0.30695\n",
      "------------------------------------------\n",
      "L1 loss: 0.09772900491952896\n",
      "Training batch 74 with loss 0.30273\n",
      "------------------------------------------\n",
      "L1 loss: 0.07585319876670837\n",
      "Training batch 75 with loss 0.27746\n",
      "------------------------------------------\n",
      "L1 loss: 0.08550181239843369\n",
      "Training batch 76 with loss 0.30026\n",
      "------------------------------------------\n",
      "L1 loss: 0.0800478458404541\n",
      "Training batch 77 with loss 0.26924\n",
      "------------------------------------------\n",
      "L1 loss: 0.07317076623439789\n",
      "Training batch 78 with loss 0.44530\n",
      "------------------------------------------\n",
      "L1 loss: 0.10564126819372177\n",
      "Training batch 79 with loss 0.37977\n",
      "------------------------------------------\n",
      "L1 loss: 0.07589049637317657\n",
      "Training batch 80 with loss 0.28137\n",
      "------------------------------------------\n",
      "L1 loss: 0.07562976330518723\n",
      "Training batch 81 with loss 0.28304\n",
      "------------------------------------------\n",
      "L1 loss: 0.08702635020017624\n",
      "Training batch 82 with loss 0.35301\n",
      "------------------------------------------\n",
      "L1 loss: 0.08036164194345474\n",
      "Training batch 83 with loss 0.29155\n",
      "------------------------------------------\n",
      "L1 loss: 0.07030271738767624\n",
      "Training batch 84 with loss 0.29737\n",
      "------------------------------------------\n",
      "L1 loss: 0.0647984966635704\n",
      "Training batch 85 with loss 0.28309\n",
      "------------------------------------------\n",
      "L1 loss: 0.08137494325637817\n",
      "Training batch 86 with loss 0.28107\n",
      "------------------------------------------\n",
      "L1 loss: 0.10193481296300888\n",
      "Training batch 87 with loss 0.39958\n",
      "------------------------------------------\n",
      "L1 loss: 0.0760752260684967\n",
      "Training batch 88 with loss 0.33457\n",
      "------------------------------------------\n",
      "L1 loss: 0.07126154005527496\n",
      "Training batch 89 with loss 0.26760\n",
      "------------------------------------------\n",
      "L1 loss: 0.0721559152007103\n",
      "Training batch 90 with loss 0.27902\n",
      "------------------------------------------\n",
      "L1 loss: 0.05803403630852699\n",
      "Training batch 91 with loss 0.24752\n",
      "------------------------------------------\n",
      "L1 loss: 0.09222453087568283\n",
      "Training batch 92 with loss 0.28672\n",
      "------------------------------------------\n",
      "L1 loss: 0.06866247951984406\n",
      "Training batch 93 with loss 0.25869\n",
      "------------------------------------------\n",
      "L1 loss: 0.08315882831811905\n",
      "Training batch 94 with loss 0.29307\n",
      "------------------------------------------\n",
      "L1 loss: 0.07290787249803543\n",
      "Training batch 95 with loss 0.26065\n",
      "------------------------------------------\n",
      "L1 loss: 0.07152224332094193\n",
      "Training batch 96 with loss 0.29510\n",
      "------------------------------------------\n",
      "L1 loss: 0.08625339716672897\n",
      "Training batch 97 with loss 0.35260\n",
      "------------------------------------------\n",
      "L1 loss: 0.05558821186423302\n",
      "Training batch 98 with loss 0.23614\n",
      "------------------------------------------\n",
      "L1 loss: 0.06216583400964737\n",
      "Training batch 99 with loss 0.26123\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2922338643670082\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4858\n",
      "------------------------------------------\n",
      "L1 loss: 0.06124226376414299\n",
      "Training batch 0 with loss 0.25177\n",
      "------------------------------------------\n",
      "L1 loss: 0.08506488800048828\n",
      "Training batch 1 with loss 0.29737\n",
      "------------------------------------------\n",
      "L1 loss: 0.09309330582618713\n",
      "Training batch 2 with loss 0.32346\n",
      "------------------------------------------\n",
      "L1 loss: 0.0754421204328537\n",
      "Training batch 3 with loss 0.27040\n",
      "------------------------------------------\n",
      "L1 loss: 0.08370041102170944\n",
      "Training batch 4 with loss 0.32999\n",
      "------------------------------------------\n",
      "L1 loss: 0.06382862478494644\n",
      "Training batch 5 with loss 0.31678\n",
      "------------------------------------------\n",
      "L1 loss: 0.079585500061512\n",
      "Training batch 6 with loss 0.26695\n",
      "------------------------------------------\n",
      "L1 loss: 0.0832643136382103\n",
      "Training batch 7 with loss 0.34014\n",
      "------------------------------------------\n",
      "L1 loss: 0.08312389999628067\n",
      "Training batch 8 with loss 0.33541\n",
      "------------------------------------------\n",
      "L1 loss: 0.05700799450278282\n",
      "Training batch 9 with loss 0.24775\n",
      "------------------------------------------\n",
      "L1 loss: 0.08381757885217667\n",
      "Training batch 10 with loss 0.30710\n",
      "------------------------------------------\n",
      "L1 loss: 0.05875718221068382\n",
      "Training batch 11 with loss 0.28201\n",
      "------------------------------------------\n",
      "L1 loss: 0.0745074599981308\n",
      "Training batch 12 with loss 0.59934\n",
      "------------------------------------------\n",
      "L1 loss: 0.07045109570026398\n",
      "Training batch 13 with loss 0.23988\n",
      "------------------------------------------\n",
      "L1 loss: 0.053299613296985626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 14 with loss 0.25746\n",
      "------------------------------------------\n",
      "L1 loss: 0.08353161066770554\n",
      "Training batch 15 with loss 0.26496\n",
      "------------------------------------------\n",
      "L1 loss: 0.06123412027955055\n",
      "Training batch 16 with loss 0.27867\n",
      "------------------------------------------\n",
      "L1 loss: 0.08191627264022827\n",
      "Training batch 17 with loss 0.32583\n",
      "------------------------------------------\n",
      "L1 loss: 0.0805896669626236\n",
      "Training batch 18 with loss 0.29177\n",
      "------------------------------------------\n",
      "L1 loss: 0.06105940043926239\n",
      "Training batch 19 with loss 0.22873\n",
      "------------------------------------------\n",
      "L1 loss: 0.07339852303266525\n",
      "Training batch 20 with loss 0.31760\n",
      "------------------------------------------\n",
      "L1 loss: 0.0684930756688118\n",
      "Training batch 21 with loss 0.28421\n",
      "------------------------------------------\n",
      "L1 loss: 0.06670016795396805\n",
      "Training batch 22 with loss 0.30356\n",
      "------------------------------------------\n",
      "L1 loss: 0.08379945904016495\n",
      "Training batch 23 with loss 0.36404\n",
      "------------------------------------------\n",
      "L1 loss: 0.07005377858877182\n",
      "Training batch 24 with loss 0.30060\n",
      "------------------------------------------\n",
      "L1 loss: 0.07444242388010025\n",
      "Training batch 25 with loss 0.28473\n",
      "------------------------------------------\n",
      "L1 loss: 0.07271357625722885\n",
      "Training batch 26 with loss 0.26832\n",
      "------------------------------------------\n",
      "L1 loss: 0.04935605823993683\n",
      "Training batch 27 with loss 0.32181\n",
      "------------------------------------------\n",
      "L1 loss: 0.057805150747299194\n",
      "Training batch 28 with loss 0.28910\n",
      "------------------------------------------\n",
      "L1 loss: 0.06268088519573212\n",
      "Training batch 29 with loss 0.27017\n",
      "------------------------------------------\n",
      "L1 loss: 0.06179351359605789\n",
      "Training batch 30 with loss 0.26920\n",
      "------------------------------------------\n",
      "L1 loss: 0.055023740977048874\n",
      "Training batch 31 with loss 0.37011\n",
      "------------------------------------------\n",
      "L1 loss: 0.07040461897850037\n",
      "Training batch 32 with loss 0.28521\n",
      "------------------------------------------\n",
      "L1 loss: 0.06541615724563599\n",
      "Training batch 33 with loss 0.27139\n",
      "------------------------------------------\n",
      "L1 loss: 0.07479362189769745\n",
      "Training batch 34 with loss 0.32887\n",
      "------------------------------------------\n",
      "L1 loss: 0.08457227051258087\n",
      "Training batch 35 with loss 0.38251\n",
      "------------------------------------------\n",
      "L1 loss: 0.08364970982074738\n",
      "Training batch 36 with loss 0.28139\n",
      "------------------------------------------\n",
      "L1 loss: 0.06765297055244446\n",
      "Training batch 37 with loss 0.25651\n",
      "------------------------------------------\n",
      "L1 loss: 0.05856122076511383\n",
      "Training batch 38 with loss 0.39035\n",
      "------------------------------------------\n",
      "L1 loss: 0.06750466674566269\n",
      "Training batch 39 with loss 0.30046\n",
      "------------------------------------------\n",
      "L1 loss: 0.06829444319009781\n",
      "Training batch 40 with loss 0.25355\n",
      "------------------------------------------\n",
      "L1 loss: 0.05444653332233429\n",
      "Training batch 41 with loss 0.44758\n",
      "------------------------------------------\n",
      "L1 loss: 0.07950051128864288\n",
      "Training batch 42 with loss 0.28738\n",
      "------------------------------------------\n",
      "L1 loss: 0.06955019384622574\n",
      "Training batch 43 with loss 0.26918\n",
      "------------------------------------------\n",
      "L1 loss: 0.05586416646838188\n",
      "Training batch 44 with loss 0.27980\n",
      "------------------------------------------\n",
      "L1 loss: 0.09349118173122406\n",
      "Training batch 45 with loss 0.36548\n",
      "------------------------------------------\n",
      "L1 loss: 0.07084391266107559\n",
      "Training batch 46 with loss 0.27085\n",
      "------------------------------------------\n",
      "L1 loss: 0.10846074670553207\n",
      "Training batch 47 with loss 0.37973\n",
      "------------------------------------------\n",
      "L1 loss: 0.04395081475377083\n",
      "Training batch 48 with loss 0.23276\n",
      "------------------------------------------\n",
      "L1 loss: 0.08386274427175522\n",
      "Training batch 49 with loss 0.37127\n",
      "------------------------------------------\n",
      "L1 loss: 0.07469286024570465\n",
      "Training batch 50 with loss 0.31349\n",
      "------------------------------------------\n",
      "L1 loss: 0.08201943337917328\n",
      "Training batch 51 with loss 0.34852\n",
      "------------------------------------------\n",
      "L1 loss: 0.09065106511116028\n",
      "Training batch 52 with loss 0.29349\n",
      "------------------------------------------\n",
      "L1 loss: 0.07894325256347656\n",
      "Training batch 53 with loss 0.28248\n",
      "------------------------------------------\n",
      "L1 loss: 0.07670290768146515\n",
      "Training batch 54 with loss 0.26278\n",
      "------------------------------------------\n",
      "L1 loss: 0.0627712532877922\n",
      "Training batch 55 with loss 0.29756\n",
      "------------------------------------------\n",
      "L1 loss: 0.05067956820130348\n",
      "Training batch 56 with loss 0.23904\n",
      "------------------------------------------\n",
      "L1 loss: 0.07047489285469055\n",
      "Training batch 57 with loss 0.27393\n",
      "------------------------------------------\n",
      "L1 loss: 0.06511608511209488\n",
      "Training batch 58 with loss 0.26572\n",
      "------------------------------------------\n",
      "L1 loss: 0.07424627989530563\n",
      "Training batch 59 with loss 0.59754\n",
      "------------------------------------------\n",
      "L1 loss: 0.06550897657871246\n",
      "Training batch 60 with loss 0.27796\n",
      "------------------------------------------\n",
      "L1 loss: 0.09439390152692795\n",
      "Training batch 61 with loss 0.33684\n",
      "------------------------------------------\n",
      "L1 loss: 0.057771604508161545\n",
      "Training batch 62 with loss 0.25684\n",
      "------------------------------------------\n",
      "L1 loss: 0.0636499896645546\n",
      "Training batch 63 with loss 0.26670\n",
      "------------------------------------------\n",
      "L1 loss: 0.07035072147846222\n",
      "Training batch 64 with loss 0.27573\n",
      "------------------------------------------\n",
      "L1 loss: 0.06129344180226326\n",
      "Training batch 65 with loss 0.22046\n",
      "------------------------------------------\n",
      "L1 loss: 0.06630664318799973\n",
      "Training batch 66 with loss 0.26845\n",
      "------------------------------------------\n",
      "L1 loss: 0.06475698202848434\n",
      "Training batch 67 with loss 0.26140\n",
      "------------------------------------------\n",
      "L1 loss: 0.07477172464132309\n",
      "Training batch 68 with loss 0.29762\n",
      "------------------------------------------\n",
      "L1 loss: 0.05178593844175339\n",
      "Training batch 69 with loss 0.27302\n",
      "------------------------------------------\n",
      "L1 loss: 0.07715661823749542\n",
      "Training batch 70 with loss 0.29845\n",
      "------------------------------------------\n",
      "L1 loss: 0.07142888754606247\n",
      "Training batch 71 with loss 0.29596\n",
      "------------------------------------------\n",
      "L1 loss: 0.08631177991628647\n",
      "Training batch 72 with loss 0.31841\n",
      "------------------------------------------\n",
      "L1 loss: 0.0700022354722023\n",
      "Training batch 73 with loss 0.29053\n",
      "------------------------------------------\n",
      "L1 loss: 0.09441980719566345\n",
      "Training batch 74 with loss 0.30578\n",
      "------------------------------------------\n",
      "L1 loss: 0.07714984565973282\n",
      "Training batch 75 with loss 0.28296\n",
      "------------------------------------------\n",
      "L1 loss: 0.0866113230586052\n",
      "Training batch 76 with loss 0.29790\n",
      "------------------------------------------\n",
      "L1 loss: 0.07807443290948868\n",
      "Training batch 77 with loss 0.27820\n",
      "------------------------------------------\n",
      "L1 loss: 0.08102616667747498\n",
      "Training batch 78 with loss 0.30901\n",
      "------------------------------------------\n",
      "L1 loss: 0.10936331003904343\n",
      "Training batch 79 with loss 0.36178\n",
      "------------------------------------------\n",
      "L1 loss: 0.07661296427249908\n",
      "Training batch 80 with loss 0.28790\n",
      "------------------------------------------\n",
      "L1 loss: 0.0734357237815857\n",
      "Training batch 81 with loss 0.27861\n",
      "------------------------------------------\n",
      "L1 loss: 0.08398132771253586\n",
      "Training batch 82 with loss 0.36022\n",
      "------------------------------------------\n",
      "L1 loss: 0.07758670300245285\n",
      "Training batch 83 with loss 0.27919\n",
      "------------------------------------------\n",
      "L1 loss: 0.06869146972894669\n",
      "Training batch 84 with loss 0.29437\n",
      "------------------------------------------\n",
      "L1 loss: 0.06814651936292648\n",
      "Training batch 85 with loss 0.29250\n",
      "------------------------------------------\n",
      "L1 loss: 0.07675116509199142\n",
      "Training batch 86 with loss 0.29346\n",
      "------------------------------------------\n",
      "L1 loss: 0.10091794282197952\n",
      "Training batch 87 with loss 0.39078\n",
      "------------------------------------------\n",
      "L1 loss: 0.0761473998427391\n",
      "Training batch 88 with loss 0.31516\n",
      "------------------------------------------\n",
      "L1 loss: 0.06982321292161942\n",
      "Training batch 89 with loss 0.27833\n",
      "------------------------------------------\n",
      "L1 loss: 0.07362391799688339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 90 with loss 0.27594\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394719332456589\n",
      "Training batch 91 with loss 0.26346\n",
      "------------------------------------------\n",
      "L1 loss: 0.09636641293764114\n",
      "Training batch 92 with loss 0.32152\n",
      "------------------------------------------\n",
      "L1 loss: 0.06618174910545349\n",
      "Training batch 93 with loss 0.24650\n",
      "------------------------------------------\n",
      "L1 loss: 0.08644983917474747\n",
      "Training batch 94 with loss 0.30953\n",
      "------------------------------------------\n",
      "L1 loss: 0.07234101742506027\n",
      "Training batch 95 with loss 0.25594\n",
      "------------------------------------------\n",
      "L1 loss: 0.06328287720680237\n",
      "Training batch 96 with loss 0.28186\n",
      "------------------------------------------\n",
      "L1 loss: 0.08374922722578049\n",
      "Training batch 97 with loss 0.35405\n",
      "------------------------------------------\n",
      "L1 loss: 0.05789489299058914\n",
      "Training batch 98 with loss 0.24595\n",
      "------------------------------------------\n",
      "L1 loss: 0.06444268673658371\n",
      "Training batch 99 with loss 0.28135\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.302086773365736\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4859\n",
      "------------------------------------------\n",
      "L1 loss: 0.06283901631832123\n",
      "Training batch 0 with loss 0.24282\n",
      "------------------------------------------\n",
      "L1 loss: 0.08130273222923279\n",
      "Training batch 1 with loss 0.28655\n",
      "------------------------------------------\n",
      "L1 loss: 0.09240442514419556\n",
      "Training batch 2 with loss 0.32364\n",
      "------------------------------------------\n",
      "L1 loss: 0.07717353105545044\n",
      "Training batch 3 with loss 0.26054\n",
      "------------------------------------------\n",
      "L1 loss: 0.0824902132153511\n",
      "Training batch 4 with loss 0.32004\n",
      "------------------------------------------\n",
      "L1 loss: 0.06806458532810211\n",
      "Training batch 5 with loss 0.32528\n",
      "------------------------------------------\n",
      "L1 loss: 0.07850261777639389\n",
      "Training batch 6 with loss 0.26195\n",
      "------------------------------------------\n",
      "L1 loss: 0.08238535374403\n",
      "Training batch 7 with loss 0.33883\n",
      "------------------------------------------\n",
      "L1 loss: 0.08084598928689957\n",
      "Training batch 8 with loss 0.30644\n",
      "------------------------------------------\n",
      "L1 loss: 0.06341928243637085\n",
      "Training batch 9 with loss 0.24819\n",
      "------------------------------------------\n",
      "L1 loss: 0.08054421097040176\n",
      "Training batch 10 with loss 0.30419\n",
      "------------------------------------------\n",
      "L1 loss: 0.058894410729408264\n",
      "Training batch 11 with loss 0.29466\n",
      "------------------------------------------\n",
      "L1 loss: 0.07678905874490738\n",
      "Training batch 12 with loss 0.31510\n",
      "------------------------------------------\n",
      "L1 loss: 0.0679003894329071\n",
      "Training batch 13 with loss 0.23661\n",
      "------------------------------------------\n",
      "L1 loss: 0.052103616297245026\n",
      "Training batch 14 with loss 0.25304\n",
      "------------------------------------------\n",
      "L1 loss: 0.08345109224319458\n",
      "Training batch 15 with loss 0.27228\n",
      "------------------------------------------\n",
      "L1 loss: 0.06412447988986969\n",
      "Training batch 16 with loss 0.27856\n",
      "------------------------------------------\n",
      "L1 loss: 0.08109649270772934\n",
      "Training batch 17 with loss 0.33100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08104119449853897\n",
      "Training batch 18 with loss 0.30696\n",
      "------------------------------------------\n",
      "L1 loss: 0.06164727360010147\n",
      "Training batch 19 with loss 0.24233\n",
      "------------------------------------------\n",
      "L1 loss: 0.07240208983421326\n",
      "Training batch 20 with loss 0.33894\n",
      "------------------------------------------\n",
      "L1 loss: 0.06730271875858307\n",
      "Training batch 21 with loss 0.36141\n",
      "------------------------------------------\n",
      "L1 loss: 0.06930290907621384\n",
      "Training batch 22 with loss 0.29527\n",
      "------------------------------------------\n",
      "L1 loss: 0.08803518116474152\n",
      "Training batch 23 with loss 0.35818\n",
      "------------------------------------------\n",
      "L1 loss: 0.06705199927091599\n",
      "Training batch 24 with loss 0.30424\n",
      "------------------------------------------\n",
      "L1 loss: 0.07466326653957367\n",
      "Training batch 25 with loss 0.29153\n",
      "------------------------------------------\n",
      "L1 loss: 0.0717277079820633\n",
      "Training batch 26 with loss 0.26741\n",
      "------------------------------------------\n",
      "L1 loss: 0.05510217323899269\n",
      "Training batch 27 with loss 0.30062\n",
      "------------------------------------------\n",
      "L1 loss: 0.05732659995555878\n",
      "Training batch 28 with loss 0.29528\n",
      "------------------------------------------\n",
      "L1 loss: 0.06055902689695358\n",
      "Training batch 29 with loss 0.24544\n",
      "------------------------------------------\n",
      "L1 loss: 0.061094239354133606\n",
      "Training batch 30 with loss 0.25580\n",
      "------------------------------------------\n",
      "L1 loss: 0.0672164037823677\n",
      "Training batch 31 with loss 0.31410\n",
      "------------------------------------------\n",
      "L1 loss: 0.07006923109292984\n",
      "Training batch 32 with loss 0.28003\n",
      "------------------------------------------\n",
      "L1 loss: 0.06574085354804993\n",
      "Training batch 33 with loss 0.26759\n",
      "------------------------------------------\n",
      "L1 loss: 0.07337220013141632\n",
      "Training batch 34 with loss 0.31616\n",
      "------------------------------------------\n",
      "L1 loss: 0.09357041865587234\n",
      "Training batch 35 with loss 0.31416\n",
      "------------------------------------------\n",
      "L1 loss: 0.08518257737159729\n",
      "Training batch 36 with loss 0.28291\n",
      "------------------------------------------\n",
      "L1 loss: 0.0678677186369896\n",
      "Training batch 37 with loss 0.25906\n",
      "------------------------------------------\n",
      "L1 loss: 0.06060829013586044\n",
      "Training batch 38 with loss 0.21849\n",
      "------------------------------------------\n",
      "L1 loss: 0.06589338928461075\n",
      "Training batch 39 with loss 0.30400\n",
      "------------------------------------------\n",
      "L1 loss: 0.06978153437376022\n",
      "Training batch 40 with loss 0.24108\n",
      "------------------------------------------\n",
      "L1 loss: 0.05703873932361603\n",
      "Training batch 41 with loss 0.28785\n",
      "------------------------------------------\n",
      "L1 loss: 0.07754112035036087\n",
      "Training batch 42 with loss 0.27861\n",
      "------------------------------------------\n",
      "L1 loss: 0.07079969346523285\n",
      "Training batch 43 with loss 0.28445\n",
      "------------------------------------------\n",
      "L1 loss: 0.05703047662973404\n",
      "Training batch 44 with loss 0.25965\n",
      "------------------------------------------\n",
      "L1 loss: 0.09212492406368256\n",
      "Training batch 45 with loss 0.36306\n",
      "------------------------------------------\n",
      "L1 loss: 0.0761091336607933\n",
      "Training batch 46 with loss 0.26419\n",
      "------------------------------------------\n",
      "L1 loss: 0.11158197373151779\n",
      "Training batch 47 with loss 0.38535\n",
      "------------------------------------------\n",
      "L1 loss: 0.04258207976818085\n",
      "Training batch 48 with loss 0.23187\n",
      "------------------------------------------\n",
      "L1 loss: 0.0819411650300026\n",
      "Training batch 49 with loss 0.35370\n",
      "------------------------------------------\n",
      "L1 loss: 0.07283101230859756\n",
      "Training batch 50 with loss 0.28211\n",
      "------------------------------------------\n",
      "L1 loss: 0.07628139853477478\n",
      "Training batch 51 with loss 0.31462\n",
      "------------------------------------------\n",
      "L1 loss: 0.09002235531806946\n",
      "Training batch 52 with loss 0.28672\n",
      "------------------------------------------\n",
      "L1 loss: 0.08124245703220367\n",
      "Training batch 53 with loss 0.29572\n",
      "------------------------------------------\n",
      "L1 loss: 0.07042387127876282\n",
      "Training batch 54 with loss 0.26459\n",
      "------------------------------------------\n",
      "L1 loss: 0.061568811535835266\n",
      "Training batch 55 with loss 0.29491\n",
      "------------------------------------------\n",
      "L1 loss: 0.051928941160440445\n",
      "Training batch 56 with loss 0.24539\n",
      "------------------------------------------\n",
      "L1 loss: 0.06785048544406891\n",
      "Training batch 57 with loss 0.28080\n",
      "------------------------------------------\n",
      "L1 loss: 0.06492175906896591\n",
      "Training batch 58 with loss 0.24835\n",
      "------------------------------------------\n",
      "L1 loss: 0.08009327948093414\n",
      "Training batch 59 with loss 0.30873\n",
      "------------------------------------------\n",
      "L1 loss: 0.06649833172559738\n",
      "Training batch 60 with loss 0.28008\n",
      "------------------------------------------\n",
      "L1 loss: 0.10540083795785904\n",
      "Training batch 61 with loss 0.38734\n",
      "------------------------------------------\n",
      "L1 loss: 0.0555349625647068\n",
      "Training batch 62 with loss 0.25655\n",
      "------------------------------------------\n",
      "L1 loss: 0.046210166066884995\n",
      "Training batch 63 with loss 0.19162\n",
      "------------------------------------------\n",
      "L1 loss: 0.06991167366504669\n",
      "Training batch 64 with loss 0.28836\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06173178181052208\n",
      "Training batch 65 with loss 0.24118\n",
      "------------------------------------------\n",
      "L1 loss: 0.06525526195764542\n",
      "Training batch 66 with loss 0.27353\n",
      "------------------------------------------\n",
      "L1 loss: 0.06389997899532318\n",
      "Training batch 67 with loss 0.27094\n",
      "------------------------------------------\n",
      "L1 loss: 0.0739414319396019\n",
      "Training batch 68 with loss 0.29943\n",
      "------------------------------------------\n",
      "L1 loss: 0.051285747438669205\n",
      "Training batch 69 with loss 0.27392\n",
      "------------------------------------------\n",
      "L1 loss: 0.07809535413980484\n",
      "Training batch 70 with loss 0.29567\n",
      "------------------------------------------\n",
      "L1 loss: 0.06972930580377579\n",
      "Training batch 71 with loss 0.32370\n",
      "------------------------------------------\n",
      "L1 loss: 0.08302421122789383\n",
      "Training batch 72 with loss 0.30201\n",
      "------------------------------------------\n",
      "L1 loss: 0.07339828461408615\n",
      "Training batch 73 with loss 0.29874\n",
      "------------------------------------------\n",
      "L1 loss: 0.09609625488519669\n",
      "Training batch 74 with loss 0.31248\n",
      "------------------------------------------\n",
      "L1 loss: 0.0743556097149849\n",
      "Training batch 75 with loss 0.29051\n",
      "------------------------------------------\n",
      "L1 loss: 0.08684534579515457\n",
      "Training batch 76 with loss 0.30469\n",
      "------------------------------------------\n",
      "L1 loss: 0.08062759041786194\n",
      "Training batch 77 with loss 0.28200\n",
      "------------------------------------------\n",
      "L1 loss: 0.0785725936293602\n",
      "Training batch 78 with loss 0.30564\n",
      "------------------------------------------\n",
      "L1 loss: 0.10730131715536118\n",
      "Training batch 79 with loss 0.36439\n",
      "------------------------------------------\n",
      "L1 loss: 0.07601884752511978\n",
      "Training batch 80 with loss 0.28687\n",
      "------------------------------------------\n",
      "L1 loss: 0.07166218012571335\n",
      "Training batch 81 with loss 0.28488\n",
      "------------------------------------------\n",
      "L1 loss: 0.08437294512987137\n",
      "Training batch 82 with loss 0.34556\n",
      "------------------------------------------\n",
      "L1 loss: 0.0783056989312172\n",
      "Training batch 83 with loss 0.27569\n",
      "------------------------------------------\n",
      "L1 loss: 0.06970375776290894\n",
      "Training batch 84 with loss 0.61972\n",
      "------------------------------------------\n",
      "L1 loss: 0.062474362552165985\n",
      "Training batch 85 with loss 0.27300\n",
      "------------------------------------------\n",
      "L1 loss: 0.07624753564596176\n",
      "Training batch 86 with loss 0.28202\n",
      "------------------------------------------\n",
      "L1 loss: 0.10051313787698746\n",
      "Training batch 87 with loss 0.38688\n",
      "------------------------------------------\n",
      "L1 loss: 0.07922554761171341\n",
      "Training batch 88 with loss 0.32457\n",
      "------------------------------------------\n",
      "L1 loss: 0.0675201341509819\n",
      "Training batch 89 with loss 0.26736\n",
      "------------------------------------------\n",
      "L1 loss: 0.07365012168884277\n",
      "Training batch 90 with loss 0.27042\n",
      "------------------------------------------\n",
      "L1 loss: 0.0653359517455101\n",
      "Training batch 91 with loss 0.26470\n",
      "------------------------------------------\n",
      "L1 loss: 0.09100432693958282\n",
      "Training batch 92 with loss 0.30090\n",
      "------------------------------------------\n",
      "L1 loss: 0.06675988435745239\n",
      "Training batch 93 with loss 0.24829\n",
      "------------------------------------------\n",
      "L1 loss: 0.08456005156040192\n",
      "Training batch 94 with loss 0.31197\n",
      "------------------------------------------\n",
      "L1 loss: 0.06919723749160767\n",
      "Training batch 95 with loss 0.24611\n",
      "------------------------------------------\n",
      "L1 loss: 0.06645829975605011\n",
      "Training batch 96 with loss 0.28076\n",
      "------------------------------------------\n",
      "L1 loss: 0.08163801580667496\n",
      "Training batch 97 with loss 0.35269\n",
      "------------------------------------------\n",
      "L1 loss: 0.05543956905603409\n",
      "Training batch 98 with loss 0.25354\n",
      "------------------------------------------\n",
      "L1 loss: 0.06085223704576492\n",
      "Training batch 99 with loss 0.24912\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2935920161008835\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4860\n",
      "------------------------------------------\n",
      "L1 loss: 0.06495233625173569\n",
      "Training batch 0 with loss 0.25906\n",
      "------------------------------------------\n",
      "L1 loss: 0.08600457012653351\n",
      "Training batch 1 with loss 0.29167\n",
      "------------------------------------------\n",
      "L1 loss: 0.0941348522901535\n",
      "Training batch 2 with loss 0.32927\n",
      "------------------------------------------\n",
      "L1 loss: 0.07199481874704361\n",
      "Training batch 3 with loss 0.26484\n",
      "------------------------------------------\n",
      "L1 loss: 0.08329152315855026\n",
      "Training batch 4 with loss 0.31562\n",
      "------------------------------------------\n",
      "L1 loss: 0.06807369738817215\n",
      "Training batch 5 with loss 0.34319\n",
      "------------------------------------------\n",
      "L1 loss: 0.08069990575313568\n",
      "Training batch 6 with loss 0.27234\n",
      "------------------------------------------\n",
      "L1 loss: 0.08364416658878326\n",
      "Training batch 7 with loss 0.32172\n",
      "------------------------------------------\n",
      "L1 loss: 0.08292288333177567\n",
      "Training batch 8 with loss 0.33891\n",
      "------------------------------------------\n",
      "L1 loss: 0.05752186477184296\n",
      "Training batch 9 with loss 0.24421\n",
      "------------------------------------------\n",
      "L1 loss: 0.08100785315036774\n",
      "Training batch 10 with loss 0.29011\n",
      "------------------------------------------\n",
      "L1 loss: 0.0643080323934555\n",
      "Training batch 11 with loss 0.31220\n",
      "------------------------------------------\n",
      "L1 loss: 0.07464832812547684\n",
      "Training batch 12 with loss 0.30450\n",
      "------------------------------------------\n",
      "L1 loss: 0.0682547390460968\n",
      "Training batch 13 with loss 0.22770\n",
      "------------------------------------------\n",
      "L1 loss: 0.05149717256426811\n",
      "Training batch 14 with loss 0.23410\n",
      "------------------------------------------\n",
      "L1 loss: 0.0827854722738266\n",
      "Training batch 15 with loss 0.27766\n",
      "------------------------------------------\n",
      "L1 loss: 0.06370442360639572\n",
      "Training batch 16 with loss 0.26764\n",
      "------------------------------------------\n",
      "L1 loss: 0.08294864743947983\n",
      "Training batch 17 with loss 0.32963\n",
      "------------------------------------------\n",
      "L1 loss: 0.08063521236181259\n",
      "Training batch 18 with loss 0.30926\n",
      "------------------------------------------\n",
      "L1 loss: 0.06179461628198624\n",
      "Training batch 19 with loss 0.24330\n",
      "------------------------------------------\n",
      "L1 loss: 0.0717899352312088\n",
      "Training batch 20 with loss 0.32485\n",
      "------------------------------------------\n",
      "L1 loss: 0.07117334008216858\n",
      "Training batch 21 with loss 0.28010\n",
      "------------------------------------------\n",
      "L1 loss: 0.06941092759370804\n",
      "Training batch 22 with loss 0.28041\n",
      "------------------------------------------\n",
      "L1 loss: 0.08227591216564178\n",
      "Training batch 23 with loss 0.36947\n",
      "------------------------------------------\n",
      "L1 loss: 0.06783146411180496\n",
      "Training batch 24 with loss 0.28968\n",
      "------------------------------------------\n",
      "L1 loss: 0.07298886030912399\n",
      "Training batch 25 with loss 0.30269\n",
      "------------------------------------------\n",
      "L1 loss: 0.07655405253171921\n",
      "Training batch 26 with loss 0.27764\n",
      "------------------------------------------\n",
      "L1 loss: 0.05613589659333229\n",
      "Training batch 27 with loss 0.28450\n",
      "------------------------------------------\n",
      "L1 loss: 0.05733618140220642\n",
      "Training batch 28 with loss 0.29064\n",
      "------------------------------------------\n",
      "L1 loss: 0.059642236679792404\n",
      "Training batch 29 with loss 0.24934\n",
      "------------------------------------------\n",
      "L1 loss: 0.06063031405210495\n",
      "Training batch 30 with loss 0.26516\n",
      "------------------------------------------\n",
      "L1 loss: 0.06384258717298508\n",
      "Training batch 31 with loss 0.30169\n",
      "------------------------------------------\n",
      "L1 loss: 0.06811399012804031\n",
      "Training batch 32 with loss 0.27434\n",
      "------------------------------------------\n",
      "L1 loss: 0.06099323183298111\n",
      "Training batch 33 with loss 0.25743\n",
      "------------------------------------------\n",
      "L1 loss: 0.078792043030262\n",
      "Training batch 34 with loss 0.33886\n",
      "------------------------------------------\n",
      "L1 loss: 0.09506845474243164\n",
      "Training batch 35 with loss 0.34600\n",
      "------------------------------------------\n",
      "L1 loss: 0.08828158676624298\n",
      "Training batch 36 with loss 0.29530\n",
      "------------------------------------------\n",
      "L1 loss: 0.06714067608118057\n",
      "Training batch 37 with loss 0.25213\n",
      "------------------------------------------\n",
      "L1 loss: 0.06383563578128815\n",
      "Training batch 38 with loss 0.21943\n",
      "------------------------------------------\n",
      "L1 loss: 0.06560464203357697\n",
      "Training batch 39 with loss 0.29552\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06744968891143799\n",
      "Training batch 40 with loss 0.25188\n",
      "------------------------------------------\n",
      "L1 loss: 0.06049242243170738\n",
      "Training batch 41 with loss 0.28940\n",
      "------------------------------------------\n",
      "L1 loss: 0.07910219579935074\n",
      "Training batch 42 with loss 0.28244\n",
      "------------------------------------------\n",
      "L1 loss: 0.0537087619304657\n",
      "Training batch 43 with loss 0.22271\n",
      "------------------------------------------\n",
      "L1 loss: 0.05681171640753746\n",
      "Training batch 44 with loss 0.27215\n",
      "------------------------------------------\n",
      "L1 loss: 0.09005977213382721\n",
      "Training batch 45 with loss 0.37786\n",
      "------------------------------------------\n",
      "L1 loss: 0.076352059841156\n",
      "Training batch 46 with loss 0.26265\n",
      "------------------------------------------\n",
      "L1 loss: 0.11204212158918381\n",
      "Training batch 47 with loss 0.38005\n",
      "------------------------------------------\n",
      "L1 loss: 0.049885343760252\n",
      "Training batch 48 with loss 0.25580\n",
      "------------------------------------------\n",
      "L1 loss: 0.08077911287546158\n",
      "Training batch 49 with loss 0.35173\n",
      "------------------------------------------\n",
      "L1 loss: 0.07483214139938354\n",
      "Training batch 50 with loss 0.29026\n",
      "------------------------------------------\n",
      "L1 loss: 0.08213622868061066\n",
      "Training batch 51 with loss 0.33240\n",
      "------------------------------------------\n",
      "L1 loss: 0.08937803655862808\n",
      "Training batch 52 with loss 0.28867\n",
      "------------------------------------------\n",
      "L1 loss: 0.08184849470853806\n",
      "Training batch 53 with loss 0.30315\n",
      "------------------------------------------\n",
      "L1 loss: 0.07649402320384979\n",
      "Training batch 54 with loss 0.27116\n",
      "------------------------------------------\n",
      "L1 loss: 0.059120725840330124\n",
      "Training batch 55 with loss 0.31379\n",
      "------------------------------------------\n",
      "L1 loss: 0.05065174400806427\n",
      "Training batch 56 with loss 0.23751\n",
      "------------------------------------------\n",
      "L1 loss: 0.06573370844125748\n",
      "Training batch 57 with loss 0.28118\n",
      "------------------------------------------\n",
      "L1 loss: 0.0674557089805603\n",
      "Training batch 58 with loss 0.26853\n",
      "------------------------------------------\n",
      "L1 loss: 0.08282004296779633\n",
      "Training batch 59 with loss 0.29456\n",
      "------------------------------------------\n",
      "L1 loss: 0.06233706697821617\n",
      "Training batch 60 with loss 0.29302\n",
      "------------------------------------------\n",
      "L1 loss: 0.09840408712625504\n",
      "Training batch 61 with loss 0.34404\n",
      "------------------------------------------\n",
      "L1 loss: 0.05534937605261803\n",
      "Training batch 62 with loss 0.25336\n",
      "------------------------------------------\n",
      "L1 loss: 0.061763931065797806\n",
      "Training batch 63 with loss 0.24634\n",
      "------------------------------------------\n",
      "L1 loss: 0.07139257341623306\n",
      "Training batch 64 with loss 0.28577\n",
      "------------------------------------------\n",
      "L1 loss: 0.05797518044710159\n",
      "Training batch 65 with loss 0.22678\n",
      "------------------------------------------\n",
      "L1 loss: 0.06564892828464508\n",
      "Training batch 66 with loss 0.26783\n",
      "------------------------------------------\n",
      "L1 loss: 0.06429872661828995\n",
      "Training batch 67 with loss 0.25948\n",
      "------------------------------------------\n",
      "L1 loss: 0.07195218652486801\n",
      "Training batch 68 with loss 0.29881\n",
      "------------------------------------------\n",
      "L1 loss: 0.052382610738277435\n",
      "Training batch 69 with loss 0.27069\n",
      "------------------------------------------\n",
      "L1 loss: 0.07985653728246689\n",
      "Training batch 70 with loss 0.29229\n",
      "------------------------------------------\n",
      "L1 loss: 0.072701595723629\n",
      "Training batch 71 with loss 0.30798\n",
      "------------------------------------------\n",
      "L1 loss: 0.08794594556093216\n",
      "Training batch 72 with loss 0.32522\n",
      "------------------------------------------\n",
      "L1 loss: 0.07313491404056549\n",
      "Training batch 73 with loss 0.30336\n",
      "------------------------------------------\n",
      "L1 loss: 0.09674733132123947\n",
      "Training batch 74 with loss 0.31161\n",
      "------------------------------------------\n",
      "L1 loss: 0.07406039535999298\n",
      "Training batch 75 with loss 0.29149\n",
      "------------------------------------------\n",
      "L1 loss: 0.08696828782558441\n",
      "Training batch 76 with loss 0.29042\n",
      "------------------------------------------\n",
      "L1 loss: 0.07901483029127121\n",
      "Training batch 77 with loss 0.27400\n",
      "------------------------------------------\n",
      "L1 loss: 0.08364211767911911\n",
      "Training batch 78 with loss 0.30035\n",
      "------------------------------------------\n",
      "L1 loss: 0.10680241137742996\n",
      "Training batch 79 with loss 0.37400\n",
      "------------------------------------------\n",
      "L1 loss: 0.0757182240486145\n",
      "Training batch 80 with loss 0.28190\n",
      "------------------------------------------\n",
      "L1 loss: 0.08179657906293869\n",
      "Training batch 81 with loss 0.29667\n",
      "------------------------------------------\n",
      "L1 loss: 0.0821356251835823\n",
      "Training batch 82 with loss 0.34829\n",
      "------------------------------------------\n",
      "L1 loss: 0.07815703004598618\n",
      "Training batch 83 with loss 0.27867\n",
      "------------------------------------------\n",
      "L1 loss: 0.07319706678390503\n",
      "Training batch 84 with loss 0.31355\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776996701955795\n",
      "Training batch 85 with loss 0.29252\n",
      "------------------------------------------\n",
      "L1 loss: 0.0828021690249443\n",
      "Training batch 86 with loss 0.29578\n",
      "------------------------------------------\n",
      "L1 loss: 0.09969921410083771\n",
      "Training batch 87 with loss 0.42157\n",
      "------------------------------------------\n",
      "L1 loss: 0.0792689323425293\n",
      "Training batch 88 with loss 0.32642\n",
      "------------------------------------------\n",
      "L1 loss: 0.06953762471675873\n",
      "Training batch 89 with loss 0.25200\n",
      "------------------------------------------\n",
      "L1 loss: 0.07461348921060562\n",
      "Training batch 90 with loss 0.27648\n",
      "------------------------------------------\n",
      "L1 loss: 0.06280039995908737\n",
      "Training batch 91 with loss 0.25931\n",
      "------------------------------------------\n",
      "L1 loss: 0.09290509670972824\n",
      "Training batch 92 with loss 0.31416\n",
      "------------------------------------------\n",
      "L1 loss: 0.06621605157852173\n",
      "Training batch 93 with loss 0.25903\n",
      "------------------------------------------\n",
      "L1 loss: 0.0866205245256424\n",
      "Training batch 94 with loss 0.31461\n",
      "------------------------------------------\n",
      "L1 loss: 0.07410648465156555\n",
      "Training batch 95 with loss 0.24800\n",
      "------------------------------------------\n",
      "L1 loss: 0.0651833787560463\n",
      "Training batch 96 with loss 0.27664\n",
      "------------------------------------------\n",
      "L1 loss: 0.08326452970504761\n",
      "Training batch 97 with loss 0.34974\n",
      "------------------------------------------\n",
      "L1 loss: 0.05442478880286217\n",
      "Training batch 98 with loss 0.23622\n",
      "------------------------------------------\n",
      "L1 loss: 0.06494428217411041\n",
      "Training batch 99 with loss 0.26637\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2912478789687157\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4861\n",
      "------------------------------------------\n",
      "L1 loss: 0.06001373752951622\n",
      "Training batch 0 with loss 0.26374\n",
      "------------------------------------------\n",
      "L1 loss: 0.08350406587123871\n",
      "Training batch 1 with loss 0.28997\n",
      "------------------------------------------\n",
      "L1 loss: 0.09592539072036743\n",
      "Training batch 2 with loss 0.32887\n",
      "------------------------------------------\n",
      "L1 loss: 0.07605718076229095\n",
      "Training batch 3 with loss 0.25832\n",
      "------------------------------------------\n",
      "L1 loss: 0.08371959626674652\n",
      "Training batch 4 with loss 0.31971\n",
      "------------------------------------------\n",
      "L1 loss: 0.06502342969179153\n",
      "Training batch 5 with loss 0.31761\n",
      "------------------------------------------\n",
      "L1 loss: 0.08112884312868118\n",
      "Training batch 6 with loss 0.26229\n",
      "------------------------------------------\n",
      "L1 loss: 0.08357362449169159\n",
      "Training batch 7 with loss 0.33802\n",
      "------------------------------------------\n",
      "L1 loss: 0.0820983275771141\n",
      "Training batch 8 with loss 0.34937\n",
      "------------------------------------------\n",
      "L1 loss: 0.057395949959754944\n",
      "Training batch 9 with loss 0.25443\n",
      "------------------------------------------\n",
      "L1 loss: 0.08079812675714493\n",
      "Training batch 10 with loss 0.29800\n",
      "------------------------------------------\n",
      "L1 loss: 0.06319985538721085\n",
      "Training batch 11 with loss 0.28565\n",
      "------------------------------------------\n",
      "L1 loss: 0.07495581358671188\n",
      "Training batch 12 with loss 0.30168\n",
      "------------------------------------------\n",
      "L1 loss: 0.06887657940387726\n",
      "Training batch 13 with loss 0.24512\n",
      "------------------------------------------\n",
      "L1 loss: 0.05171380564570427\n",
      "Training batch 14 with loss 0.25375\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08178626000881195\n",
      "Training batch 15 with loss 0.27673\n",
      "------------------------------------------\n",
      "L1 loss: 0.06333710253238678\n",
      "Training batch 16 with loss 0.26478\n",
      "------------------------------------------\n",
      "L1 loss: 0.082403264939785\n",
      "Training batch 17 with loss 0.33561\n",
      "------------------------------------------\n",
      "L1 loss: 0.07908936589956284\n",
      "Training batch 18 with loss 0.29888\n",
      "------------------------------------------\n",
      "L1 loss: 0.06086970120668411\n",
      "Training batch 19 with loss 0.22447\n",
      "------------------------------------------\n",
      "L1 loss: 0.07296806573867798\n",
      "Training batch 20 with loss 0.31049\n",
      "------------------------------------------\n",
      "L1 loss: 0.07180480659008026\n",
      "Training batch 21 with loss 0.25921\n",
      "------------------------------------------\n",
      "L1 loss: 0.06840565800666809\n",
      "Training batch 22 with loss 0.29055\n",
      "------------------------------------------\n",
      "L1 loss: 0.09044194966554642\n",
      "Training batch 23 with loss 0.35966\n",
      "------------------------------------------\n",
      "L1 loss: 0.06993766129016876\n",
      "Training batch 24 with loss 0.30243\n",
      "------------------------------------------\n",
      "L1 loss: 0.0756056159734726\n",
      "Training batch 25 with loss 0.29490\n",
      "------------------------------------------\n",
      "L1 loss: 0.07512371987104416\n",
      "Training batch 26 with loss 0.27775\n",
      "------------------------------------------\n",
      "L1 loss: 0.05606893450021744\n",
      "Training batch 27 with loss 0.28774\n",
      "------------------------------------------\n",
      "L1 loss: 0.05832314491271973\n",
      "Training batch 28 with loss 0.28983\n",
      "------------------------------------------\n",
      "L1 loss: 0.05545300245285034\n",
      "Training batch 29 with loss 0.24315\n",
      "------------------------------------------\n",
      "L1 loss: 0.05983823537826538\n",
      "Training batch 30 with loss 0.26228\n",
      "------------------------------------------\n",
      "L1 loss: 0.06524316221475601\n",
      "Training batch 31 with loss 0.31142\n",
      "------------------------------------------\n",
      "L1 loss: 0.06929673254489899\n",
      "Training batch 32 with loss 0.27374\n",
      "------------------------------------------\n",
      "L1 loss: 0.06623805314302444\n",
      "Training batch 33 with loss 0.27529\n",
      "------------------------------------------\n",
      "L1 loss: 0.07617749273777008\n",
      "Training batch 34 with loss 0.32771\n",
      "------------------------------------------\n",
      "L1 loss: 0.09370717406272888\n",
      "Training batch 35 with loss 0.33674\n",
      "------------------------------------------\n",
      "L1 loss: 0.08350762724876404\n",
      "Training batch 36 with loss 0.27574\n",
      "------------------------------------------\n",
      "L1 loss: 0.06751058250665665\n",
      "Training batch 37 with loss 0.24278\n",
      "------------------------------------------\n",
      "L1 loss: 0.06371215730905533\n",
      "Training batch 38 with loss 0.22816\n",
      "------------------------------------------\n",
      "L1 loss: 0.06756945699453354\n",
      "Training batch 39 with loss 0.29550\n",
      "------------------------------------------\n",
      "L1 loss: 0.06822238117456436\n",
      "Training batch 40 with loss 0.24391\n",
      "------------------------------------------\n",
      "L1 loss: 0.05803197622299194\n",
      "Training batch 41 with loss 0.28352\n",
      "------------------------------------------\n",
      "L1 loss: 0.07638702541589737\n",
      "Training batch 42 with loss 0.27984\n",
      "------------------------------------------\n",
      "L1 loss: 0.07237058877944946\n",
      "Training batch 43 with loss 0.30185\n",
      "------------------------------------------\n",
      "L1 loss: 0.05598389729857445\n",
      "Training batch 44 with loss 0.26949\n",
      "------------------------------------------\n",
      "L1 loss: 0.09143666923046112\n",
      "Training batch 45 with loss 0.36193\n",
      "------------------------------------------\n",
      "L1 loss: 0.07430916279554367\n",
      "Training batch 46 with loss 0.27204\n",
      "------------------------------------------\n",
      "L1 loss: 0.11116556078195572\n",
      "Training batch 47 with loss 0.36542\n",
      "------------------------------------------\n",
      "L1 loss: 0.04828726127743721\n",
      "Training batch 48 with loss 0.23713\n",
      "------------------------------------------\n",
      "L1 loss: 0.08304218202829361\n",
      "Training batch 49 with loss 0.35993\n",
      "------------------------------------------\n",
      "L1 loss: 0.07334879040718079\n",
      "Training batch 50 with loss 0.29081\n",
      "------------------------------------------\n",
      "L1 loss: 0.07702508568763733\n",
      "Training batch 51 with loss 0.30888\n",
      "------------------------------------------\n",
      "L1 loss: 0.08984346687793732\n",
      "Training batch 52 with loss 0.29043\n",
      "------------------------------------------\n",
      "L1 loss: 0.079304538667202\n",
      "Training batch 53 with loss 0.29824\n",
      "------------------------------------------\n",
      "L1 loss: 0.0779867172241211\n",
      "Training batch 54 with loss 0.27282\n",
      "------------------------------------------\n",
      "L1 loss: 0.06063352897763252\n",
      "Training batch 55 with loss 0.29201\n",
      "------------------------------------------\n",
      "L1 loss: 0.05575104057788849\n",
      "Training batch 56 with loss 0.24956\n",
      "------------------------------------------\n",
      "L1 loss: 0.06791721284389496\n",
      "Training batch 57 with loss 0.28608\n",
      "------------------------------------------\n",
      "L1 loss: 0.06853703409433365\n",
      "Training batch 58 with loss 0.25704\n",
      "------------------------------------------\n",
      "L1 loss: 0.08074598759412766\n",
      "Training batch 59 with loss 0.29416\n",
      "------------------------------------------\n",
      "L1 loss: 0.06627820432186127\n",
      "Training batch 60 with loss 0.27853\n",
      "------------------------------------------\n",
      "L1 loss: 0.09113116562366486\n",
      "Training batch 61 with loss 0.34958\n",
      "------------------------------------------\n",
      "L1 loss: 0.05680925026535988\n",
      "Training batch 62 with loss 0.25303\n",
      "------------------------------------------\n",
      "L1 loss: 0.0612248070538044\n",
      "Training batch 63 with loss 0.26240\n",
      "------------------------------------------\n",
      "L1 loss: 0.06858103722333908\n",
      "Training batch 64 with loss 0.27823\n",
      "------------------------------------------\n",
      "L1 loss: 0.06098020449280739\n",
      "Training batch 65 with loss 0.23028\n",
      "------------------------------------------\n",
      "L1 loss: 0.06210325285792351\n",
      "Training batch 66 with loss 0.26114\n",
      "------------------------------------------\n",
      "L1 loss: 0.06675765663385391\n",
      "Training batch 67 with loss 0.26611\n",
      "------------------------------------------\n",
      "L1 loss: 0.0726928636431694\n",
      "Training batch 68 with loss 0.30662\n",
      "------------------------------------------\n",
      "L1 loss: 0.052347976714372635\n",
      "Training batch 69 with loss 0.26927\n",
      "------------------------------------------\n",
      "L1 loss: 0.07520445436239243\n",
      "Training batch 70 with loss 0.29711\n",
      "------------------------------------------\n",
      "L1 loss: 0.07043065875768661\n",
      "Training batch 71 with loss 0.28910\n",
      "------------------------------------------\n",
      "L1 loss: 0.08684186637401581\n",
      "Training batch 72 with loss 0.31704\n",
      "------------------------------------------\n",
      "L1 loss: 0.07158736139535904\n",
      "Training batch 73 with loss 0.29965\n",
      "------------------------------------------\n",
      "L1 loss: 0.09297055006027222\n",
      "Training batch 74 with loss 0.32163\n",
      "------------------------------------------\n",
      "L1 loss: 0.07481273263692856\n",
      "Training batch 75 with loss 0.28574\n",
      "------------------------------------------\n",
      "L1 loss: 0.08652538061141968\n",
      "Training batch 76 with loss 0.30721\n",
      "------------------------------------------\n",
      "L1 loss: 0.07854574173688889\n",
      "Training batch 77 with loss 0.25828\n",
      "------------------------------------------\n",
      "L1 loss: 0.08020901679992676\n",
      "Training batch 78 with loss 0.30302\n",
      "------------------------------------------\n",
      "L1 loss: 0.1037941724061966\n",
      "Training batch 79 with loss 0.37524\n",
      "------------------------------------------\n",
      "L1 loss: 0.07768320292234421\n",
      "Training batch 80 with loss 0.27825\n",
      "------------------------------------------\n",
      "L1 loss: 0.07495617866516113\n",
      "Training batch 81 with loss 0.29639\n",
      "------------------------------------------\n",
      "L1 loss: 0.08476864546537399\n",
      "Training batch 82 with loss 0.35445\n",
      "------------------------------------------\n",
      "L1 loss: 0.07851986587047577\n",
      "Training batch 83 with loss 0.27824\n",
      "------------------------------------------\n",
      "L1 loss: 0.05938071757555008\n",
      "Training batch 84 with loss 0.26324\n",
      "------------------------------------------\n",
      "L1 loss: 0.06409084051847458\n",
      "Training batch 85 with loss 0.28384\n",
      "------------------------------------------\n",
      "L1 loss: 0.08363385498523712\n",
      "Training batch 86 with loss 0.29339\n",
      "------------------------------------------\n",
      "L1 loss: 0.10338040441274643\n",
      "Training batch 87 with loss 0.38020\n",
      "------------------------------------------\n",
      "L1 loss: 0.07792218774557114\n",
      "Training batch 88 with loss 0.32189\n",
      "------------------------------------------\n",
      "L1 loss: 0.07055158168077469\n",
      "Training batch 89 with loss 0.25973\n",
      "------------------------------------------\n",
      "L1 loss: 0.07578141987323761\n",
      "Training batch 90 with loss 0.29152\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.05959371477365494\n",
      "Training batch 91 with loss 0.24527\n",
      "------------------------------------------\n",
      "L1 loss: 0.09342610836029053\n",
      "Training batch 92 with loss 0.30922\n",
      "------------------------------------------\n",
      "L1 loss: 0.07126934081315994\n",
      "Training batch 93 with loss 0.28063\n",
      "------------------------------------------\n",
      "L1 loss: 0.0837898775935173\n",
      "Training batch 94 with loss 0.48402\n",
      "------------------------------------------\n",
      "L1 loss: 0.06931217014789581\n",
      "Training batch 95 with loss 0.25255\n",
      "------------------------------------------\n",
      "L1 loss: 0.06220870092511177\n",
      "Training batch 96 with loss 0.29262\n",
      "------------------------------------------\n",
      "L1 loss: 0.08611937612295151\n",
      "Training batch 97 with loss 0.35990\n",
      "------------------------------------------\n",
      "L1 loss: 0.05581478029489517\n",
      "Training batch 98 with loss 0.24364\n",
      "------------------------------------------\n",
      "L1 loss: 0.06399606913328171\n",
      "Training batch 99 with loss 0.24857\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2915392901003361\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4862\n",
      "------------------------------------------\n",
      "L1 loss: 0.06092099845409393\n",
      "Training batch 0 with loss 0.24642\n",
      "------------------------------------------\n",
      "L1 loss: 0.08596635609865189\n",
      "Training batch 1 with loss 0.29121\n",
      "------------------------------------------\n",
      "L1 loss: 0.09505748003721237\n",
      "Training batch 2 with loss 0.32321\n",
      "------------------------------------------\n",
      "L1 loss: 0.07944458723068237\n",
      "Training batch 3 with loss 0.27542\n",
      "------------------------------------------\n",
      "L1 loss: 0.08519771695137024\n",
      "Training batch 4 with loss 0.32212\n",
      "------------------------------------------\n",
      "L1 loss: 0.06434612721204758\n",
      "Training batch 5 with loss 0.58466\n",
      "------------------------------------------\n",
      "L1 loss: 0.06820780783891678\n",
      "Training batch 6 with loss 0.27564\n",
      "------------------------------------------\n",
      "L1 loss: 0.08445066958665848\n",
      "Training batch 7 with loss 0.34002\n",
      "------------------------------------------\n",
      "L1 loss: 0.08132348209619522\n",
      "Training batch 8 with loss 0.31877\n",
      "------------------------------------------\n",
      "L1 loss: 0.06392617523670197\n",
      "Training batch 9 with loss 0.25102\n",
      "------------------------------------------\n",
      "L1 loss: 0.08146391808986664\n",
      "Training batch 10 with loss 0.30017\n",
      "------------------------------------------\n",
      "L1 loss: 0.05947931110858917\n",
      "Training batch 11 with loss 0.28300\n",
      "------------------------------------------\n",
      "L1 loss: 0.07473984360694885\n",
      "Training batch 12 with loss 0.31098\n",
      "------------------------------------------\n",
      "L1 loss: 0.06916990876197815\n",
      "Training batch 13 with loss 0.25239\n",
      "------------------------------------------\n",
      "L1 loss: 0.05294336751103401\n",
      "Training batch 14 with loss 0.25931\n",
      "------------------------------------------\n",
      "L1 loss: 0.08171888440847397\n",
      "Training batch 15 with loss 0.47921\n",
      "------------------------------------------\n",
      "L1 loss: 0.06311262398958206\n",
      "Training batch 16 with loss 0.27404\n",
      "------------------------------------------\n",
      "L1 loss: 0.08497188985347748\n",
      "Training batch 17 with loss 0.32722\n",
      "------------------------------------------\n",
      "L1 loss: 0.08117179572582245\n",
      "Training batch 18 with loss 0.30573\n",
      "------------------------------------------\n",
      "L1 loss: 0.062313228845596313\n",
      "Training batch 19 with loss 0.21852\n",
      "------------------------------------------\n",
      "L1 loss: 0.07305513322353363\n",
      "Training batch 20 with loss 0.33159\n",
      "------------------------------------------\n",
      "L1 loss: 0.07368014752864838\n",
      "Training batch 21 with loss 0.28783\n",
      "------------------------------------------\n",
      "L1 loss: 0.06954436749219894\n",
      "Training batch 22 with loss 0.28438\n",
      "------------------------------------------\n",
      "L1 loss: 0.09131873399019241\n",
      "Training batch 23 with loss 0.35133\n",
      "------------------------------------------\n",
      "L1 loss: 0.06863974034786224\n",
      "Training batch 24 with loss 0.30406\n",
      "------------------------------------------\n",
      "L1 loss: 0.07712762802839279\n",
      "Training batch 25 with loss 0.29051\n",
      "------------------------------------------\n",
      "L1 loss: 0.07468272000551224\n",
      "Training batch 26 with loss 0.27073\n",
      "------------------------------------------\n",
      "L1 loss: 0.0565127357840538\n",
      "Training batch 27 with loss 0.28846\n",
      "------------------------------------------\n",
      "L1 loss: 0.05780797451734543\n",
      "Training batch 28 with loss 0.28622\n",
      "------------------------------------------\n",
      "L1 loss: 0.05535005405545235\n",
      "Training batch 29 with loss 0.24671\n",
      "------------------------------------------\n",
      "L1 loss: 0.06217721849679947\n",
      "Training batch 30 with loss 0.26800\n",
      "------------------------------------------\n",
      "L1 loss: 0.06461440026760101\n",
      "Training batch 31 with loss 0.30577\n",
      "------------------------------------------\n",
      "L1 loss: 0.07036793231964111\n",
      "Training batch 32 with loss 0.27285\n",
      "------------------------------------------\n",
      "L1 loss: 0.063385970890522\n",
      "Training batch 33 with loss 0.26195\n",
      "------------------------------------------\n",
      "L1 loss: 0.07505346089601517\n",
      "Training batch 34 with loss 0.31830\n",
      "------------------------------------------\n",
      "L1 loss: 0.091549351811409\n",
      "Training batch 35 with loss 0.33148\n",
      "------------------------------------------\n",
      "L1 loss: 0.0820155143737793\n",
      "Training batch 36 with loss 0.29027\n",
      "------------------------------------------\n",
      "L1 loss: 0.06719346344470978\n",
      "Training batch 37 with loss 0.25875\n",
      "------------------------------------------\n",
      "L1 loss: 0.06452733278274536\n",
      "Training batch 38 with loss 0.21794\n",
      "------------------------------------------\n",
      "L1 loss: 0.06875957548618317\n",
      "Training batch 39 with loss 0.29774\n",
      "------------------------------------------\n",
      "L1 loss: 0.07020972669124603\n",
      "Training batch 40 with loss 0.26653\n",
      "------------------------------------------\n",
      "L1 loss: 0.06103518232703209\n",
      "Training batch 41 with loss 0.29989\n",
      "------------------------------------------\n",
      "L1 loss: 0.07613088190555573\n",
      "Training batch 42 with loss 0.27505\n",
      "------------------------------------------\n",
      "L1 loss: 0.06956098228693008\n",
      "Training batch 43 with loss 0.29611\n",
      "------------------------------------------\n",
      "L1 loss: 0.05297515541315079\n",
      "Training batch 44 with loss 0.37761\n",
      "------------------------------------------\n",
      "L1 loss: 0.09232991188764572\n",
      "Training batch 45 with loss 0.35942\n",
      "------------------------------------------\n",
      "L1 loss: 0.07231231033802032\n",
      "Training batch 46 with loss 0.26106\n",
      "------------------------------------------\n",
      "L1 loss: 0.11055482178926468\n",
      "Training batch 47 with loss 0.37294\n",
      "------------------------------------------\n",
      "L1 loss: 0.04633134603500366\n",
      "Training batch 48 with loss 0.22758\n",
      "------------------------------------------\n",
      "L1 loss: 0.07907205075025558\n",
      "Training batch 49 with loss 0.34401\n",
      "------------------------------------------\n",
      "L1 loss: 0.07424169778823853\n",
      "Training batch 50 with loss 0.29639\n",
      "------------------------------------------\n",
      "L1 loss: 0.079351045191288\n",
      "Training batch 51 with loss 0.33149\n",
      "------------------------------------------\n",
      "L1 loss: 0.08963800966739655\n",
      "Training batch 52 with loss 0.29335\n",
      "------------------------------------------\n",
      "L1 loss: 0.07848596572875977\n",
      "Training batch 53 with loss 0.28191\n",
      "------------------------------------------\n",
      "L1 loss: 0.07586853951215744\n",
      "Training batch 54 with loss 0.27392\n",
      "------------------------------------------\n",
      "L1 loss: 0.059132564812898636\n",
      "Training batch 55 with loss 0.28636\n",
      "------------------------------------------\n",
      "L1 loss: 0.05111122503876686\n",
      "Training batch 56 with loss 0.23916\n",
      "------------------------------------------\n",
      "L1 loss: 0.0696006491780281\n",
      "Training batch 57 with loss 0.27037\n",
      "------------------------------------------\n",
      "L1 loss: 0.06481120735406876\n",
      "Training batch 58 with loss 0.26353\n",
      "------------------------------------------\n",
      "L1 loss: 0.0782933384180069\n",
      "Training batch 59 with loss 0.28542\n",
      "------------------------------------------\n",
      "L1 loss: 0.06591974943876266\n",
      "Training batch 60 with loss 0.27366\n",
      "------------------------------------------\n",
      "L1 loss: 0.08835659921169281\n",
      "Training batch 61 with loss 0.33857\n",
      "------------------------------------------\n",
      "L1 loss: 0.05662882328033447\n",
      "Training batch 62 with loss 0.25999\n",
      "------------------------------------------\n",
      "L1 loss: 0.06157819181680679\n",
      "Training batch 63 with loss 0.24848\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709078311920166\n",
      "Training batch 64 with loss 0.26295\n",
      "------------------------------------------\n",
      "L1 loss: 0.06209998577833176\n",
      "Training batch 65 with loss 0.22977\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06274786591529846\n",
      "Training batch 66 with loss 0.26477\n",
      "------------------------------------------\n",
      "L1 loss: 0.06543667614459991\n",
      "Training batch 67 with loss 0.25811\n",
      "------------------------------------------\n",
      "L1 loss: 0.05934896692633629\n",
      "Training batch 68 with loss 0.47263\n",
      "------------------------------------------\n",
      "L1 loss: 0.0506400465965271\n",
      "Training batch 69 with loss 0.26430\n",
      "------------------------------------------\n",
      "L1 loss: 0.07590989768505096\n",
      "Training batch 70 with loss 0.30568\n",
      "------------------------------------------\n",
      "L1 loss: 0.07173804938793182\n",
      "Training batch 71 with loss 0.30439\n",
      "------------------------------------------\n",
      "L1 loss: 0.08669818192720413\n",
      "Training batch 72 with loss 0.30740\n",
      "------------------------------------------\n",
      "L1 loss: 0.06900949776172638\n",
      "Training batch 73 with loss 0.27823\n",
      "------------------------------------------\n",
      "L1 loss: 0.09791665524244308\n",
      "Training batch 74 with loss 0.30550\n",
      "------------------------------------------\n",
      "L1 loss: 0.07505962997674942\n",
      "Training batch 75 with loss 0.28089\n",
      "------------------------------------------\n",
      "L1 loss: 0.08589869737625122\n",
      "Training batch 76 with loss 0.31181\n",
      "------------------------------------------\n",
      "L1 loss: 0.08101382851600647\n",
      "Training batch 77 with loss 0.26000\n",
      "------------------------------------------\n",
      "L1 loss: 0.0800735354423523\n",
      "Training batch 78 with loss 0.29140\n",
      "------------------------------------------\n",
      "L1 loss: 0.10613976418972015\n",
      "Training batch 79 with loss 0.36692\n",
      "------------------------------------------\n",
      "L1 loss: 0.07483246922492981\n",
      "Training batch 80 with loss 0.27098\n",
      "------------------------------------------\n",
      "L1 loss: 0.07844313979148865\n",
      "Training batch 81 with loss 0.31242\n",
      "------------------------------------------\n",
      "L1 loss: 0.08189455419778824\n",
      "Training batch 82 with loss 0.36326\n",
      "------------------------------------------\n",
      "L1 loss: 0.0798562541604042\n",
      "Training batch 83 with loss 0.28947\n",
      "------------------------------------------\n",
      "L1 loss: 0.06615041196346283\n",
      "Training batch 84 with loss 0.28844\n",
      "------------------------------------------\n",
      "L1 loss: 0.05977184697985649\n",
      "Training batch 85 with loss 0.27681\n",
      "------------------------------------------\n",
      "L1 loss: 0.08099279552698135\n",
      "Training batch 86 with loss 0.29457\n",
      "------------------------------------------\n",
      "L1 loss: 0.1016845852136612\n",
      "Training batch 87 with loss 0.38038\n",
      "------------------------------------------\n",
      "L1 loss: 0.07468369603157043\n",
      "Training batch 88 with loss 0.31443\n",
      "------------------------------------------\n",
      "L1 loss: 0.0699448436498642\n",
      "Training batch 89 with loss 0.26237\n",
      "------------------------------------------\n",
      "L1 loss: 0.07167468965053558\n",
      "Training batch 90 with loss 0.26960\n",
      "------------------------------------------\n",
      "L1 loss: 0.06163613870739937\n",
      "Training batch 91 with loss 0.27519\n",
      "------------------------------------------\n",
      "L1 loss: 0.09253133088350296\n",
      "Training batch 92 with loss 0.30096\n",
      "------------------------------------------\n",
      "L1 loss: 0.05462464690208435\n",
      "Training batch 93 with loss 0.22771\n",
      "------------------------------------------\n",
      "L1 loss: 0.08287734538316727\n",
      "Training batch 94 with loss 0.30853\n",
      "------------------------------------------\n",
      "L1 loss: 0.07123800367116928\n",
      "Training batch 95 with loss 0.25071\n",
      "------------------------------------------\n",
      "L1 loss: 0.06453970819711685\n",
      "Training batch 96 with loss 0.29596\n",
      "------------------------------------------\n",
      "L1 loss: 0.08927115797996521\n",
      "Training batch 97 with loss 0.36192\n",
      "------------------------------------------\n",
      "L1 loss: 0.058354899287223816\n",
      "Training batch 98 with loss 0.23824\n",
      "------------------------------------------\n",
      "L1 loss: 0.06658415496349335\n",
      "Training batch 99 with loss 0.27790\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2965130370855331\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4863\n",
      "------------------------------------------\n",
      "L1 loss: 0.06270793825387955\n",
      "Training batch 0 with loss 0.25006\n",
      "------------------------------------------\n",
      "L1 loss: 0.08373246341943741\n",
      "Training batch 1 with loss 0.28035\n",
      "------------------------------------------\n",
      "L1 loss: 0.08047866821289062\n",
      "Training batch 2 with loss 0.71693\n",
      "------------------------------------------\n",
      "L1 loss: 0.07883407175540924\n",
      "Training batch 3 with loss 0.27172\n",
      "------------------------------------------\n",
      "L1 loss: 0.08441349118947983\n",
      "Training batch 4 with loss 0.34371\n",
      "------------------------------------------\n",
      "L1 loss: 0.06858650594949722\n",
      "Training batch 5 with loss 0.34935\n",
      "------------------------------------------\n",
      "L1 loss: 0.08115530014038086\n",
      "Training batch 6 with loss 0.27328\n",
      "------------------------------------------\n",
      "L1 loss: 0.08391857147216797\n",
      "Training batch 7 with loss 0.31712\n",
      "------------------------------------------\n",
      "L1 loss: 0.08249946683645248\n",
      "Training batch 8 with loss 0.32931\n",
      "------------------------------------------\n",
      "L1 loss: 0.0626126155257225\n",
      "Training batch 9 with loss 0.24876\n",
      "------------------------------------------\n",
      "L1 loss: 0.08478167653083801\n",
      "Training batch 10 with loss 0.29186\n",
      "------------------------------------------\n",
      "L1 loss: 0.06236233189702034\n",
      "Training batch 11 with loss 0.29170\n",
      "------------------------------------------\n",
      "L1 loss: 0.07722096890211105\n",
      "Training batch 12 with loss 0.31981\n",
      "------------------------------------------\n",
      "L1 loss: 0.0694919005036354\n",
      "Training batch 13 with loss 0.24479\n",
      "------------------------------------------\n",
      "L1 loss: 0.053488589823246\n",
      "Training batch 14 with loss 0.26498\n",
      "------------------------------------------\n",
      "L1 loss: 0.08405798673629761\n",
      "Training batch 15 with loss 0.27237\n",
      "------------------------------------------\n",
      "L1 loss: 0.06459306180477142\n",
      "Training batch 16 with loss 0.25938\n",
      "------------------------------------------\n",
      "L1 loss: 0.08539193123579025\n",
      "Training batch 17 with loss 0.33800\n",
      "------------------------------------------\n",
      "L1 loss: 0.07907909899950027\n",
      "Training batch 18 with loss 0.28612\n",
      "------------------------------------------\n",
      "L1 loss: 0.06289052963256836\n",
      "Training batch 19 with loss 0.25000\n",
      "------------------------------------------\n",
      "L1 loss: 0.07357309758663177\n",
      "Training batch 20 with loss 0.30585\n",
      "------------------------------------------\n",
      "L1 loss: 0.07446359843015671\n",
      "Training batch 21 with loss 0.29739\n",
      "------------------------------------------\n",
      "L1 loss: 0.06835876405239105\n",
      "Training batch 22 with loss 0.29415\n",
      "------------------------------------------\n",
      "L1 loss: 0.089483842253685\n",
      "Training batch 23 with loss 0.35775\n",
      "------------------------------------------\n",
      "L1 loss: 0.0693146362900734\n",
      "Training batch 24 with loss 0.30212\n",
      "------------------------------------------\n",
      "L1 loss: 0.07630380988121033\n",
      "Training batch 25 with loss 0.31634\n",
      "------------------------------------------\n",
      "L1 loss: 0.07771755754947662\n",
      "Training batch 26 with loss 0.28493\n",
      "------------------------------------------\n",
      "L1 loss: 0.05734012648463249\n",
      "Training batch 27 with loss 0.28104\n",
      "------------------------------------------\n",
      "L1 loss: 0.058600232005119324\n",
      "Training batch 28 with loss 0.28511\n",
      "------------------------------------------\n",
      "L1 loss: 0.060269445180892944\n",
      "Training batch 29 with loss 0.26098\n",
      "------------------------------------------\n",
      "L1 loss: 0.052464962005615234\n",
      "Training batch 30 with loss 0.31629\n",
      "------------------------------------------\n",
      "L1 loss: 0.06613409519195557\n",
      "Training batch 31 with loss 0.30760\n",
      "------------------------------------------\n",
      "L1 loss: 0.06919319927692413\n",
      "Training batch 32 with loss 0.27669\n",
      "------------------------------------------\n",
      "L1 loss: 0.059380535036325455\n",
      "Training batch 33 with loss 0.25957\n",
      "------------------------------------------\n",
      "L1 loss: 0.07512789964675903\n",
      "Training batch 34 with loss 0.29678\n",
      "------------------------------------------\n",
      "L1 loss: 0.09259460866451263\n",
      "Training batch 35 with loss 0.33951\n",
      "------------------------------------------\n",
      "L1 loss: 0.08552989363670349\n",
      "Training batch 36 with loss 0.29128\n",
      "------------------------------------------\n",
      "L1 loss: 0.07289553433656693\n",
      "Training batch 37 with loss 0.28643\n",
      "------------------------------------------\n",
      "L1 loss: 0.0624360516667366\n",
      "Training batch 38 with loss 0.22425\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776473671197891\n",
      "Training batch 39 with loss 0.30404\n",
      "------------------------------------------\n",
      "L1 loss: 0.07251667976379395\n",
      "Training batch 40 with loss 0.25440\n",
      "------------------------------------------\n",
      "L1 loss: 0.05890163779258728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 41 with loss 0.27658\n",
      "------------------------------------------\n",
      "L1 loss: 0.07693765312433243\n",
      "Training batch 42 with loss 0.27135\n",
      "------------------------------------------\n",
      "L1 loss: 0.07314789295196533\n",
      "Training batch 43 with loss 0.27833\n",
      "------------------------------------------\n",
      "L1 loss: 0.056305620819330215\n",
      "Training batch 44 with loss 0.26243\n",
      "------------------------------------------\n",
      "L1 loss: 0.09330461174249649\n",
      "Training batch 45 with loss 0.37043\n",
      "------------------------------------------\n",
      "L1 loss: 0.0716107115149498\n",
      "Training batch 46 with loss 0.28185\n",
      "------------------------------------------\n",
      "L1 loss: 0.11159563064575195\n",
      "Training batch 47 with loss 0.36113\n",
      "------------------------------------------\n",
      "L1 loss: 0.04540251940488815\n",
      "Training batch 48 with loss 0.22499\n",
      "------------------------------------------\n",
      "L1 loss: 0.07375828921794891\n",
      "Training batch 49 with loss 0.41613\n",
      "------------------------------------------\n",
      "L1 loss: 0.07431968301534653\n",
      "Training batch 50 with loss 0.29878\n",
      "------------------------------------------\n",
      "L1 loss: 0.08155911415815353\n",
      "Training batch 51 with loss 0.33827\n",
      "------------------------------------------\n",
      "L1 loss: 0.09024646878242493\n",
      "Training batch 52 with loss 0.29256\n",
      "------------------------------------------\n",
      "L1 loss: 0.08019286394119263\n",
      "Training batch 53 with loss 0.28601\n",
      "------------------------------------------\n",
      "L1 loss: 0.0758652314543724\n",
      "Training batch 54 with loss 0.26953\n",
      "------------------------------------------\n",
      "L1 loss: 0.059955280274152756\n",
      "Training batch 55 with loss 0.33526\n",
      "------------------------------------------\n",
      "L1 loss: 0.05341826751828194\n",
      "Training batch 56 with loss 0.25382\n",
      "------------------------------------------\n",
      "L1 loss: 0.06780768930912018\n",
      "Training batch 57 with loss 0.29450\n",
      "------------------------------------------\n",
      "L1 loss: 0.06839334964752197\n",
      "Training batch 58 with loss 0.26640\n",
      "------------------------------------------\n",
      "L1 loss: 0.077530138194561\n",
      "Training batch 59 with loss 0.29730\n",
      "------------------------------------------\n",
      "L1 loss: 0.06480375677347183\n",
      "Training batch 60 with loss 0.27520\n",
      "------------------------------------------\n",
      "L1 loss: 0.09208398312330246\n",
      "Training batch 61 with loss 0.34297\n",
      "------------------------------------------\n",
      "L1 loss: 0.05775480717420578\n",
      "Training batch 62 with loss 0.26650\n",
      "------------------------------------------\n",
      "L1 loss: 0.06204812601208687\n",
      "Training batch 63 with loss 0.25708\n",
      "------------------------------------------\n",
      "L1 loss: 0.0706481859087944\n",
      "Training batch 64 with loss 0.28255\n",
      "------------------------------------------\n",
      "L1 loss: 0.05362045392394066\n",
      "Training batch 65 with loss 0.23677\n",
      "------------------------------------------\n",
      "L1 loss: 0.06611020863056183\n",
      "Training batch 66 with loss 0.26832\n",
      "------------------------------------------\n",
      "L1 loss: 0.06711991131305695\n",
      "Training batch 67 with loss 0.26737\n",
      "------------------------------------------\n",
      "L1 loss: 0.07242214679718018\n",
      "Training batch 68 with loss 0.29689\n",
      "------------------------------------------\n",
      "L1 loss: 0.04974904656410217\n",
      "Training batch 69 with loss 0.25823\n",
      "------------------------------------------\n",
      "L1 loss: 0.07678564637899399\n",
      "Training batch 70 with loss 0.30396\n",
      "------------------------------------------\n",
      "L1 loss: 0.07165893912315369\n",
      "Training batch 71 with loss 0.29211\n",
      "------------------------------------------\n",
      "L1 loss: 0.08596628904342651\n",
      "Training batch 72 with loss 0.31633\n",
      "------------------------------------------\n",
      "L1 loss: 0.07430421561002731\n",
      "Training batch 73 with loss 0.29048\n",
      "------------------------------------------\n",
      "L1 loss: 0.09608662128448486\n",
      "Training batch 74 with loss 0.29680\n",
      "------------------------------------------\n",
      "L1 loss: 0.0734943076968193\n",
      "Training batch 75 with loss 0.27660\n",
      "------------------------------------------\n",
      "L1 loss: 0.0864800363779068\n",
      "Training batch 76 with loss 0.28833\n",
      "------------------------------------------\n",
      "L1 loss: 0.07828155159950256\n",
      "Training batch 77 with loss 0.29006\n",
      "------------------------------------------\n",
      "L1 loss: 0.08069520443677902\n",
      "Training batch 78 with loss 0.28822\n",
      "------------------------------------------\n",
      "L1 loss: 0.10273005068302155\n",
      "Training batch 79 with loss 0.36581\n",
      "------------------------------------------\n",
      "L1 loss: 0.07677110284566879\n",
      "Training batch 80 with loss 0.28392\n",
      "------------------------------------------\n",
      "L1 loss: 0.07984976470470428\n",
      "Training batch 81 with loss 0.29688\n",
      "------------------------------------------\n",
      "L1 loss: 0.08428796380758286\n",
      "Training batch 82 with loss 0.35976\n",
      "------------------------------------------\n",
      "L1 loss: 0.07685938477516174\n",
      "Training batch 83 with loss 0.27804\n",
      "------------------------------------------\n",
      "L1 loss: 0.06932353973388672\n",
      "Training batch 84 with loss 0.29637\n",
      "------------------------------------------\n",
      "L1 loss: 0.06063354015350342\n",
      "Training batch 85 with loss 0.27010\n",
      "------------------------------------------\n",
      "L1 loss: 0.08253014832735062\n",
      "Training batch 86 with loss 0.28401\n",
      "------------------------------------------\n",
      "L1 loss: 0.10133635997772217\n",
      "Training batch 87 with loss 0.39980\n",
      "------------------------------------------\n",
      "L1 loss: 0.07574625313282013\n",
      "Training batch 88 with loss 0.30570\n",
      "------------------------------------------\n",
      "L1 loss: 0.06900394707918167\n",
      "Training batch 89 with loss 0.26925\n",
      "------------------------------------------\n",
      "L1 loss: 0.07202156633138657\n",
      "Training batch 90 with loss 0.28848\n",
      "------------------------------------------\n",
      "L1 loss: 0.06309831887483597\n",
      "Training batch 91 with loss 0.25190\n",
      "------------------------------------------\n",
      "L1 loss: 0.09076747298240662\n",
      "Training batch 92 with loss 0.30414\n",
      "------------------------------------------\n",
      "L1 loss: 0.06747722625732422\n",
      "Training batch 93 with loss 0.26869\n",
      "------------------------------------------\n",
      "L1 loss: 0.08674036711454391\n",
      "Training batch 94 with loss 0.31114\n",
      "------------------------------------------\n",
      "L1 loss: 0.07081995904445648\n",
      "Training batch 95 with loss 0.25249\n",
      "------------------------------------------\n",
      "L1 loss: 0.05693013593554497\n",
      "Training batch 96 with loss 0.26309\n",
      "------------------------------------------\n",
      "L1 loss: 0.08544453978538513\n",
      "Training batch 97 with loss 0.35210\n",
      "------------------------------------------\n",
      "L1 loss: 0.056258246302604675\n",
      "Training batch 98 with loss 0.23168\n",
      "------------------------------------------\n",
      "L1 loss: 0.06492049992084503\n",
      "Training batch 99 with loss 0.26389\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29609793722629546\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4864\n",
      "------------------------------------------\n",
      "L1 loss: 0.06370484083890915\n",
      "Training batch 0 with loss 0.26536\n",
      "------------------------------------------\n",
      "L1 loss: 0.08347838371992111\n",
      "Training batch 1 with loss 0.28368\n",
      "------------------------------------------\n",
      "L1 loss: 0.09268732368946075\n",
      "Training batch 2 with loss 0.31760\n",
      "------------------------------------------\n",
      "L1 loss: 0.07093597203493118\n",
      "Training batch 3 with loss 0.25444\n",
      "------------------------------------------\n",
      "L1 loss: 0.085475854575634\n",
      "Training batch 4 with loss 0.32997\n",
      "------------------------------------------\n",
      "L1 loss: 0.06725981831550598\n",
      "Training batch 5 with loss 0.32536\n",
      "------------------------------------------\n",
      "L1 loss: 0.08015651255846024\n",
      "Training batch 6 with loss 0.27006\n",
      "------------------------------------------\n",
      "L1 loss: 0.08369101583957672\n",
      "Training batch 7 with loss 0.34503\n",
      "------------------------------------------\n",
      "L1 loss: 0.08017553389072418\n",
      "Training batch 8 with loss 0.31578\n",
      "------------------------------------------\n",
      "L1 loss: 0.05751890689134598\n",
      "Training batch 9 with loss 0.25365\n",
      "------------------------------------------\n",
      "L1 loss: 0.08323822170495987\n",
      "Training batch 10 with loss 0.29590\n",
      "------------------------------------------\n",
      "L1 loss: 0.061741944402456284\n",
      "Training batch 11 with loss 0.27966\n",
      "------------------------------------------\n",
      "L1 loss: 0.07635975629091263\n",
      "Training batch 12 with loss 0.30922\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976187974214554\n",
      "Training batch 13 with loss 0.24925\n",
      "------------------------------------------\n",
      "L1 loss: 0.05354633927345276\n",
      "Training batch 14 with loss 0.25211\n",
      "------------------------------------------\n",
      "L1 loss: 0.08487902581691742\n",
      "Training batch 15 with loss 0.27764\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06354106217622757\n",
      "Training batch 16 with loss 0.26122\n",
      "------------------------------------------\n",
      "L1 loss: 0.08978524059057236\n",
      "Training batch 17 with loss 0.35005\n",
      "------------------------------------------\n",
      "L1 loss: 0.08074770867824554\n",
      "Training batch 18 with loss 0.30844\n",
      "------------------------------------------\n",
      "L1 loss: 0.06259329617023468\n",
      "Training batch 19 with loss 0.22374\n",
      "------------------------------------------\n",
      "L1 loss: 0.06026811525225639\n",
      "Training batch 20 with loss 0.55892\n",
      "------------------------------------------\n",
      "L1 loss: 0.06993617117404938\n",
      "Training batch 21 with loss 0.27534\n",
      "------------------------------------------\n",
      "L1 loss: 0.055263541638851166\n",
      "Training batch 22 with loss 0.23289\n",
      "------------------------------------------\n",
      "L1 loss: 0.07503662258386612\n",
      "Training batch 23 with loss 0.47418\n",
      "------------------------------------------\n",
      "L1 loss: 0.06932638585567474\n",
      "Training batch 24 with loss 0.30801\n",
      "------------------------------------------\n",
      "L1 loss: 0.0779217779636383\n",
      "Training batch 25 with loss 0.29864\n",
      "------------------------------------------\n",
      "L1 loss: 0.08002690970897675\n",
      "Training batch 26 with loss 0.29294\n",
      "------------------------------------------\n",
      "L1 loss: 0.05487438663840294\n",
      "Training batch 27 with loss 0.28104\n",
      "------------------------------------------\n",
      "L1 loss: 0.057380110025405884\n",
      "Training batch 28 with loss 0.29694\n",
      "------------------------------------------\n",
      "L1 loss: 0.06110871210694313\n",
      "Training batch 29 with loss 0.26094\n",
      "------------------------------------------\n",
      "L1 loss: 0.0605817474424839\n",
      "Training batch 30 with loss 0.27202\n",
      "------------------------------------------\n",
      "L1 loss: 0.06648653000593185\n",
      "Training batch 31 with loss 0.30795\n",
      "------------------------------------------\n",
      "L1 loss: 0.06949799507856369\n",
      "Training batch 32 with loss 0.28217\n",
      "------------------------------------------\n",
      "L1 loss: 0.06581078469753265\n",
      "Training batch 33 with loss 0.26564\n",
      "------------------------------------------\n",
      "L1 loss: 0.0732797235250473\n",
      "Training batch 34 with loss 0.32278\n",
      "------------------------------------------\n",
      "L1 loss: 0.09518738836050034\n",
      "Training batch 35 with loss 0.33610\n",
      "------------------------------------------\n",
      "L1 loss: 0.08219733089208603\n",
      "Training batch 36 with loss 0.28253\n",
      "------------------------------------------\n",
      "L1 loss: 0.06851182132959366\n",
      "Training batch 37 with loss 0.26040\n",
      "------------------------------------------\n",
      "L1 loss: 0.06368761509656906\n",
      "Training batch 38 with loss 0.22286\n",
      "------------------------------------------\n",
      "L1 loss: 0.06708929687738419\n",
      "Training batch 39 with loss 0.29634\n",
      "------------------------------------------\n",
      "L1 loss: 0.0692015290260315\n",
      "Training batch 40 with loss 0.25131\n",
      "------------------------------------------\n",
      "L1 loss: 0.059951622039079666\n",
      "Training batch 41 with loss 0.28779\n",
      "------------------------------------------\n",
      "L1 loss: 0.07648517191410065\n",
      "Training batch 42 with loss 0.28084\n",
      "------------------------------------------\n",
      "L1 loss: 0.07210313528776169\n",
      "Training batch 43 with loss 0.28375\n",
      "------------------------------------------\n",
      "L1 loss: 0.05449942871928215\n",
      "Training batch 44 with loss 0.25683\n",
      "------------------------------------------\n",
      "L1 loss: 0.09264286607503891\n",
      "Training batch 45 with loss 0.34612\n",
      "------------------------------------------\n",
      "L1 loss: 0.07481905072927475\n",
      "Training batch 46 with loss 0.27217\n",
      "------------------------------------------\n",
      "L1 loss: 0.09583260118961334\n",
      "Training batch 47 with loss 0.59579\n",
      "------------------------------------------\n",
      "L1 loss: 0.048202481120824814\n",
      "Training batch 48 with loss 0.26191\n",
      "------------------------------------------\n",
      "L1 loss: 0.08259675651788712\n",
      "Training batch 49 with loss 0.35889\n",
      "------------------------------------------\n",
      "L1 loss: 0.07146728038787842\n",
      "Training batch 50 with loss 0.28190\n",
      "------------------------------------------\n",
      "L1 loss: 0.08021964877843857\n",
      "Training batch 51 with loss 0.33121\n",
      "------------------------------------------\n",
      "L1 loss: 0.08992358297109604\n",
      "Training batch 52 with loss 0.30146\n",
      "------------------------------------------\n",
      "L1 loss: 0.07921794056892395\n",
      "Training batch 53 with loss 0.29566\n",
      "------------------------------------------\n",
      "L1 loss: 0.07657255232334137\n",
      "Training batch 54 with loss 0.27007\n",
      "------------------------------------------\n",
      "L1 loss: 0.06216929480433464\n",
      "Training batch 55 with loss 0.30252\n",
      "------------------------------------------\n",
      "L1 loss: 0.05625038594007492\n",
      "Training batch 56 with loss 0.25307\n",
      "------------------------------------------\n",
      "L1 loss: 0.06844283640384674\n",
      "Training batch 57 with loss 0.28397\n",
      "------------------------------------------\n",
      "L1 loss: 0.06739295274019241\n",
      "Training batch 58 with loss 0.26432\n",
      "------------------------------------------\n",
      "L1 loss: 0.08287867903709412\n",
      "Training batch 59 with loss 0.29927\n",
      "------------------------------------------\n",
      "L1 loss: 0.06790431588888168\n",
      "Training batch 60 with loss 0.28478\n",
      "------------------------------------------\n",
      "L1 loss: 0.10340385138988495\n",
      "Training batch 61 with loss 0.36773\n",
      "------------------------------------------\n",
      "L1 loss: 0.05837971344590187\n",
      "Training batch 62 with loss 0.25554\n",
      "------------------------------------------\n",
      "L1 loss: 0.06358765065670013\n",
      "Training batch 63 with loss 0.27746\n",
      "------------------------------------------\n",
      "L1 loss: 0.07233105599880219\n",
      "Training batch 64 with loss 0.30147\n",
      "------------------------------------------\n",
      "L1 loss: 0.05985362455248833\n",
      "Training batch 65 with loss 0.23244\n",
      "------------------------------------------\n",
      "L1 loss: 0.06661076098680496\n",
      "Training batch 66 with loss 0.28091\n",
      "------------------------------------------\n",
      "L1 loss: 0.06589022278785706\n",
      "Training batch 67 with loss 0.27492\n",
      "------------------------------------------\n",
      "L1 loss: 0.07141809165477753\n",
      "Training batch 68 with loss 0.30569\n",
      "------------------------------------------\n",
      "L1 loss: 0.053071122616529465\n",
      "Training batch 69 with loss 0.26600\n",
      "------------------------------------------\n",
      "L1 loss: 0.07364415377378464\n",
      "Training batch 70 with loss 0.28971\n",
      "------------------------------------------\n",
      "L1 loss: 0.07178309559822083\n",
      "Training batch 71 with loss 0.30408\n",
      "------------------------------------------\n",
      "L1 loss: 0.08151358366012573\n",
      "Training batch 72 with loss 0.28623\n",
      "------------------------------------------\n",
      "L1 loss: 0.07206452637910843\n",
      "Training batch 73 with loss 0.30210\n",
      "------------------------------------------\n",
      "L1 loss: 0.09592664241790771\n",
      "Training batch 74 with loss 0.33676\n",
      "------------------------------------------\n",
      "L1 loss: 0.07322898507118225\n",
      "Training batch 75 with loss 0.29079\n",
      "------------------------------------------\n",
      "L1 loss: 0.08721932023763657\n",
      "Training batch 76 with loss 0.28462\n",
      "------------------------------------------\n",
      "L1 loss: 0.07913732528686523\n",
      "Training batch 77 with loss 0.25981\n",
      "------------------------------------------\n",
      "L1 loss: 0.07806649804115295\n",
      "Training batch 78 with loss 0.29931\n",
      "------------------------------------------\n",
      "L1 loss: 0.094724141061306\n",
      "Training batch 79 with loss 0.38102\n",
      "------------------------------------------\n",
      "L1 loss: 0.07620959728956223\n",
      "Training batch 80 with loss 0.28191\n",
      "------------------------------------------\n",
      "L1 loss: 0.07890082895755768\n",
      "Training batch 81 with loss 0.28498\n",
      "------------------------------------------\n",
      "L1 loss: 0.078687883913517\n",
      "Training batch 82 with loss 0.38142\n",
      "------------------------------------------\n",
      "L1 loss: 0.07503629475831985\n",
      "Training batch 83 with loss 0.26696\n",
      "------------------------------------------\n",
      "L1 loss: 0.0697377547621727\n",
      "Training batch 84 with loss 0.29538\n",
      "------------------------------------------\n",
      "L1 loss: 0.06463619321584702\n",
      "Training batch 85 with loss 0.28874\n",
      "------------------------------------------\n",
      "L1 loss: 0.07462494820356369\n",
      "Training batch 86 with loss 0.29663\n",
      "------------------------------------------\n",
      "L1 loss: 0.10246571153402328\n",
      "Training batch 87 with loss 0.40567\n",
      "------------------------------------------\n",
      "L1 loss: 0.07957414537668228\n",
      "Training batch 88 with loss 0.33402\n",
      "------------------------------------------\n",
      "L1 loss: 0.06972302496433258\n",
      "Training batch 89 with loss 0.25603\n",
      "------------------------------------------\n",
      "L1 loss: 0.07263778150081635\n",
      "Training batch 90 with loss 0.28500\n",
      "------------------------------------------\n",
      "L1 loss: 0.0615118071436882\n",
      "Training batch 91 with loss 0.25991\n",
      "------------------------------------------\n",
      "L1 loss: 0.09172725677490234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 92 with loss 0.29863\n",
      "------------------------------------------\n",
      "L1 loss: 0.06201888993382454\n",
      "Training batch 93 with loss 0.24237\n",
      "------------------------------------------\n",
      "L1 loss: 0.08241042494773865\n",
      "Training batch 94 with loss 0.29996\n",
      "------------------------------------------\n",
      "L1 loss: 0.0711401030421257\n",
      "Training batch 95 with loss 0.26546\n",
      "------------------------------------------\n",
      "L1 loss: 0.06038183718919754\n",
      "Training batch 96 with loss 0.27677\n",
      "------------------------------------------\n",
      "L1 loss: 0.0849936455488205\n",
      "Training batch 97 with loss 0.35143\n",
      "------------------------------------------\n",
      "L1 loss: 0.057514455169439316\n",
      "Training batch 98 with loss 0.23523\n",
      "------------------------------------------\n",
      "L1 loss: 0.06522085517644882\n",
      "Training batch 99 with loss 0.25212\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2971563668549061\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4865\n",
      "------------------------------------------\n",
      "L1 loss: 0.06027558073401451\n",
      "Training batch 0 with loss 0.25467\n",
      "------------------------------------------\n",
      "L1 loss: 0.0821928158402443\n",
      "Training batch 1 with loss 0.30180\n",
      "------------------------------------------\n",
      "L1 loss: 0.0934867262840271\n",
      "Training batch 2 with loss 0.32382\n",
      "------------------------------------------\n",
      "L1 loss: 0.07504214346408844\n",
      "Training batch 3 with loss 0.25791\n",
      "------------------------------------------\n",
      "L1 loss: 0.08381803333759308\n",
      "Training batch 4 with loss 0.31228\n",
      "------------------------------------------\n",
      "L1 loss: 0.06657703965902328\n",
      "Training batch 5 with loss 0.31625\n",
      "------------------------------------------\n",
      "L1 loss: 0.08015988022089005\n",
      "Training batch 6 with loss 0.26499\n",
      "------------------------------------------\n",
      "L1 loss: 0.0847218856215477\n",
      "Training batch 7 with loss 0.32495\n",
      "------------------------------------------\n",
      "L1 loss: 0.0804581493139267\n",
      "Training batch 8 with loss 0.32096\n",
      "------------------------------------------\n",
      "L1 loss: 0.06292589008808136\n",
      "Training batch 9 with loss 0.25284\n",
      "------------------------------------------\n",
      "L1 loss: 0.08257031440734863\n",
      "Training batch 10 with loss 0.28360\n",
      "------------------------------------------\n",
      "L1 loss: 0.059976477175951004\n",
      "Training batch 11 with loss 0.27986\n",
      "------------------------------------------\n",
      "L1 loss: 0.0773991122841835\n",
      "Training batch 12 with loss 0.32289\n",
      "------------------------------------------\n",
      "L1 loss: 0.07004719227552414\n",
      "Training batch 13 with loss 0.24394\n",
      "------------------------------------------\n",
      "L1 loss: 0.053061164915561676\n",
      "Training batch 14 with loss 0.25274\n",
      "------------------------------------------\n",
      "L1 loss: 0.08563684672117233\n",
      "Training batch 15 with loss 0.26949\n",
      "------------------------------------------\n",
      "L1 loss: 0.06147647649049759\n",
      "Training batch 16 with loss 0.25741\n",
      "------------------------------------------\n",
      "L1 loss: 0.08060146123170853\n",
      "Training batch 17 with loss 0.31607\n",
      "------------------------------------------\n",
      "L1 loss: 0.06087103486061096\n",
      "Training batch 18 with loss 0.23696\n",
      "------------------------------------------\n",
      "L1 loss: 0.059941768646240234\n",
      "Training batch 19 with loss 0.24373\n",
      "------------------------------------------\n",
      "L1 loss: 0.07298976182937622\n",
      "Training batch 20 with loss 0.29756\n",
      "------------------------------------------\n",
      "L1 loss: 0.07160408794879913\n",
      "Training batch 21 with loss 0.28076\n",
      "------------------------------------------\n",
      "L1 loss: 0.06914152204990387\n",
      "Training batch 22 with loss 0.27562\n",
      "------------------------------------------\n",
      "L1 loss: 0.09113970398902893\n",
      "Training batch 23 with loss 0.37413\n",
      "------------------------------------------\n",
      "L1 loss: 0.0699990764260292\n",
      "Training batch 24 with loss 0.30081\n",
      "------------------------------------------\n",
      "L1 loss: 0.07268683612346649\n",
      "Training batch 25 with loss 0.28844\n",
      "------------------------------------------\n",
      "L1 loss: 0.0749814584851265\n",
      "Training batch 26 with loss 0.35919\n",
      "------------------------------------------\n",
      "L1 loss: 0.0544818677008152\n",
      "Training batch 27 with loss 0.28474\n",
      "------------------------------------------\n",
      "L1 loss: 0.058317333459854126\n",
      "Training batch 28 with loss 0.28283\n",
      "------------------------------------------\n",
      "L1 loss: 0.05809957906603813\n",
      "Training batch 29 with loss 0.26052\n",
      "------------------------------------------\n",
      "L1 loss: 0.06308942288160324\n",
      "Training batch 30 with loss 0.28312\n",
      "------------------------------------------\n",
      "L1 loss: 0.06578296422958374\n",
      "Training batch 31 with loss 0.31236\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976848095655441\n",
      "Training batch 32 with loss 0.26963\n",
      "------------------------------------------\n",
      "L1 loss: 0.06662202626466751\n",
      "Training batch 33 with loss 0.28340\n",
      "------------------------------------------\n",
      "L1 loss: 0.07850099354982376\n",
      "Training batch 34 with loss 0.33136\n",
      "------------------------------------------\n",
      "L1 loss: 0.0939604789018631\n",
      "Training batch 35 with loss 0.33109\n",
      "------------------------------------------\n",
      "L1 loss: 0.08404755592346191\n",
      "Training batch 36 with loss 0.28456\n",
      "------------------------------------------\n",
      "L1 loss: 0.06628189980983734\n",
      "Training batch 37 with loss 0.25460\n",
      "------------------------------------------\n",
      "L1 loss: 0.06266184151172638\n",
      "Training batch 38 with loss 0.21994\n",
      "------------------------------------------\n",
      "L1 loss: 0.06757950037717819\n",
      "Training batch 39 with loss 0.30422\n",
      "------------------------------------------\n",
      "L1 loss: 0.06979075074195862\n",
      "Training batch 40 with loss 0.25321\n",
      "------------------------------------------\n",
      "L1 loss: 0.06080036610364914\n",
      "Training batch 41 with loss 0.28983\n",
      "------------------------------------------\n",
      "L1 loss: 0.07932557165622711\n",
      "Training batch 42 with loss 0.26557\n",
      "------------------------------------------\n",
      "L1 loss: 0.07330627739429474\n",
      "Training batch 43 with loss 0.29404\n",
      "------------------------------------------\n",
      "L1 loss: 0.050687506794929504\n",
      "Training batch 44 with loss 0.34154\n",
      "------------------------------------------\n",
      "L1 loss: 0.09268426895141602\n",
      "Training batch 45 with loss 0.36756\n",
      "------------------------------------------\n",
      "L1 loss: 0.07405322790145874\n",
      "Training batch 46 with loss 0.25862\n",
      "------------------------------------------\n",
      "L1 loss: 0.10912603884935379\n",
      "Training batch 47 with loss 0.35935\n",
      "------------------------------------------\n",
      "L1 loss: 0.044611889868974686\n",
      "Training batch 48 with loss 0.22626\n",
      "------------------------------------------\n",
      "L1 loss: 0.08345053344964981\n",
      "Training batch 49 with loss 0.35310\n",
      "------------------------------------------\n",
      "L1 loss: 0.07475819438695908\n",
      "Training batch 50 with loss 0.28752\n",
      "------------------------------------------\n",
      "L1 loss: 0.07804224640130997\n",
      "Training batch 51 with loss 0.33658\n",
      "------------------------------------------\n",
      "L1 loss: 0.08944668620824814\n",
      "Training batch 52 with loss 0.29276\n",
      "------------------------------------------\n",
      "L1 loss: 0.08139976859092712\n",
      "Training batch 53 with loss 0.28534\n",
      "------------------------------------------\n",
      "L1 loss: 0.07696833461523056\n",
      "Training batch 54 with loss 0.26186\n",
      "------------------------------------------\n",
      "L1 loss: 0.05765959247946739\n",
      "Training batch 55 with loss 0.30726\n",
      "------------------------------------------\n",
      "L1 loss: 0.05791302025318146\n",
      "Training batch 56 with loss 0.25437\n",
      "------------------------------------------\n",
      "L1 loss: 0.06607450544834137\n",
      "Training batch 57 with loss 0.28032\n",
      "------------------------------------------\n",
      "L1 loss: 0.06572195142507553\n",
      "Training batch 58 with loss 0.25716\n",
      "------------------------------------------\n",
      "L1 loss: 0.07950592786073685\n",
      "Training batch 59 with loss 0.29466\n",
      "------------------------------------------\n",
      "L1 loss: 0.06473933160305023\n",
      "Training batch 60 with loss 0.27275\n",
      "------------------------------------------\n",
      "L1 loss: 0.0770794078707695\n",
      "Training batch 61 with loss 0.48576\n",
      "------------------------------------------\n",
      "L1 loss: 0.05580827221274376\n",
      "Training batch 62 with loss 0.25677\n",
      "------------------------------------------\n",
      "L1 loss: 0.05929102748632431\n",
      "Training batch 63 with loss 0.24613\n",
      "------------------------------------------\n",
      "L1 loss: 0.06817654520273209\n",
      "Training batch 64 with loss 0.28009\n",
      "------------------------------------------\n",
      "L1 loss: 0.06131066754460335\n",
      "Training batch 65 with loss 0.23421\n",
      "------------------------------------------\n",
      "L1 loss: 0.06697283685207367\n",
      "Training batch 66 with loss 0.27145\n",
      "------------------------------------------\n",
      "L1 loss: 0.06511659920215607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 67 with loss 0.25935\n",
      "------------------------------------------\n",
      "L1 loss: 0.07088305056095123\n",
      "Training batch 68 with loss 0.30362\n",
      "------------------------------------------\n",
      "L1 loss: 0.0499177947640419\n",
      "Training batch 69 with loss 0.26328\n",
      "------------------------------------------\n",
      "L1 loss: 0.08014406263828278\n",
      "Training batch 70 with loss 0.33404\n",
      "------------------------------------------\n",
      "L1 loss: 0.07237794250249863\n",
      "Training batch 71 with loss 0.29396\n",
      "------------------------------------------\n",
      "L1 loss: 0.08540626615285873\n",
      "Training batch 72 with loss 0.33118\n",
      "------------------------------------------\n",
      "L1 loss: 0.07067573815584183\n",
      "Training batch 73 with loss 0.29281\n",
      "------------------------------------------\n",
      "L1 loss: 0.09743069857358932\n",
      "Training batch 74 with loss 0.31887\n",
      "------------------------------------------\n",
      "L1 loss: 0.07469494640827179\n",
      "Training batch 75 with loss 0.29271\n",
      "------------------------------------------\n",
      "L1 loss: 0.08560319244861603\n",
      "Training batch 76 with loss 0.31746\n",
      "------------------------------------------\n",
      "L1 loss: 0.0789150521159172\n",
      "Training batch 77 with loss 0.27059\n",
      "------------------------------------------\n",
      "L1 loss: 0.08501346409320831\n",
      "Training batch 78 with loss 0.31028\n",
      "------------------------------------------\n",
      "L1 loss: 0.08160410821437836\n",
      "Training batch 79 with loss 0.62122\n",
      "------------------------------------------\n",
      "L1 loss: 0.07524556666612625\n",
      "Training batch 80 with loss 0.27248\n",
      "------------------------------------------\n",
      "L1 loss: 0.07429149746894836\n",
      "Training batch 81 with loss 0.29078\n",
      "------------------------------------------\n",
      "L1 loss: 0.08389312028884888\n",
      "Training batch 82 with loss 0.36484\n",
      "------------------------------------------\n",
      "L1 loss: 0.07411009073257446\n",
      "Training batch 83 with loss 0.29630\n",
      "------------------------------------------\n",
      "L1 loss: 0.06918714195489883\n",
      "Training batch 84 with loss 0.31877\n",
      "------------------------------------------\n",
      "L1 loss: 0.05547231435775757\n",
      "Training batch 85 with loss 0.34380\n",
      "------------------------------------------\n",
      "L1 loss: 0.07903987169265747\n",
      "Training batch 86 with loss 0.28749\n",
      "------------------------------------------\n",
      "L1 loss: 0.10188348591327667\n",
      "Training batch 87 with loss 0.39872\n",
      "------------------------------------------\n",
      "L1 loss: 0.07754352688789368\n",
      "Training batch 88 with loss 0.30592\n",
      "------------------------------------------\n",
      "L1 loss: 0.06929150968790054\n",
      "Training batch 89 with loss 0.26181\n",
      "------------------------------------------\n",
      "L1 loss: 0.07300690561532974\n",
      "Training batch 90 with loss 0.26515\n",
      "------------------------------------------\n",
      "L1 loss: 0.06355255097150803\n",
      "Training batch 91 with loss 0.26883\n",
      "------------------------------------------\n",
      "L1 loss: 0.09275778383016586\n",
      "Training batch 92 with loss 0.30844\n",
      "------------------------------------------\n",
      "L1 loss: 0.0698419064283371\n",
      "Training batch 93 with loss 0.25955\n",
      "------------------------------------------\n",
      "L1 loss: 0.08317797631025314\n",
      "Training batch 94 with loss 0.30989\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709901675581932\n",
      "Training batch 95 with loss 0.25166\n",
      "------------------------------------------\n",
      "L1 loss: 0.06798194348812103\n",
      "Training batch 96 with loss 0.31877\n",
      "------------------------------------------\n",
      "L1 loss: 0.08720111101865768\n",
      "Training batch 97 with loss 0.36782\n",
      "------------------------------------------\n",
      "L1 loss: 0.05780934914946556\n",
      "Training batch 98 with loss 0.24276\n",
      "------------------------------------------\n",
      "L1 loss: 0.06369956582784653\n",
      "Training batch 99 with loss 0.25908\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.296323766708374\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4866\n",
      "------------------------------------------\n",
      "L1 loss: 0.06131274625658989\n",
      "Training batch 0 with loss 0.24631\n",
      "------------------------------------------\n",
      "L1 loss: 0.08394350856542587\n",
      "Training batch 1 with loss 0.28226\n",
      "------------------------------------------\n",
      "L1 loss: 0.09290056675672531\n",
      "Training batch 2 with loss 0.31918\n",
      "------------------------------------------\n",
      "L1 loss: 0.06805730611085892\n",
      "Training batch 3 with loss 0.24355\n",
      "------------------------------------------\n",
      "L1 loss: 0.08398808538913727\n",
      "Training batch 4 with loss 0.33136\n",
      "------------------------------------------\n",
      "L1 loss: 0.0671352744102478\n",
      "Training batch 5 with loss 0.31786\n",
      "------------------------------------------\n",
      "L1 loss: 0.07887507975101471\n",
      "Training batch 6 with loss 0.27161\n",
      "------------------------------------------\n",
      "L1 loss: 0.08406082540750504\n",
      "Training batch 7 with loss 0.33347\n",
      "------------------------------------------\n",
      "L1 loss: 0.07817292958498001\n",
      "Training batch 8 with loss 0.32260\n",
      "------------------------------------------\n",
      "L1 loss: 0.05975368618965149\n",
      "Training batch 9 with loss 0.24754\n",
      "------------------------------------------\n",
      "L1 loss: 0.08179327100515366\n",
      "Training batch 10 with loss 0.30190\n",
      "------------------------------------------\n",
      "L1 loss: 0.05926186591386795\n",
      "Training batch 11 with loss 0.29915\n",
      "------------------------------------------\n",
      "L1 loss: 0.0742921233177185\n",
      "Training batch 12 with loss 0.30148\n",
      "------------------------------------------\n",
      "L1 loss: 0.07074085623025894\n",
      "Training batch 13 with loss 0.25151\n",
      "------------------------------------------\n",
      "L1 loss: 0.05207531154155731\n",
      "Training batch 14 with loss 0.25012\n",
      "------------------------------------------\n",
      "L1 loss: 0.08427266031503677\n",
      "Training batch 15 with loss 0.27689\n",
      "------------------------------------------\n",
      "L1 loss: 0.06315398961305618\n",
      "Training batch 16 with loss 0.26233\n",
      "------------------------------------------\n",
      "L1 loss: 0.08259209245443344\n",
      "Training batch 17 with loss 0.31922\n",
      "------------------------------------------\n",
      "L1 loss: 0.08006613701581955\n",
      "Training batch 18 with loss 0.29672\n",
      "------------------------------------------\n",
      "L1 loss: 0.06265739351511002\n",
      "Training batch 19 with loss 0.22824\n",
      "------------------------------------------\n",
      "L1 loss: 0.07377167046070099\n",
      "Training batch 20 with loss 0.32900\n",
      "------------------------------------------\n",
      "L1 loss: 0.07317996770143509\n",
      "Training batch 21 with loss 0.27912\n",
      "------------------------------------------\n",
      "L1 loss: 0.06879746913909912\n",
      "Training batch 22 with loss 0.27998\n",
      "------------------------------------------\n",
      "L1 loss: 0.09217876940965652\n",
      "Training batch 23 with loss 0.35789\n",
      "------------------------------------------\n",
      "L1 loss: 0.06653779745101929\n",
      "Training batch 24 with loss 0.30178\n",
      "------------------------------------------\n",
      "L1 loss: 0.07410154491662979\n",
      "Training batch 25 with loss 0.30947\n",
      "------------------------------------------\n",
      "L1 loss: 0.07993488013744354\n",
      "Training batch 26 with loss 0.28198\n",
      "------------------------------------------\n",
      "L1 loss: 0.04505479708313942\n",
      "Training batch 27 with loss 0.23288\n",
      "------------------------------------------\n",
      "L1 loss: 0.05780184268951416\n",
      "Training batch 28 with loss 0.28536\n",
      "------------------------------------------\n",
      "L1 loss: 0.06187434121966362\n",
      "Training batch 29 with loss 0.28222\n",
      "------------------------------------------\n",
      "L1 loss: 0.06052990257740021\n",
      "Training batch 30 with loss 0.27101\n",
      "------------------------------------------\n",
      "L1 loss: 0.05388258025050163\n",
      "Training batch 31 with loss 0.40120\n",
      "------------------------------------------\n",
      "L1 loss: 0.06857186555862427\n",
      "Training batch 32 with loss 0.27405\n",
      "------------------------------------------\n",
      "L1 loss: 0.06254436075687408\n",
      "Training batch 33 with loss 0.25284\n",
      "------------------------------------------\n",
      "L1 loss: 0.07670101523399353\n",
      "Training batch 34 with loss 0.31070\n",
      "------------------------------------------\n",
      "L1 loss: 0.09562621265649796\n",
      "Training batch 35 with loss 0.33078\n",
      "------------------------------------------\n",
      "L1 loss: 0.08685865253210068\n",
      "Training batch 36 with loss 0.27795\n",
      "------------------------------------------\n",
      "L1 loss: 0.06721697747707367\n",
      "Training batch 37 with loss 0.25693\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394077837467194\n",
      "Training batch 38 with loss 0.21724\n",
      "------------------------------------------\n",
      "L1 loss: 0.06651762127876282\n",
      "Training batch 39 with loss 0.30129\n",
      "------------------------------------------\n",
      "L1 loss: 0.07216636091470718\n",
      "Training batch 40 with loss 0.26637\n",
      "------------------------------------------\n",
      "L1 loss: 0.05855794623494148\n",
      "Training batch 41 with loss 0.29482\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0801912397146225\n",
      "Training batch 42 with loss 0.26949\n",
      "------------------------------------------\n",
      "L1 loss: 0.06956401467323303\n",
      "Training batch 43 with loss 0.29027\n",
      "------------------------------------------\n",
      "L1 loss: 0.05498092249035835\n",
      "Training batch 44 with loss 0.26878\n",
      "------------------------------------------\n",
      "L1 loss: 0.09256010502576828\n",
      "Training batch 45 with loss 0.36978\n",
      "------------------------------------------\n",
      "L1 loss: 0.07212338596582413\n",
      "Training batch 46 with loss 0.27286\n",
      "------------------------------------------\n",
      "L1 loss: 0.09750007838010788\n",
      "Training batch 47 with loss 0.52701\n",
      "------------------------------------------\n",
      "L1 loss: 0.04881540685892105\n",
      "Training batch 48 with loss 0.25531\n",
      "------------------------------------------\n",
      "L1 loss: 0.08301261812448502\n",
      "Training batch 49 with loss 0.35689\n",
      "------------------------------------------\n",
      "L1 loss: 0.06371879577636719\n",
      "Training batch 50 with loss 0.41637\n",
      "------------------------------------------\n",
      "L1 loss: 0.08279763907194138\n",
      "Training batch 51 with loss 0.35982\n",
      "------------------------------------------\n",
      "L1 loss: 0.09004520624876022\n",
      "Training batch 52 with loss 0.27881\n",
      "------------------------------------------\n",
      "L1 loss: 0.07817480713129044\n",
      "Training batch 53 with loss 0.29938\n",
      "------------------------------------------\n",
      "L1 loss: 0.07588061690330505\n",
      "Training batch 54 with loss 0.26217\n",
      "------------------------------------------\n",
      "L1 loss: 0.06063193082809448\n",
      "Training batch 55 with loss 0.29952\n",
      "------------------------------------------\n",
      "L1 loss: 0.050094909965991974\n",
      "Training batch 56 with loss 0.23732\n",
      "------------------------------------------\n",
      "L1 loss: 0.0679323822259903\n",
      "Training batch 57 with loss 0.28353\n",
      "------------------------------------------\n",
      "L1 loss: 0.050764553248882294\n",
      "Training batch 58 with loss 0.34305\n",
      "------------------------------------------\n",
      "L1 loss: 0.0777391716837883\n",
      "Training batch 59 with loss 0.30011\n",
      "------------------------------------------\n",
      "L1 loss: 0.0542721226811409\n",
      "Training batch 60 with loss 0.35366\n",
      "------------------------------------------\n",
      "L1 loss: 0.10006675869226456\n",
      "Training batch 61 with loss 0.35342\n",
      "------------------------------------------\n",
      "L1 loss: 0.05723714083433151\n",
      "Training batch 62 with loss 0.25700\n",
      "------------------------------------------\n",
      "L1 loss: 0.06163892149925232\n",
      "Training batch 63 with loss 0.25967\n",
      "------------------------------------------\n",
      "L1 loss: 0.07213371247053146\n",
      "Training batch 64 with loss 0.27624\n",
      "------------------------------------------\n",
      "L1 loss: 0.059863846749067307\n",
      "Training batch 65 with loss 0.22674\n",
      "------------------------------------------\n",
      "L1 loss: 0.06796728819608688\n",
      "Training batch 66 with loss 0.27342\n",
      "------------------------------------------\n",
      "L1 loss: 0.06478320062160492\n",
      "Training batch 67 with loss 0.26922\n",
      "------------------------------------------\n",
      "L1 loss: 0.07123833894729614\n",
      "Training batch 68 with loss 0.29478\n",
      "------------------------------------------\n",
      "L1 loss: 0.05204067379236221\n",
      "Training batch 69 with loss 0.27191\n",
      "------------------------------------------\n",
      "L1 loss: 0.07594526559114456\n",
      "Training batch 70 with loss 0.32214\n",
      "------------------------------------------\n",
      "L1 loss: 0.07304215431213379\n",
      "Training batch 71 with loss 0.30706\n",
      "------------------------------------------\n",
      "L1 loss: 0.08812960982322693\n",
      "Training batch 72 with loss 0.33318\n",
      "------------------------------------------\n",
      "L1 loss: 0.07411610335111618\n",
      "Training batch 73 with loss 0.29435\n",
      "------------------------------------------\n",
      "L1 loss: 0.09401322156190872\n",
      "Training batch 74 with loss 0.32211\n",
      "------------------------------------------\n",
      "L1 loss: 0.0766105204820633\n",
      "Training batch 75 with loss 0.28429\n",
      "------------------------------------------\n",
      "L1 loss: 0.08664913475513458\n",
      "Training batch 76 with loss 0.29766\n",
      "------------------------------------------\n",
      "L1 loss: 0.07727180421352386\n",
      "Training batch 77 with loss 0.27049\n",
      "------------------------------------------\n",
      "L1 loss: 0.08130568265914917\n",
      "Training batch 78 with loss 0.30263\n",
      "------------------------------------------\n",
      "L1 loss: 0.10538247972726822\n",
      "Training batch 79 with loss 0.34964\n",
      "------------------------------------------\n",
      "L1 loss: 0.07575684040784836\n",
      "Training batch 80 with loss 0.26796\n",
      "------------------------------------------\n",
      "L1 loss: 0.08244050294160843\n",
      "Training batch 81 with loss 0.30259\n",
      "------------------------------------------\n",
      "L1 loss: 0.08625182509422302\n",
      "Training batch 82 with loss 0.34855\n",
      "------------------------------------------\n",
      "L1 loss: 0.07334839552640915\n",
      "Training batch 83 with loss 0.28085\n",
      "------------------------------------------\n",
      "L1 loss: 0.07231266051530838\n",
      "Training batch 84 with loss 0.30713\n",
      "------------------------------------------\n",
      "L1 loss: 0.06831099092960358\n",
      "Training batch 85 with loss 0.27699\n",
      "------------------------------------------\n",
      "L1 loss: 0.08260920643806458\n",
      "Training batch 86 with loss 0.29162\n",
      "------------------------------------------\n",
      "L1 loss: 0.1009746864438057\n",
      "Training batch 87 with loss 0.39825\n",
      "------------------------------------------\n",
      "L1 loss: 0.07761244475841522\n",
      "Training batch 88 with loss 0.31085\n",
      "------------------------------------------\n",
      "L1 loss: 0.06904345750808716\n",
      "Training batch 89 with loss 0.25913\n",
      "------------------------------------------\n",
      "L1 loss: 0.07394440472126007\n",
      "Training batch 90 with loss 0.28269\n",
      "------------------------------------------\n",
      "L1 loss: 0.06267426908016205\n",
      "Training batch 91 with loss 0.25864\n",
      "------------------------------------------\n",
      "L1 loss: 0.09424004703760147\n",
      "Training batch 92 with loss 0.31106\n",
      "------------------------------------------\n",
      "L1 loss: 0.068413145840168\n",
      "Training batch 93 with loss 0.26091\n",
      "------------------------------------------\n",
      "L1 loss: 0.08535392582416534\n",
      "Training batch 94 with loss 0.30071\n",
      "------------------------------------------\n",
      "L1 loss: 0.07137278467416763\n",
      "Training batch 95 with loss 0.25848\n",
      "------------------------------------------\n",
      "L1 loss: 0.06622959673404694\n",
      "Training batch 96 with loss 0.31336\n",
      "------------------------------------------\n",
      "L1 loss: 0.08785078674554825\n",
      "Training batch 97 with loss 0.34452\n",
      "------------------------------------------\n",
      "L1 loss: 0.05609782412648201\n",
      "Training batch 98 with loss 0.25186\n",
      "------------------------------------------\n",
      "L1 loss: 0.06275659799575806\n",
      "Training batch 99 with loss 0.26373\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29598117783665656\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4867\n",
      "------------------------------------------\n",
      "L1 loss: 0.0606565847992897\n",
      "Training batch 0 with loss 0.25242\n",
      "------------------------------------------\n",
      "L1 loss: 0.08127535134553909\n",
      "Training batch 1 with loss 0.29059\n",
      "------------------------------------------\n",
      "L1 loss: 0.09367688745260239\n",
      "Training batch 2 with loss 0.31750\n",
      "------------------------------------------\n",
      "L1 loss: 0.07041506469249725\n",
      "Training batch 3 with loss 0.25372\n",
      "------------------------------------------\n",
      "L1 loss: 0.08527152985334396\n",
      "Training batch 4 with loss 0.32482\n",
      "------------------------------------------\n",
      "L1 loss: 0.06616400182247162\n",
      "Training batch 5 with loss 0.30511\n",
      "------------------------------------------\n",
      "L1 loss: 0.07909022271633148\n",
      "Training batch 6 with loss 0.27875\n",
      "------------------------------------------\n",
      "L1 loss: 0.08431310206651688\n",
      "Training batch 7 with loss 0.33932\n",
      "------------------------------------------\n",
      "L1 loss: 0.08070545643568039\n",
      "Training batch 8 with loss 0.31494\n",
      "------------------------------------------\n",
      "L1 loss: 0.062079254537820816\n",
      "Training batch 9 with loss 0.24451\n",
      "------------------------------------------\n",
      "L1 loss: 0.08121871948242188\n",
      "Training batch 10 with loss 0.28980\n",
      "------------------------------------------\n",
      "L1 loss: 0.06102271005511284\n",
      "Training batch 11 with loss 0.27500\n",
      "------------------------------------------\n",
      "L1 loss: 0.07424197345972061\n",
      "Training batch 12 with loss 0.29894\n",
      "------------------------------------------\n",
      "L1 loss: 0.07047947496175766\n",
      "Training batch 13 with loss 0.24752\n",
      "------------------------------------------\n",
      "L1 loss: 0.053430989384651184\n",
      "Training batch 14 with loss 0.25156\n",
      "------------------------------------------\n",
      "L1 loss: 0.08382488787174225\n",
      "Training batch 15 with loss 0.27166\n",
      "------------------------------------------\n",
      "L1 loss: 0.06464766710996628\n",
      "Training batch 16 with loss 0.26899\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08170424401760101\n",
      "Training batch 17 with loss 0.31012\n",
      "------------------------------------------\n",
      "L1 loss: 0.07712878286838531\n",
      "Training batch 18 with loss 0.48077\n",
      "------------------------------------------\n",
      "L1 loss: 0.06115107983350754\n",
      "Training batch 19 with loss 0.24182\n",
      "------------------------------------------\n",
      "L1 loss: 0.07190226763486862\n",
      "Training batch 20 with loss 0.32066\n",
      "------------------------------------------\n",
      "L1 loss: 0.07117602974176407\n",
      "Training batch 21 with loss 0.26589\n",
      "------------------------------------------\n",
      "L1 loss: 0.06814928352832794\n",
      "Training batch 22 with loss 0.26777\n",
      "------------------------------------------\n",
      "L1 loss: 0.090053491294384\n",
      "Training batch 23 with loss 0.36229\n",
      "------------------------------------------\n",
      "L1 loss: 0.06677893549203873\n",
      "Training batch 24 with loss 0.28924\n",
      "------------------------------------------\n",
      "L1 loss: 0.0731094479560852\n",
      "Training batch 25 with loss 0.30624\n",
      "------------------------------------------\n",
      "L1 loss: 0.07569088786840439\n",
      "Training batch 26 with loss 0.28204\n",
      "------------------------------------------\n",
      "L1 loss: 0.05718211457133293\n",
      "Training batch 27 with loss 0.31166\n",
      "------------------------------------------\n",
      "L1 loss: 0.05709605664014816\n",
      "Training batch 28 with loss 0.30066\n",
      "------------------------------------------\n",
      "L1 loss: 0.053418416529893875\n",
      "Training batch 29 with loss 0.23011\n",
      "------------------------------------------\n",
      "L1 loss: 0.06263978779315948\n",
      "Training batch 30 with loss 0.25754\n",
      "------------------------------------------\n",
      "L1 loss: 0.06202921271324158\n",
      "Training batch 31 with loss 0.30071\n",
      "------------------------------------------\n",
      "L1 loss: 0.06808696687221527\n",
      "Training batch 32 with loss 0.28269\n",
      "------------------------------------------\n",
      "L1 loss: 0.06451481580734253\n",
      "Training batch 33 with loss 0.25149\n",
      "------------------------------------------\n",
      "L1 loss: 0.0764574408531189\n",
      "Training batch 34 with loss 0.32660\n",
      "------------------------------------------\n",
      "L1 loss: 0.0936017856001854\n",
      "Training batch 35 with loss 0.33339\n",
      "------------------------------------------\n",
      "L1 loss: 0.08463741838932037\n",
      "Training batch 36 with loss 0.29522\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394398957490921\n",
      "Training batch 37 with loss 0.24914\n",
      "------------------------------------------\n",
      "L1 loss: 0.06159670278429985\n",
      "Training batch 38 with loss 0.21322\n",
      "------------------------------------------\n",
      "L1 loss: 0.06538049131631851\n",
      "Training batch 39 with loss 0.29328\n",
      "------------------------------------------\n",
      "L1 loss: 0.06880846619606018\n",
      "Training batch 40 with loss 0.25482\n",
      "------------------------------------------\n",
      "L1 loss: 0.05984513834118843\n",
      "Training batch 41 with loss 0.29097\n",
      "------------------------------------------\n",
      "L1 loss: 0.0812855213880539\n",
      "Training batch 42 with loss 0.27852\n",
      "------------------------------------------\n",
      "L1 loss: 0.07046402990818024\n",
      "Training batch 43 with loss 0.27500\n",
      "------------------------------------------\n",
      "L1 loss: 0.05769570544362068\n",
      "Training batch 44 with loss 0.27968\n",
      "------------------------------------------\n",
      "L1 loss: 0.0924292728304863\n",
      "Training batch 45 with loss 0.36132\n",
      "------------------------------------------\n",
      "L1 loss: 0.076277956366539\n",
      "Training batch 46 with loss 0.27158\n",
      "------------------------------------------\n",
      "L1 loss: 0.11043155193328857\n",
      "Training batch 47 with loss 0.37085\n",
      "------------------------------------------\n",
      "L1 loss: 0.048361603170633316\n",
      "Training batch 48 with loss 0.23460\n",
      "------------------------------------------\n",
      "L1 loss: 0.08078158646821976\n",
      "Training batch 49 with loss 0.34314\n",
      "------------------------------------------\n",
      "L1 loss: 0.07200144231319427\n",
      "Training batch 50 with loss 0.29252\n",
      "------------------------------------------\n",
      "L1 loss: 0.07919290661811829\n",
      "Training batch 51 with loss 0.33325\n",
      "------------------------------------------\n",
      "L1 loss: 0.09012654423713684\n",
      "Training batch 52 with loss 0.28644\n",
      "------------------------------------------\n",
      "L1 loss: 0.08025329560041428\n",
      "Training batch 53 with loss 0.28537\n",
      "------------------------------------------\n",
      "L1 loss: 0.07131390273571014\n",
      "Training batch 54 with loss 0.27302\n",
      "------------------------------------------\n",
      "L1 loss: 0.057762667536735535\n",
      "Training batch 55 with loss 0.31054\n",
      "------------------------------------------\n",
      "L1 loss: 0.05532410368323326\n",
      "Training batch 56 with loss 0.25642\n",
      "------------------------------------------\n",
      "L1 loss: 0.0722687765955925\n",
      "Training batch 57 with loss 0.28140\n",
      "------------------------------------------\n",
      "L1 loss: 0.06851988285779953\n",
      "Training batch 58 with loss 0.26517\n",
      "------------------------------------------\n",
      "L1 loss: 0.08114176243543625\n",
      "Training batch 59 with loss 0.27810\n",
      "------------------------------------------\n",
      "L1 loss: 0.06753163784742355\n",
      "Training batch 60 with loss 0.29082\n",
      "------------------------------------------\n",
      "L1 loss: 0.09294486045837402\n",
      "Training batch 61 with loss 0.33694\n",
      "------------------------------------------\n",
      "L1 loss: 0.05700530856847763\n",
      "Training batch 62 with loss 0.25118\n",
      "------------------------------------------\n",
      "L1 loss: 0.06333931535482407\n",
      "Training batch 63 with loss 0.25573\n",
      "------------------------------------------\n",
      "L1 loss: 0.07043910771608353\n",
      "Training batch 64 with loss 0.27736\n",
      "------------------------------------------\n",
      "L1 loss: 0.06254982948303223\n",
      "Training batch 65 with loss 0.23038\n",
      "------------------------------------------\n",
      "L1 loss: 0.06445261090993881\n",
      "Training batch 66 with loss 0.26987\n",
      "------------------------------------------\n",
      "L1 loss: 0.06519556790590286\n",
      "Training batch 67 with loss 0.26107\n",
      "------------------------------------------\n",
      "L1 loss: 0.07144647091627121\n",
      "Training batch 68 with loss 0.29337\n",
      "------------------------------------------\n",
      "L1 loss: 0.04915803670883179\n",
      "Training batch 69 with loss 0.26725\n",
      "------------------------------------------\n",
      "L1 loss: 0.07737652212381363\n",
      "Training batch 70 with loss 0.30728\n",
      "------------------------------------------\n",
      "L1 loss: 0.07062561810016632\n",
      "Training batch 71 with loss 0.29769\n",
      "------------------------------------------\n",
      "L1 loss: 0.08042734116315842\n",
      "Training batch 72 with loss 0.28773\n",
      "------------------------------------------\n",
      "L1 loss: 0.07075170427560806\n",
      "Training batch 73 with loss 0.28513\n",
      "------------------------------------------\n",
      "L1 loss: 0.09769590198993683\n",
      "Training batch 74 with loss 0.30449\n",
      "------------------------------------------\n",
      "L1 loss: 0.0753810852766037\n",
      "Training batch 75 with loss 0.27584\n",
      "------------------------------------------\n",
      "L1 loss: 0.08633343130350113\n",
      "Training batch 76 with loss 0.28846\n",
      "------------------------------------------\n",
      "L1 loss: 0.07866061478853226\n",
      "Training batch 77 with loss 0.25811\n",
      "------------------------------------------\n",
      "L1 loss: 0.07924394309520721\n",
      "Training batch 78 with loss 0.30322\n",
      "------------------------------------------\n",
      "L1 loss: 0.10650990158319473\n",
      "Training batch 79 with loss 0.37473\n",
      "------------------------------------------\n",
      "L1 loss: 0.07437434047460556\n",
      "Training batch 80 with loss 0.28741\n",
      "------------------------------------------\n",
      "L1 loss: 0.08379784971475601\n",
      "Training batch 81 with loss 0.29972\n",
      "------------------------------------------\n",
      "L1 loss: 0.08444426208734512\n",
      "Training batch 82 with loss 0.35977\n",
      "------------------------------------------\n",
      "L1 loss: 0.07980972528457642\n",
      "Training batch 83 with loss 0.29638\n",
      "------------------------------------------\n",
      "L1 loss: 0.07248014956712723\n",
      "Training batch 84 with loss 0.30925\n",
      "------------------------------------------\n",
      "L1 loss: 0.06335010379552841\n",
      "Training batch 85 with loss 0.28349\n",
      "------------------------------------------\n",
      "L1 loss: 0.08042672276496887\n",
      "Training batch 86 with loss 0.28625\n",
      "------------------------------------------\n",
      "L1 loss: 0.10048022121191025\n",
      "Training batch 87 with loss 0.39327\n",
      "------------------------------------------\n",
      "L1 loss: 0.07789237797260284\n",
      "Training batch 88 with loss 0.31585\n",
      "------------------------------------------\n",
      "L1 loss: 0.06888855248689651\n",
      "Training batch 89 with loss 0.26332\n",
      "------------------------------------------\n",
      "L1 loss: 0.07342496514320374\n",
      "Training batch 90 with loss 0.27630\n",
      "------------------------------------------\n",
      "L1 loss: 0.06211996451020241\n",
      "Training batch 91 with loss 0.25120\n",
      "------------------------------------------\n",
      "L1 loss: 0.09237044304609299\n",
      "Training batch 92 with loss 0.29261\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0654534325003624\n",
      "Training batch 93 with loss 0.24621\n",
      "------------------------------------------\n",
      "L1 loss: 0.08111079037189484\n",
      "Training batch 94 with loss 0.30238\n",
      "------------------------------------------\n",
      "L1 loss: 0.07082489132881165\n",
      "Training batch 95 with loss 0.25630\n",
      "------------------------------------------\n",
      "L1 loss: 0.07053696364164352\n",
      "Training batch 96 with loss 0.29575\n",
      "------------------------------------------\n",
      "L1 loss: 0.08938688784837723\n",
      "Training batch 97 with loss 0.38284\n",
      "------------------------------------------\n",
      "L1 loss: 0.05694156885147095\n",
      "Training batch 98 with loss 0.24616\n",
      "------------------------------------------\n",
      "L1 loss: 0.06459378451108932\n",
      "Training batch 99 with loss 0.27459\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29056153059005735\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4868\n",
      "------------------------------------------\n",
      "L1 loss: 0.05995846539735794\n",
      "Training batch 0 with loss 0.26278\n",
      "------------------------------------------\n",
      "L1 loss: 0.08387508988380432\n",
      "Training batch 1 with loss 0.30173\n",
      "------------------------------------------\n",
      "L1 loss: 0.09500876069068909\n",
      "Training batch 2 with loss 0.32453\n",
      "------------------------------------------\n",
      "L1 loss: 0.07870578020811081\n",
      "Training batch 3 with loss 0.26758\n",
      "------------------------------------------\n",
      "L1 loss: 0.08495195209980011\n",
      "Training batch 4 with loss 0.32059\n",
      "------------------------------------------\n",
      "L1 loss: 0.06219729408621788\n",
      "Training batch 5 with loss 0.33492\n",
      "------------------------------------------\n",
      "L1 loss: 0.07763274013996124\n",
      "Training batch 6 with loss 0.26442\n",
      "------------------------------------------\n",
      "L1 loss: 0.08350040018558502\n",
      "Training batch 7 with loss 0.32273\n",
      "------------------------------------------\n",
      "L1 loss: 0.07876753062009811\n",
      "Training batch 8 with loss 0.31587\n",
      "------------------------------------------\n",
      "L1 loss: 0.05860908702015877\n",
      "Training batch 9 with loss 0.25502\n",
      "------------------------------------------\n",
      "L1 loss: 0.08206585049629211\n",
      "Training batch 10 with loss 0.28916\n",
      "------------------------------------------\n",
      "L1 loss: 0.06424079835414886\n",
      "Training batch 11 with loss 0.30046\n",
      "------------------------------------------\n",
      "L1 loss: 0.07467075437307358\n",
      "Training batch 12 with loss 0.30402\n",
      "------------------------------------------\n",
      "L1 loss: 0.06825537979602814\n",
      "Training batch 13 with loss 0.27490\n",
      "------------------------------------------\n",
      "L1 loss: 0.05306776240468025\n",
      "Training batch 14 with loss 0.24273\n",
      "------------------------------------------\n",
      "L1 loss: 0.08582160621881485\n",
      "Training batch 15 with loss 0.27577\n",
      "------------------------------------------\n",
      "L1 loss: 0.06452082842588425\n",
      "Training batch 16 with loss 0.26827\n",
      "------------------------------------------\n",
      "L1 loss: 0.08352555334568024\n",
      "Training batch 17 with loss 0.34827\n",
      "------------------------------------------\n",
      "L1 loss: 0.08006376773118973\n",
      "Training batch 18 with loss 0.31268\n",
      "------------------------------------------\n",
      "L1 loss: 0.06220794469118118\n",
      "Training batch 19 with loss 0.22664\n",
      "------------------------------------------\n",
      "L1 loss: 0.07395882904529572\n",
      "Training batch 20 with loss 0.32285\n",
      "------------------------------------------\n",
      "L1 loss: 0.0696479007601738\n",
      "Training batch 21 with loss 0.28369\n",
      "------------------------------------------\n",
      "L1 loss: 0.0682731494307518\n",
      "Training batch 22 with loss 0.28097\n",
      "------------------------------------------\n",
      "L1 loss: 0.09232477098703384\n",
      "Training batch 23 with loss 0.37929\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776096671819687\n",
      "Training batch 24 with loss 0.31696\n",
      "------------------------------------------\n",
      "L1 loss: 0.07591890543699265\n",
      "Training batch 25 with loss 0.30362\n",
      "------------------------------------------\n",
      "L1 loss: 0.07429031282663345\n",
      "Training batch 26 with loss 0.26961\n",
      "------------------------------------------\n",
      "L1 loss: 0.05680970102548599\n",
      "Training batch 27 with loss 0.28921\n",
      "------------------------------------------\n",
      "L1 loss: 0.0586126483976841\n",
      "Training batch 28 with loss 0.29611\n",
      "------------------------------------------\n",
      "L1 loss: 0.05525511875748634\n",
      "Training batch 29 with loss 0.23413\n",
      "------------------------------------------\n",
      "L1 loss: 0.06257506459951401\n",
      "Training batch 30 with loss 0.26998\n",
      "------------------------------------------\n",
      "L1 loss: 0.06416849792003632\n",
      "Training batch 31 with loss 0.30451\n",
      "------------------------------------------\n",
      "L1 loss: 0.07013686001300812\n",
      "Training batch 32 with loss 0.26632\n",
      "------------------------------------------\n",
      "L1 loss: 0.06438706070184708\n",
      "Training batch 33 with loss 0.27698\n",
      "------------------------------------------\n",
      "L1 loss: 0.076217420399189\n",
      "Training batch 34 with loss 0.34583\n",
      "------------------------------------------\n",
      "L1 loss: 0.09343164414167404\n",
      "Training batch 35 with loss 0.33900\n",
      "------------------------------------------\n",
      "L1 loss: 0.08331926167011261\n",
      "Training batch 36 with loss 0.27930\n",
      "------------------------------------------\n",
      "L1 loss: 0.06722554564476013\n",
      "Training batch 37 with loss 0.25989\n",
      "------------------------------------------\n",
      "L1 loss: 0.06226271390914917\n",
      "Training batch 38 with loss 0.21185\n",
      "------------------------------------------\n",
      "L1 loss: 0.06566493958234787\n",
      "Training batch 39 with loss 0.30491\n",
      "------------------------------------------\n",
      "L1 loss: 0.06607547402381897\n",
      "Training batch 40 with loss 0.24213\n",
      "------------------------------------------\n",
      "L1 loss: 0.05819234997034073\n",
      "Training batch 41 with loss 0.27558\n",
      "------------------------------------------\n",
      "L1 loss: 0.0767233669757843\n",
      "Training batch 42 with loss 0.27094\n",
      "------------------------------------------\n",
      "L1 loss: 0.06857883185148239\n",
      "Training batch 43 with loss 0.27125\n",
      "------------------------------------------\n",
      "L1 loss: 0.0552627258002758\n",
      "Training batch 44 with loss 0.26526\n",
      "------------------------------------------\n",
      "L1 loss: 0.09305021911859512\n",
      "Training batch 45 with loss 0.35750\n",
      "------------------------------------------\n",
      "L1 loss: 0.07523446530103683\n",
      "Training batch 46 with loss 0.26872\n",
      "------------------------------------------\n",
      "L1 loss: 0.11155792325735092\n",
      "Training batch 47 with loss 0.38523\n",
      "------------------------------------------\n",
      "L1 loss: 0.04861011356115341\n",
      "Training batch 48 with loss 0.24790\n",
      "------------------------------------------\n",
      "L1 loss: 0.0757594183087349\n",
      "Training batch 49 with loss 0.34924\n",
      "------------------------------------------\n",
      "L1 loss: 0.07466931641101837\n",
      "Training batch 50 with loss 0.30514\n",
      "------------------------------------------\n",
      "L1 loss: 0.08149249851703644\n",
      "Training batch 51 with loss 0.32192\n",
      "------------------------------------------\n",
      "L1 loss: 0.08973302692174911\n",
      "Training batch 52 with loss 0.29342\n",
      "------------------------------------------\n",
      "L1 loss: 0.07552304118871689\n",
      "Training batch 53 with loss 0.28575\n",
      "------------------------------------------\n",
      "L1 loss: 0.07244358956813812\n",
      "Training batch 54 with loss 0.26343\n",
      "------------------------------------------\n",
      "L1 loss: 0.06160754710435867\n",
      "Training batch 55 with loss 0.29382\n",
      "------------------------------------------\n",
      "L1 loss: 0.05772608891129494\n",
      "Training batch 56 with loss 0.25356\n",
      "------------------------------------------\n",
      "L1 loss: 0.06852025538682938\n",
      "Training batch 57 with loss 0.28280\n",
      "------------------------------------------\n",
      "L1 loss: 0.06695754081010818\n",
      "Training batch 58 with loss 0.25135\n",
      "------------------------------------------\n",
      "L1 loss: 0.07907434552907944\n",
      "Training batch 59 with loss 0.28386\n",
      "------------------------------------------\n",
      "L1 loss: 0.0661696344614029\n",
      "Training batch 60 with loss 0.27796\n",
      "------------------------------------------\n",
      "L1 loss: 0.100225068628788\n",
      "Training batch 61 with loss 0.36279\n",
      "------------------------------------------\n",
      "L1 loss: 0.055051736533641815\n",
      "Training batch 62 with loss 0.24680\n",
      "------------------------------------------\n",
      "L1 loss: 0.05955760180950165\n",
      "Training batch 63 with loss 0.25955\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990499049425125\n",
      "Training batch 64 with loss 0.27179\n",
      "------------------------------------------\n",
      "L1 loss: 0.06164924427866936\n",
      "Training batch 65 with loss 0.23416\n",
      "------------------------------------------\n",
      "L1 loss: 0.06550980359315872\n",
      "Training batch 66 with loss 0.26900\n",
      "------------------------------------------\n",
      "L1 loss: 0.06498567014932632\n",
      "Training batch 67 with loss 0.26145\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07222985476255417\n",
      "Training batch 68 with loss 0.29306\n",
      "------------------------------------------\n",
      "L1 loss: 0.052649691700935364\n",
      "Training batch 69 with loss 0.27694\n",
      "------------------------------------------\n",
      "L1 loss: 0.07634998112916946\n",
      "Training batch 70 with loss 0.29890\n",
      "------------------------------------------\n",
      "L1 loss: 0.0709775909781456\n",
      "Training batch 71 with loss 0.28296\n",
      "------------------------------------------\n",
      "L1 loss: 0.0879308208823204\n",
      "Training batch 72 with loss 0.32575\n",
      "------------------------------------------\n",
      "L1 loss: 0.0712469071149826\n",
      "Training batch 73 with loss 0.29904\n",
      "------------------------------------------\n",
      "L1 loss: 0.09448964893817902\n",
      "Training batch 74 with loss 0.32827\n",
      "------------------------------------------\n",
      "L1 loss: 0.07544812560081482\n",
      "Training batch 75 with loss 0.28182\n",
      "------------------------------------------\n",
      "L1 loss: 0.0860879197716713\n",
      "Training batch 76 with loss 0.29295\n",
      "------------------------------------------\n",
      "L1 loss: 0.08049864321947098\n",
      "Training batch 77 with loss 0.28248\n",
      "------------------------------------------\n",
      "L1 loss: 0.07922648638486862\n",
      "Training batch 78 with loss 0.30521\n",
      "------------------------------------------\n",
      "L1 loss: 0.1082339808344841\n",
      "Training batch 79 with loss 0.38271\n",
      "------------------------------------------\n",
      "L1 loss: 0.07627391070127487\n",
      "Training batch 80 with loss 0.28176\n",
      "------------------------------------------\n",
      "L1 loss: 0.0735979676246643\n",
      "Training batch 81 with loss 0.27712\n",
      "------------------------------------------\n",
      "L1 loss: 0.08574724942445755\n",
      "Training batch 82 with loss 0.37690\n",
      "------------------------------------------\n",
      "L1 loss: 0.07987514138221741\n",
      "Training batch 83 with loss 0.28740\n",
      "------------------------------------------\n",
      "L1 loss: 0.07256043702363968\n",
      "Training batch 84 with loss 0.30027\n",
      "------------------------------------------\n",
      "L1 loss: 0.05890161916613579\n",
      "Training batch 85 with loss 0.25548\n",
      "------------------------------------------\n",
      "L1 loss: 0.0805710181593895\n",
      "Training batch 86 with loss 0.30287\n",
      "------------------------------------------\n",
      "L1 loss: 0.10272836685180664\n",
      "Training batch 87 with loss 0.37871\n",
      "------------------------------------------\n",
      "L1 loss: 0.07387904077768326\n",
      "Training batch 88 with loss 0.29990\n",
      "------------------------------------------\n",
      "L1 loss: 0.06978010386228561\n",
      "Training batch 89 with loss 0.26589\n",
      "------------------------------------------\n",
      "L1 loss: 0.0726601704955101\n",
      "Training batch 90 with loss 0.27866\n",
      "------------------------------------------\n",
      "L1 loss: 0.06348589062690735\n",
      "Training batch 91 with loss 0.25444\n",
      "------------------------------------------\n",
      "L1 loss: 0.09100932627916336\n",
      "Training batch 92 with loss 0.29516\n",
      "------------------------------------------\n",
      "L1 loss: 0.05739220976829529\n",
      "Training batch 93 with loss 0.23921\n",
      "------------------------------------------\n",
      "L1 loss: 0.08655095100402832\n",
      "Training batch 94 with loss 0.31399\n",
      "------------------------------------------\n",
      "L1 loss: 0.07016635686159134\n",
      "Training batch 95 with loss 0.25071\n",
      "------------------------------------------\n",
      "L1 loss: 0.06445679813623428\n",
      "Training batch 96 with loss 0.29038\n",
      "------------------------------------------\n",
      "L1 loss: 0.08433308452367783\n",
      "Training batch 97 with loss 0.38203\n",
      "------------------------------------------\n",
      "L1 loss: 0.05421728640794754\n",
      "Training batch 98 with loss 0.37356\n",
      "------------------------------------------\n",
      "L1 loss: 0.06558834761381149\n",
      "Training batch 99 with loss 0.27534\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.292302310615778\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4869\n",
      "------------------------------------------\n",
      "L1 loss: 0.05581815168261528\n",
      "Training batch 0 with loss 0.22370\n",
      "------------------------------------------\n",
      "L1 loss: 0.08363624662160873\n",
      "Training batch 1 with loss 0.28266\n",
      "------------------------------------------\n",
      "L1 loss: 0.09370362758636475\n",
      "Training batch 2 with loss 0.31791\n",
      "------------------------------------------\n",
      "L1 loss: 0.06567898392677307\n",
      "Training batch 3 with loss 0.24262\n",
      "------------------------------------------\n",
      "L1 loss: 0.0843510627746582\n",
      "Training batch 4 with loss 0.32264\n",
      "------------------------------------------\n",
      "L1 loss: 0.07093518227338791\n",
      "Training batch 5 with loss 0.33714\n",
      "------------------------------------------\n",
      "L1 loss: 0.07842183113098145\n",
      "Training batch 6 with loss 0.26750\n",
      "------------------------------------------\n",
      "L1 loss: 0.08372321724891663\n",
      "Training batch 7 with loss 0.34303\n",
      "------------------------------------------\n",
      "L1 loss: 0.0772317498922348\n",
      "Training batch 8 with loss 0.32163\n",
      "------------------------------------------\n",
      "L1 loss: 0.05914418399333954\n",
      "Training batch 9 with loss 0.25904\n",
      "------------------------------------------\n",
      "L1 loss: 0.08321703970432281\n",
      "Training batch 10 with loss 0.28656\n",
      "------------------------------------------\n",
      "L1 loss: 0.060294318944215775\n",
      "Training batch 11 with loss 0.28415\n",
      "------------------------------------------\n",
      "L1 loss: 0.07429308444261551\n",
      "Training batch 12 with loss 0.31451\n",
      "------------------------------------------\n",
      "L1 loss: 0.06944700330495834\n",
      "Training batch 13 with loss 0.24037\n",
      "------------------------------------------\n",
      "L1 loss: 0.05316406860947609\n",
      "Training batch 14 with loss 0.24563\n",
      "------------------------------------------\n",
      "L1 loss: 0.08308568596839905\n",
      "Training batch 15 with loss 0.27058\n",
      "------------------------------------------\n",
      "L1 loss: 0.06306338310241699\n",
      "Training batch 16 with loss 0.25131\n",
      "------------------------------------------\n",
      "L1 loss: 0.0881015732884407\n",
      "Training batch 17 with loss 0.34221\n",
      "------------------------------------------\n",
      "L1 loss: 0.08038019388914108\n",
      "Training batch 18 with loss 0.29099\n",
      "------------------------------------------\n",
      "L1 loss: 0.060746289789676666\n",
      "Training batch 19 with loss 0.23981\n",
      "------------------------------------------\n",
      "L1 loss: 0.0720873475074768\n",
      "Training batch 20 with loss 0.31590\n",
      "------------------------------------------\n",
      "L1 loss: 0.07097238302230835\n",
      "Training batch 21 with loss 0.26597\n",
      "------------------------------------------\n",
      "L1 loss: 0.06889666616916656\n",
      "Training batch 22 with loss 0.27376\n",
      "------------------------------------------\n",
      "L1 loss: 0.09041959047317505\n",
      "Training batch 23 with loss 0.35394\n",
      "------------------------------------------\n",
      "L1 loss: 0.06608052551746368\n",
      "Training batch 24 with loss 0.29797\n",
      "------------------------------------------\n",
      "L1 loss: 0.0769992247223854\n",
      "Training batch 25 with loss 0.29838\n",
      "------------------------------------------\n",
      "L1 loss: 0.07370856404304504\n",
      "Training batch 26 with loss 0.26642\n",
      "------------------------------------------\n",
      "L1 loss: 0.055192746222019196\n",
      "Training batch 27 with loss 0.29947\n",
      "------------------------------------------\n",
      "L1 loss: 0.058721594512462616\n",
      "Training batch 28 with loss 0.29620\n",
      "------------------------------------------\n",
      "L1 loss: 0.05577699467539787\n",
      "Training batch 29 with loss 0.24541\n",
      "------------------------------------------\n",
      "L1 loss: 0.059729959815740585\n",
      "Training batch 30 with loss 0.27338\n",
      "------------------------------------------\n",
      "L1 loss: 0.06363200396299362\n",
      "Training batch 31 with loss 0.28483\n",
      "------------------------------------------\n",
      "L1 loss: 0.06880997121334076\n",
      "Training batch 32 with loss 0.28979\n",
      "------------------------------------------\n",
      "L1 loss: 0.06250018626451492\n",
      "Training batch 33 with loss 0.25636\n",
      "------------------------------------------\n",
      "L1 loss: 0.07425765693187714\n",
      "Training batch 34 with loss 0.30090\n",
      "------------------------------------------\n",
      "L1 loss: 0.09535697102546692\n",
      "Training batch 35 with loss 0.33463\n",
      "------------------------------------------\n",
      "L1 loss: 0.08775769174098969\n",
      "Training batch 36 with loss 0.27935\n",
      "------------------------------------------\n",
      "L1 loss: 0.07077398896217346\n",
      "Training batch 37 with loss 0.27801\n",
      "------------------------------------------\n",
      "L1 loss: 0.0640803724527359\n",
      "Training batch 38 with loss 0.23711\n",
      "------------------------------------------\n",
      "L1 loss: 0.0647997260093689\n",
      "Training batch 39 with loss 0.30280\n",
      "------------------------------------------\n",
      "L1 loss: 0.07214243710041046\n",
      "Training batch 40 with loss 0.26118\n",
      "------------------------------------------\n",
      "L1 loss: 0.059831589460372925\n",
      "Training batch 41 with loss 0.28210\n",
      "------------------------------------------\n",
      "L1 loss: 0.08024062216281891\n",
      "Training batch 42 with loss 0.27537\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07077949494123459\n",
      "Training batch 43 with loss 0.28000\n",
      "------------------------------------------\n",
      "L1 loss: 0.05729830637574196\n",
      "Training batch 44 with loss 0.25825\n",
      "------------------------------------------\n",
      "L1 loss: 0.09273604303598404\n",
      "Training batch 45 with loss 0.37732\n",
      "------------------------------------------\n",
      "L1 loss: 0.07184918224811554\n",
      "Training batch 46 with loss 0.27064\n",
      "------------------------------------------\n",
      "L1 loss: 0.10969560593366623\n",
      "Training batch 47 with loss 0.37203\n",
      "------------------------------------------\n",
      "L1 loss: 0.04968292638659477\n",
      "Training batch 48 with loss 0.24475\n",
      "------------------------------------------\n",
      "L1 loss: 0.06141841411590576\n",
      "Training batch 49 with loss 0.33732\n",
      "------------------------------------------\n",
      "L1 loss: 0.07189077138900757\n",
      "Training batch 50 with loss 0.28858\n",
      "------------------------------------------\n",
      "L1 loss: 0.0836876705288887\n",
      "Training batch 51 with loss 0.33259\n",
      "------------------------------------------\n",
      "L1 loss: 0.08922082185745239\n",
      "Training batch 52 with loss 0.29448\n",
      "------------------------------------------\n",
      "L1 loss: 0.0799500122666359\n",
      "Training batch 53 with loss 0.28259\n",
      "------------------------------------------\n",
      "L1 loss: 0.07696973532438278\n",
      "Training batch 54 with loss 0.26680\n",
      "------------------------------------------\n",
      "L1 loss: 0.05785297602415085\n",
      "Training batch 55 with loss 0.29993\n",
      "------------------------------------------\n",
      "L1 loss: 0.05034457519650459\n",
      "Training batch 56 with loss 0.22699\n",
      "------------------------------------------\n",
      "L1 loss: 0.0673002302646637\n",
      "Training batch 57 with loss 0.27966\n",
      "------------------------------------------\n",
      "L1 loss: 0.06622902303934097\n",
      "Training batch 58 with loss 0.26346\n",
      "------------------------------------------\n",
      "L1 loss: 0.08098888397216797\n",
      "Training batch 59 with loss 0.29682\n",
      "------------------------------------------\n",
      "L1 loss: 0.06473688781261444\n",
      "Training batch 60 with loss 0.27963\n",
      "------------------------------------------\n",
      "L1 loss: 0.0984402596950531\n",
      "Training batch 61 with loss 0.35987\n",
      "------------------------------------------\n",
      "L1 loss: 0.05628494545817375\n",
      "Training batch 62 with loss 0.25225\n",
      "------------------------------------------\n",
      "L1 loss: 0.06206909567117691\n",
      "Training batch 63 with loss 0.24584\n",
      "------------------------------------------\n",
      "L1 loss: 0.07003206014633179\n",
      "Training batch 64 with loss 0.27173\n",
      "------------------------------------------\n",
      "L1 loss: 0.062292568385601044\n",
      "Training batch 65 with loss 0.21701\n",
      "------------------------------------------\n",
      "L1 loss: 0.06862607598304749\n",
      "Training batch 66 with loss 0.28430\n",
      "------------------------------------------\n",
      "L1 loss: 0.06397753208875656\n",
      "Training batch 67 with loss 0.26328\n",
      "------------------------------------------\n",
      "L1 loss: 0.07307758927345276\n",
      "Training batch 68 with loss 0.30338\n",
      "------------------------------------------\n",
      "L1 loss: 0.051220860332250595\n",
      "Training batch 69 with loss 0.26816\n",
      "------------------------------------------\n",
      "L1 loss: 0.07845783978700638\n",
      "Training batch 70 with loss 0.30871\n",
      "------------------------------------------\n",
      "L1 loss: 0.07125882059335709\n",
      "Training batch 71 with loss 0.32271\n",
      "------------------------------------------\n",
      "L1 loss: 0.08482974022626877\n",
      "Training batch 72 with loss 0.29989\n",
      "------------------------------------------\n",
      "L1 loss: 0.07139135152101517\n",
      "Training batch 73 with loss 0.31296\n",
      "------------------------------------------\n",
      "L1 loss: 0.0958726555109024\n",
      "Training batch 74 with loss 0.33198\n",
      "------------------------------------------\n",
      "L1 loss: 0.0739993005990982\n",
      "Training batch 75 with loss 0.27932\n",
      "------------------------------------------\n",
      "L1 loss: 0.08644907921552658\n",
      "Training batch 76 with loss 0.29795\n",
      "------------------------------------------\n",
      "L1 loss: 0.07641544938087463\n",
      "Training batch 77 with loss 0.26936\n",
      "------------------------------------------\n",
      "L1 loss: 0.0847737267613411\n",
      "Training batch 78 with loss 0.30317\n",
      "------------------------------------------\n",
      "L1 loss: 0.10754217952489853\n",
      "Training batch 79 with loss 0.36992\n",
      "------------------------------------------\n",
      "L1 loss: 0.07629270851612091\n",
      "Training batch 80 with loss 0.27550\n",
      "------------------------------------------\n",
      "L1 loss: 0.08316601812839508\n",
      "Training batch 81 with loss 0.31299\n",
      "------------------------------------------\n",
      "L1 loss: 0.08455723524093628\n",
      "Training batch 82 with loss 0.34273\n",
      "------------------------------------------\n",
      "L1 loss: 0.07310650497674942\n",
      "Training batch 83 with loss 0.28988\n",
      "------------------------------------------\n",
      "L1 loss: 0.07022760808467865\n",
      "Training batch 84 with loss 0.29022\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394367665052414\n",
      "Training batch 85 with loss 0.27404\n",
      "------------------------------------------\n",
      "L1 loss: 0.08276757597923279\n",
      "Training batch 86 with loss 0.28523\n",
      "------------------------------------------\n",
      "L1 loss: 0.10152361541986465\n",
      "Training batch 87 with loss 0.41647\n",
      "------------------------------------------\n",
      "L1 loss: 0.07547664642333984\n",
      "Training batch 88 with loss 0.29153\n",
      "------------------------------------------\n",
      "L1 loss: 0.0683676153421402\n",
      "Training batch 89 with loss 0.26357\n",
      "------------------------------------------\n",
      "L1 loss: 0.074673593044281\n",
      "Training batch 90 with loss 0.27922\n",
      "------------------------------------------\n",
      "L1 loss: 0.05938727408647537\n",
      "Training batch 91 with loss 0.25110\n",
      "------------------------------------------\n",
      "L1 loss: 0.09374698996543884\n",
      "Training batch 92 with loss 0.29717\n",
      "------------------------------------------\n",
      "L1 loss: 0.045207757502794266\n",
      "Training batch 93 with loss 0.24795\n",
      "------------------------------------------\n",
      "L1 loss: 0.08501897752285004\n",
      "Training batch 94 with loss 0.30041\n",
      "------------------------------------------\n",
      "L1 loss: 0.07040730863809586\n",
      "Training batch 95 with loss 0.25577\n",
      "------------------------------------------\n",
      "L1 loss: 0.06995392590761185\n",
      "Training batch 96 with loss 0.31445\n",
      "------------------------------------------\n",
      "L1 loss: 0.08843730390071869\n",
      "Training batch 97 with loss 0.37236\n",
      "------------------------------------------\n",
      "L1 loss: 0.05794341862201691\n",
      "Training batch 98 with loss 0.23844\n",
      "------------------------------------------\n",
      "L1 loss: 0.06438976526260376\n",
      "Training batch 99 with loss 0.28176\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.28947646886110306\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4870\n",
      "------------------------------------------\n",
      "L1 loss: 0.062251776456832886\n",
      "Training batch 0 with loss 0.24229\n",
      "------------------------------------------\n",
      "L1 loss: 0.08629041165113449\n",
      "Training batch 1 with loss 0.29894\n",
      "------------------------------------------\n",
      "L1 loss: 0.09024889767169952\n",
      "Training batch 2 with loss 0.31242\n",
      "------------------------------------------\n",
      "L1 loss: 0.064618319272995\n",
      "Training batch 3 with loss 0.25166\n",
      "------------------------------------------\n",
      "L1 loss: 0.0816928818821907\n",
      "Training batch 4 with loss 0.32260\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776803731918335\n",
      "Training batch 5 with loss 0.34692\n",
      "------------------------------------------\n",
      "L1 loss: 0.07935875654220581\n",
      "Training batch 6 with loss 0.27216\n",
      "------------------------------------------\n",
      "L1 loss: 0.08307985961437225\n",
      "Training batch 7 with loss 0.33395\n",
      "------------------------------------------\n",
      "L1 loss: 0.08182082325220108\n",
      "Training batch 8 with loss 0.34904\n",
      "------------------------------------------\n",
      "L1 loss: 0.059174053370952606\n",
      "Training batch 9 with loss 0.24246\n",
      "------------------------------------------\n",
      "L1 loss: 0.08131679147481918\n",
      "Training batch 10 with loss 0.29739\n",
      "------------------------------------------\n",
      "L1 loss: 0.059764448553323746\n",
      "Training batch 11 with loss 0.27449\n",
      "------------------------------------------\n",
      "L1 loss: 0.07727383077144623\n",
      "Training batch 12 with loss 0.32237\n",
      "------------------------------------------\n",
      "L1 loss: 0.07065973430871964\n",
      "Training batch 13 with loss 0.26043\n",
      "------------------------------------------\n",
      "L1 loss: 0.052159275859594345\n",
      "Training batch 14 with loss 0.24803\n",
      "------------------------------------------\n",
      "L1 loss: 0.08271107822656631\n",
      "Training batch 15 with loss 0.27148\n",
      "------------------------------------------\n",
      "L1 loss: 0.060777489095926285\n",
      "Training batch 16 with loss 0.27458\n",
      "------------------------------------------\n",
      "L1 loss: 0.08128470182418823\n",
      "Training batch 17 with loss 0.31232\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08103998005390167\n",
      "Training batch 18 with loss 0.30504\n",
      "------------------------------------------\n",
      "L1 loss: 0.06302322447299957\n",
      "Training batch 19 with loss 0.23708\n",
      "------------------------------------------\n",
      "L1 loss: 0.07371152937412262\n",
      "Training batch 20 with loss 0.33262\n",
      "------------------------------------------\n",
      "L1 loss: 0.0720089003443718\n",
      "Training batch 21 with loss 0.27283\n",
      "------------------------------------------\n",
      "L1 loss: 0.06823068112134933\n",
      "Training batch 22 with loss 0.29184\n",
      "------------------------------------------\n",
      "L1 loss: 0.08822386711835861\n",
      "Training batch 23 with loss 0.36429\n",
      "------------------------------------------\n",
      "L1 loss: 0.06917478144168854\n",
      "Training batch 24 with loss 0.29288\n",
      "------------------------------------------\n",
      "L1 loss: 0.07438172399997711\n",
      "Training batch 25 with loss 0.30169\n",
      "------------------------------------------\n",
      "L1 loss: 0.07702631503343582\n",
      "Training batch 26 with loss 0.28399\n",
      "------------------------------------------\n",
      "L1 loss: 0.05758048966526985\n",
      "Training batch 27 with loss 0.31360\n",
      "------------------------------------------\n",
      "L1 loss: 0.05910477414727211\n",
      "Training batch 28 with loss 0.29708\n",
      "------------------------------------------\n",
      "L1 loss: 0.06042415276169777\n",
      "Training batch 29 with loss 0.24236\n",
      "------------------------------------------\n",
      "L1 loss: 0.058588769286870956\n",
      "Training batch 30 with loss 0.26023\n",
      "------------------------------------------\n",
      "L1 loss: 0.06330766528844833\n",
      "Training batch 31 with loss 0.29367\n",
      "------------------------------------------\n",
      "L1 loss: 0.06920696794986725\n",
      "Training batch 32 with loss 0.28424\n",
      "------------------------------------------\n",
      "L1 loss: 0.06573519855737686\n",
      "Training batch 33 with loss 0.28515\n",
      "------------------------------------------\n",
      "L1 loss: 0.07386229187250137\n",
      "Training batch 34 with loss 0.31775\n",
      "------------------------------------------\n",
      "L1 loss: 0.09492974728345871\n",
      "Training batch 35 with loss 0.31828\n",
      "------------------------------------------\n",
      "L1 loss: 0.08281050622463226\n",
      "Training batch 36 with loss 0.27784\n",
      "------------------------------------------\n",
      "L1 loss: 0.07027620077133179\n",
      "Training batch 37 with loss 0.26705\n",
      "------------------------------------------\n",
      "L1 loss: 0.0620918944478035\n",
      "Training batch 38 with loss 0.21833\n",
      "------------------------------------------\n",
      "L1 loss: 0.06879092007875443\n",
      "Training batch 39 with loss 0.31265\n",
      "------------------------------------------\n",
      "L1 loss: 0.07446792721748352\n",
      "Training batch 40 with loss 0.44163\n",
      "------------------------------------------\n",
      "L1 loss: 0.05846056342124939\n",
      "Training batch 41 with loss 0.28819\n",
      "------------------------------------------\n",
      "L1 loss: 0.0761919915676117\n",
      "Training batch 42 with loss 0.27338\n",
      "------------------------------------------\n",
      "L1 loss: 0.0691291019320488\n",
      "Training batch 43 with loss 0.26740\n",
      "------------------------------------------\n",
      "L1 loss: 0.05561012029647827\n",
      "Training batch 44 with loss 0.24587\n",
      "------------------------------------------\n",
      "L1 loss: 0.09285569936037064\n",
      "Training batch 45 with loss 0.36838\n",
      "------------------------------------------\n",
      "L1 loss: 0.0708804801106453\n",
      "Training batch 46 with loss 0.26572\n",
      "------------------------------------------\n",
      "L1 loss: 0.11011595278978348\n",
      "Training batch 47 with loss 0.35357\n",
      "------------------------------------------\n",
      "L1 loss: 0.04385349527001381\n",
      "Training batch 48 with loss 0.23414\n",
      "------------------------------------------\n",
      "L1 loss: 0.08197883516550064\n",
      "Training batch 49 with loss 0.34911\n",
      "------------------------------------------\n",
      "L1 loss: 0.07050816714763641\n",
      "Training batch 50 with loss 0.28598\n",
      "------------------------------------------\n",
      "L1 loss: 0.08445921540260315\n",
      "Training batch 51 with loss 0.35697\n",
      "------------------------------------------\n",
      "L1 loss: 0.08753160387277603\n",
      "Training batch 52 with loss 0.29110\n",
      "------------------------------------------\n",
      "L1 loss: 0.07994522899389267\n",
      "Training batch 53 with loss 0.28744\n",
      "------------------------------------------\n",
      "L1 loss: 0.07069483399391174\n",
      "Training batch 54 with loss 0.25877\n",
      "------------------------------------------\n",
      "L1 loss: 0.06020915135741234\n",
      "Training batch 55 with loss 0.29661\n",
      "------------------------------------------\n",
      "L1 loss: 0.05600655451416969\n",
      "Training batch 56 with loss 0.24723\n",
      "------------------------------------------\n",
      "L1 loss: 0.06745737046003342\n",
      "Training batch 57 with loss 0.28758\n",
      "------------------------------------------\n",
      "L1 loss: 0.06772129982709885\n",
      "Training batch 58 with loss 0.26252\n",
      "------------------------------------------\n",
      "L1 loss: 0.0791514664888382\n",
      "Training batch 59 with loss 0.29824\n",
      "------------------------------------------\n",
      "L1 loss: 0.06625889986753464\n",
      "Training batch 60 with loss 0.27236\n",
      "------------------------------------------\n",
      "L1 loss: 0.10044939815998077\n",
      "Training batch 61 with loss 0.34790\n",
      "------------------------------------------\n",
      "L1 loss: 0.056647416204214096\n",
      "Training batch 62 with loss 0.25623\n",
      "------------------------------------------\n",
      "L1 loss: 0.06210501119494438\n",
      "Training batch 63 with loss 0.26669\n",
      "------------------------------------------\n",
      "L1 loss: 0.07059171795845032\n",
      "Training batch 64 with loss 0.27843\n",
      "------------------------------------------\n",
      "L1 loss: 0.06072235107421875\n",
      "Training batch 65 with loss 0.24366\n",
      "------------------------------------------\n",
      "L1 loss: 0.061960864812135696\n",
      "Training batch 66 with loss 0.26362\n",
      "------------------------------------------\n",
      "L1 loss: 0.06561894714832306\n",
      "Training batch 67 with loss 0.26949\n",
      "------------------------------------------\n",
      "L1 loss: 0.07292105257511139\n",
      "Training batch 68 with loss 0.30326\n",
      "------------------------------------------\n",
      "L1 loss: 0.04863185063004494\n",
      "Training batch 69 with loss 0.23592\n",
      "------------------------------------------\n",
      "L1 loss: 0.07737646996974945\n",
      "Training batch 70 with loss 0.30408\n",
      "------------------------------------------\n",
      "L1 loss: 0.07221344858407974\n",
      "Training batch 71 with loss 0.29027\n",
      "------------------------------------------\n",
      "L1 loss: 0.08184632658958435\n",
      "Training batch 72 with loss 0.30176\n",
      "------------------------------------------\n",
      "L1 loss: 0.07163941115140915\n",
      "Training batch 73 with loss 0.30712\n",
      "------------------------------------------\n",
      "L1 loss: 0.09861139208078384\n",
      "Training batch 74 with loss 0.31107\n",
      "------------------------------------------\n",
      "L1 loss: 0.07441463321447372\n",
      "Training batch 75 with loss 0.27511\n",
      "------------------------------------------\n",
      "L1 loss: 0.08678070455789566\n",
      "Training batch 76 with loss 0.29613\n",
      "------------------------------------------\n",
      "L1 loss: 0.07774470001459122\n",
      "Training batch 77 with loss 0.27028\n",
      "------------------------------------------\n",
      "L1 loss: 0.07772205024957657\n",
      "Training batch 78 with loss 0.30554\n",
      "------------------------------------------\n",
      "L1 loss: 0.10585315525531769\n",
      "Training batch 79 with loss 0.36052\n",
      "------------------------------------------\n",
      "L1 loss: 0.07637125253677368\n",
      "Training batch 80 with loss 0.29151\n",
      "------------------------------------------\n",
      "L1 loss: 0.08127222955226898\n",
      "Training batch 81 with loss 0.30100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08482106029987335\n",
      "Training batch 82 with loss 0.36837\n",
      "------------------------------------------\n",
      "L1 loss: 0.07771115005016327\n",
      "Training batch 83 with loss 0.29523\n",
      "------------------------------------------\n",
      "L1 loss: 0.07237731665372849\n",
      "Training batch 84 with loss 0.30696\n",
      "------------------------------------------\n",
      "L1 loss: 0.061692625284194946\n",
      "Training batch 85 with loss 0.27244\n",
      "------------------------------------------\n",
      "L1 loss: 0.07927440851926804\n",
      "Training batch 86 with loss 0.29138\n",
      "------------------------------------------\n",
      "L1 loss: 0.0843401551246643\n",
      "Training batch 87 with loss 0.45246\n",
      "------------------------------------------\n",
      "L1 loss: 0.07626048475503922\n",
      "Training batch 88 with loss 0.31256\n",
      "------------------------------------------\n",
      "L1 loss: 0.06922347098588943\n",
      "Training batch 89 with loss 0.25769\n",
      "------------------------------------------\n",
      "L1 loss: 0.07682619243860245\n",
      "Training batch 90 with loss 0.28533\n",
      "------------------------------------------\n",
      "L1 loss: 0.061674781143665314\n",
      "Training batch 91 with loss 0.25724\n",
      "------------------------------------------\n",
      "L1 loss: 0.09381873905658722\n",
      "Training batch 92 with loss 0.28732\n",
      "------------------------------------------\n",
      "L1 loss: 0.06707821041345596\n",
      "Training batch 93 with loss 0.25297\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.0864986926317215\n",
      "Training batch 94 with loss 0.30723\n",
      "------------------------------------------\n",
      "L1 loss: 0.07414962351322174\n",
      "Training batch 95 with loss 0.26155\n",
      "------------------------------------------\n",
      "L1 loss: 0.05873535946011543\n",
      "Training batch 96 with loss 0.26046\n",
      "------------------------------------------\n",
      "L1 loss: 0.08638802170753479\n",
      "Training batch 97 with loss 0.36024\n",
      "------------------------------------------\n",
      "L1 loss: 0.0555899553000927\n",
      "Training batch 98 with loss 0.24392\n",
      "------------------------------------------\n",
      "L1 loss: 0.06416851282119751\n",
      "Training batch 99 with loss 0.24959\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29238709568977356\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4871\n",
      "------------------------------------------\n",
      "L1 loss: 0.06237175688147545\n",
      "Training batch 0 with loss 0.24963\n",
      "------------------------------------------\n",
      "L1 loss: 0.08204630017280579\n",
      "Training batch 1 with loss 0.30336\n",
      "------------------------------------------\n",
      "L1 loss: 0.09304017573595047\n",
      "Training batch 2 with loss 0.31619\n",
      "------------------------------------------\n",
      "L1 loss: 0.07536125928163528\n",
      "Training batch 3 with loss 0.25827\n",
      "------------------------------------------\n",
      "L1 loss: 0.08499617129564285\n",
      "Training batch 4 with loss 0.31976\n",
      "------------------------------------------\n",
      "L1 loss: 0.06734520196914673\n",
      "Training batch 5 with loss 0.30220\n",
      "------------------------------------------\n",
      "L1 loss: 0.08087873458862305\n",
      "Training batch 6 with loss 0.28392\n",
      "------------------------------------------\n",
      "L1 loss: 0.0836210772395134\n",
      "Training batch 7 with loss 0.32862\n",
      "------------------------------------------\n",
      "L1 loss: 0.07947522401809692\n",
      "Training batch 8 with loss 0.31264\n",
      "------------------------------------------\n",
      "L1 loss: 0.054572466760873795\n",
      "Training batch 9 with loss 0.27506\n",
      "------------------------------------------\n",
      "L1 loss: 0.08311863243579865\n",
      "Training batch 10 with loss 0.30087\n",
      "------------------------------------------\n",
      "L1 loss: 0.05887575447559357\n",
      "Training batch 11 with loss 0.30041\n",
      "------------------------------------------\n",
      "L1 loss: 0.07306098937988281\n",
      "Training batch 12 with loss 0.32519\n",
      "------------------------------------------\n",
      "L1 loss: 0.06729618459939957\n",
      "Training batch 13 with loss 0.25319\n",
      "------------------------------------------\n",
      "L1 loss: 0.052700866013765335\n",
      "Training batch 14 with loss 0.23243\n",
      "------------------------------------------\n",
      "L1 loss: 0.08278316259384155\n",
      "Training batch 15 with loss 0.27521\n",
      "------------------------------------------\n",
      "L1 loss: 0.06337188184261322\n",
      "Training batch 16 with loss 0.27300\n",
      "------------------------------------------\n",
      "L1 loss: 0.08989856392145157\n",
      "Training batch 17 with loss 0.35758\n",
      "------------------------------------------\n",
      "L1 loss: 0.07714667916297913\n",
      "Training batch 18 with loss 0.49028\n",
      "------------------------------------------\n",
      "L1 loss: 0.05942986160516739\n",
      "Training batch 19 with loss 0.22482\n",
      "------------------------------------------\n",
      "L1 loss: 0.07117508351802826\n",
      "Training batch 20 with loss 0.31248\n",
      "------------------------------------------\n",
      "L1 loss: 0.07381191849708557\n",
      "Training batch 21 with loss 0.31347\n",
      "------------------------------------------\n",
      "L1 loss: 0.06858839094638824\n",
      "Training batch 22 with loss 0.28404\n",
      "------------------------------------------\n",
      "L1 loss: 0.09058509021997452\n",
      "Training batch 23 with loss 0.35517\n",
      "------------------------------------------\n",
      "L1 loss: 0.06902279704809189\n",
      "Training batch 24 with loss 0.29830\n",
      "------------------------------------------\n",
      "L1 loss: 0.07336591184139252\n",
      "Training batch 25 with loss 0.28941\n",
      "------------------------------------------\n",
      "L1 loss: 0.07782901078462601\n",
      "Training batch 26 with loss 0.27821\n",
      "------------------------------------------\n",
      "L1 loss: 0.055899303406476974\n",
      "Training batch 27 with loss 0.29245\n",
      "------------------------------------------\n",
      "L1 loss: 0.058327190577983856\n",
      "Training batch 28 with loss 0.29314\n",
      "------------------------------------------\n",
      "L1 loss: 0.05395353212952614\n",
      "Training batch 29 with loss 0.23165\n",
      "------------------------------------------\n",
      "L1 loss: 0.061970967799425125\n",
      "Training batch 30 with loss 0.27702\n",
      "------------------------------------------\n",
      "L1 loss: 0.06719765812158585\n",
      "Training batch 31 with loss 0.30648\n",
      "------------------------------------------\n",
      "L1 loss: 0.06993626058101654\n",
      "Training batch 32 with loss 0.28859\n",
      "------------------------------------------\n",
      "L1 loss: 0.06362315267324448\n",
      "Training batch 33 with loss 0.27352\n",
      "------------------------------------------\n",
      "L1 loss: 0.07499442249536514\n",
      "Training batch 34 with loss 0.30805\n",
      "------------------------------------------\n",
      "L1 loss: 0.09234723448753357\n",
      "Training batch 35 with loss 0.33908\n",
      "------------------------------------------\n",
      "L1 loss: 0.06967506557703018\n",
      "Training batch 36 with loss 0.35814\n",
      "------------------------------------------\n",
      "L1 loss: 0.06818578392267227\n",
      "Training batch 37 with loss 0.26741\n",
      "------------------------------------------\n",
      "L1 loss: 0.06260672956705093\n",
      "Training batch 38 with loss 0.22355\n",
      "------------------------------------------\n",
      "L1 loss: 0.06887611001729965\n",
      "Training batch 39 with loss 0.29809\n",
      "------------------------------------------\n",
      "L1 loss: 0.07240766286849976\n",
      "Training batch 40 with loss 0.26452\n",
      "------------------------------------------\n",
      "L1 loss: 0.05864764004945755\n",
      "Training batch 41 with loss 0.29636\n",
      "------------------------------------------\n",
      "L1 loss: 0.08064261823892593\n",
      "Training batch 42 with loss 0.28428\n",
      "------------------------------------------\n",
      "L1 loss: 0.07125077396631241\n",
      "Training batch 43 with loss 0.28534\n",
      "------------------------------------------\n",
      "L1 loss: 0.05893142521381378\n",
      "Training batch 44 with loss 0.26264\n",
      "------------------------------------------\n",
      "L1 loss: 0.09263510257005692\n",
      "Training batch 45 with loss 0.36261\n",
      "------------------------------------------\n",
      "L1 loss: 0.07120984047651291\n",
      "Training batch 46 with loss 0.27465\n",
      "------------------------------------------\n",
      "L1 loss: 0.11048923432826996\n",
      "Training batch 47 with loss 0.37044\n",
      "------------------------------------------\n",
      "L1 loss: 0.03196507319808006\n",
      "Training batch 48 with loss 0.17725\n",
      "------------------------------------------\n",
      "L1 loss: 0.07894517481327057\n",
      "Training batch 49 with loss 0.34466\n",
      "------------------------------------------\n",
      "L1 loss: 0.07147875428199768\n",
      "Training batch 50 with loss 0.28526\n",
      "------------------------------------------\n",
      "L1 loss: 0.08168868720531464\n",
      "Training batch 51 with loss 0.32571\n",
      "------------------------------------------\n",
      "L1 loss: 0.0889136865735054\n",
      "Training batch 52 with loss 0.28539\n",
      "------------------------------------------\n",
      "L1 loss: 0.08063321560621262\n",
      "Training batch 53 with loss 0.28400\n",
      "------------------------------------------\n",
      "L1 loss: 0.07163157314062119\n",
      "Training batch 54 with loss 0.26452\n",
      "------------------------------------------\n",
      "L1 loss: 0.06216200068593025\n",
      "Training batch 55 with loss 0.33270\n",
      "------------------------------------------\n",
      "L1 loss: 0.05773022398352623\n",
      "Training batch 56 with loss 0.26052\n",
      "------------------------------------------\n",
      "L1 loss: 0.06835811585187912\n",
      "Training batch 57 with loss 0.28244\n",
      "------------------------------------------\n",
      "L1 loss: 0.06440754979848862\n",
      "Training batch 58 with loss 0.26773\n",
      "------------------------------------------\n",
      "L1 loss: 0.07777081429958344\n",
      "Training batch 59 with loss 0.28699\n",
      "------------------------------------------\n",
      "L1 loss: 0.06332053989171982\n",
      "Training batch 60 with loss 0.27083\n",
      "------------------------------------------\n",
      "L1 loss: 0.09156885743141174\n",
      "Training batch 61 with loss 0.32492\n",
      "------------------------------------------\n",
      "L1 loss: 0.05564069747924805\n",
      "Training batch 62 with loss 0.24827\n",
      "------------------------------------------\n",
      "L1 loss: 0.05794566869735718\n",
      "Training batch 63 with loss 0.24981\n",
      "------------------------------------------\n",
      "L1 loss: 0.06817512959241867\n",
      "Training batch 64 with loss 0.27558\n",
      "------------------------------------------\n",
      "L1 loss: 0.06268008798360825\n",
      "Training batch 65 with loss 0.23901\n",
      "------------------------------------------\n",
      "L1 loss: 0.06678149849176407\n",
      "Training batch 66 with loss 0.28224\n",
      "------------------------------------------\n",
      "L1 loss: 0.0666566714644432\n",
      "Training batch 67 with loss 0.27274\n",
      "------------------------------------------\n",
      "L1 loss: 0.07369951158761978\n",
      "Training batch 68 with loss 0.30105\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.05031919479370117\n",
      "Training batch 69 with loss 0.25502\n",
      "------------------------------------------\n",
      "L1 loss: 0.08259054273366928\n",
      "Training batch 70 with loss 0.31741\n",
      "------------------------------------------\n",
      "L1 loss: 0.07162395864725113\n",
      "Training batch 71 with loss 0.31565\n",
      "------------------------------------------\n",
      "L1 loss: 0.08744356781244278\n",
      "Training batch 72 with loss 0.32828\n",
      "------------------------------------------\n",
      "L1 loss: 0.0731864720582962\n",
      "Training batch 73 with loss 0.30139\n",
      "------------------------------------------\n",
      "L1 loss: 0.09437323361635208\n",
      "Training batch 74 with loss 0.29911\n",
      "------------------------------------------\n",
      "L1 loss: 0.07249940931797028\n",
      "Training batch 75 with loss 0.27922\n",
      "------------------------------------------\n",
      "L1 loss: 0.08569890260696411\n",
      "Training batch 76 with loss 0.31657\n",
      "------------------------------------------\n",
      "L1 loss: 0.0794900581240654\n",
      "Training batch 77 with loss 0.27111\n",
      "------------------------------------------\n",
      "L1 loss: 0.07964544743299484\n",
      "Training batch 78 with loss 0.29833\n",
      "------------------------------------------\n",
      "L1 loss: 0.10381340235471725\n",
      "Training batch 79 with loss 0.35720\n",
      "------------------------------------------\n",
      "L1 loss: 0.07633564621210098\n",
      "Training batch 80 with loss 0.27726\n",
      "------------------------------------------\n",
      "L1 loss: 0.07635927200317383\n",
      "Training batch 81 with loss 0.28555\n",
      "------------------------------------------\n",
      "L1 loss: 0.08467492461204529\n",
      "Training batch 82 with loss 0.35236\n",
      "------------------------------------------\n",
      "L1 loss: 0.07485004514455795\n",
      "Training batch 83 with loss 0.27913\n",
      "------------------------------------------\n",
      "L1 loss: 0.0676790103316307\n",
      "Training batch 84 with loss 0.28430\n",
      "------------------------------------------\n",
      "L1 loss: 0.0625900998711586\n",
      "Training batch 85 with loss 0.27565\n",
      "------------------------------------------\n",
      "L1 loss: 0.08402981609106064\n",
      "Training batch 86 with loss 0.29288\n",
      "------------------------------------------\n",
      "L1 loss: 0.09963038563728333\n",
      "Training batch 87 with loss 0.39159\n",
      "------------------------------------------\n",
      "L1 loss: 0.0767117589712143\n",
      "Training batch 88 with loss 0.30519\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702507421374321\n",
      "Training batch 89 with loss 0.26024\n",
      "------------------------------------------\n",
      "L1 loss: 0.07325876504182816\n",
      "Training batch 90 with loss 0.27250\n",
      "------------------------------------------\n",
      "L1 loss: 0.06510908901691437\n",
      "Training batch 91 with loss 0.25702\n",
      "------------------------------------------\n",
      "L1 loss: 0.09161395579576492\n",
      "Training batch 92 with loss 0.31259\n",
      "------------------------------------------\n",
      "L1 loss: 0.06885475665330887\n",
      "Training batch 93 with loss 0.26028\n",
      "------------------------------------------\n",
      "L1 loss: 0.08261176198720932\n",
      "Training batch 94 with loss 0.27947\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976814568042755\n",
      "Training batch 95 with loss 0.24523\n",
      "------------------------------------------\n",
      "L1 loss: 0.06458073854446411\n",
      "Training batch 96 with loss 0.29855\n",
      "------------------------------------------\n",
      "L1 loss: 0.0873555988073349\n",
      "Training batch 97 with loss 0.35392\n",
      "------------------------------------------\n",
      "L1 loss: 0.0578349344432354\n",
      "Training batch 98 with loss 0.23127\n",
      "------------------------------------------\n",
      "L1 loss: 0.06665211170911789\n",
      "Training batch 99 with loss 0.27480\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2926241825520992\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4872\n",
      "------------------------------------------\n",
      "L1 loss: 0.06288217753171921\n",
      "Training batch 0 with loss 0.24476\n",
      "------------------------------------------\n",
      "L1 loss: 0.0854601338505745\n",
      "Training batch 1 with loss 0.28310\n",
      "------------------------------------------\n",
      "L1 loss: 0.092374287545681\n",
      "Training batch 2 with loss 0.32332\n",
      "------------------------------------------\n",
      "L1 loss: 0.07694994658231735\n",
      "Training batch 3 with loss 0.25650\n",
      "------------------------------------------\n",
      "L1 loss: 0.08467478305101395\n",
      "Training batch 4 with loss 0.32604\n",
      "------------------------------------------\n",
      "L1 loss: 0.06534427404403687\n",
      "Training batch 5 with loss 0.31453\n",
      "------------------------------------------\n",
      "L1 loss: 0.08028168231248856\n",
      "Training batch 6 with loss 0.26353\n",
      "------------------------------------------\n",
      "L1 loss: 0.08399772644042969\n",
      "Training batch 7 with loss 0.33433\n",
      "------------------------------------------\n",
      "L1 loss: 0.08363182842731476\n",
      "Training batch 8 with loss 0.33413\n",
      "------------------------------------------\n",
      "L1 loss: 0.05966273695230484\n",
      "Training batch 9 with loss 0.24342\n",
      "------------------------------------------\n",
      "L1 loss: 0.08235491812229156\n",
      "Training batch 10 with loss 0.30318\n",
      "------------------------------------------\n",
      "L1 loss: 0.05852258577942848\n",
      "Training batch 11 with loss 0.29425\n",
      "------------------------------------------\n",
      "L1 loss: 0.0722385123372078\n",
      "Training batch 12 with loss 0.30036\n",
      "------------------------------------------\n",
      "L1 loss: 0.06952410191297531\n",
      "Training batch 13 with loss 0.23584\n",
      "------------------------------------------\n",
      "L1 loss: 0.051892925053834915\n",
      "Training batch 14 with loss 0.24875\n",
      "------------------------------------------\n",
      "L1 loss: 0.08595925569534302\n",
      "Training batch 15 with loss 0.27506\n",
      "------------------------------------------\n",
      "L1 loss: 0.062262870371341705\n",
      "Training batch 16 with loss 0.27951\n",
      "------------------------------------------\n",
      "L1 loss: 0.08915180712938309\n",
      "Training batch 17 with loss 0.34120\n",
      "------------------------------------------\n",
      "L1 loss: 0.08115127682685852\n",
      "Training batch 18 with loss 0.31170\n",
      "------------------------------------------\n",
      "L1 loss: 0.06287812441587448\n",
      "Training batch 19 with loss 0.22887\n",
      "------------------------------------------\n",
      "L1 loss: 0.0714666023850441\n",
      "Training batch 20 with loss 0.32175\n",
      "------------------------------------------\n",
      "L1 loss: 0.06967943161725998\n",
      "Training batch 21 with loss 0.27768\n",
      "------------------------------------------\n",
      "L1 loss: 0.06670357286930084\n",
      "Training batch 22 with loss 0.28784\n",
      "------------------------------------------\n",
      "L1 loss: 0.08951330184936523\n",
      "Training batch 23 with loss 0.34647\n",
      "------------------------------------------\n",
      "L1 loss: 0.06732986867427826\n",
      "Training batch 24 with loss 0.30433\n",
      "------------------------------------------\n",
      "L1 loss: 0.07746753096580505\n",
      "Training batch 25 with loss 0.30695\n",
      "------------------------------------------\n",
      "L1 loss: 0.07492642849683762\n",
      "Training batch 26 with loss 0.26585\n",
      "------------------------------------------\n",
      "L1 loss: 0.05553244799375534\n",
      "Training batch 27 with loss 0.28853\n",
      "------------------------------------------\n",
      "L1 loss: 0.05676872655749321\n",
      "Training batch 28 with loss 0.29529\n",
      "------------------------------------------\n",
      "L1 loss: 0.053695932030677795\n",
      "Training batch 29 with loss 0.23200\n",
      "------------------------------------------\n",
      "L1 loss: 0.06055433675646782\n",
      "Training batch 30 with loss 0.26508\n",
      "------------------------------------------\n",
      "L1 loss: 0.06143530085682869\n",
      "Training batch 31 with loss 0.30882\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018517702817917\n",
      "Training batch 32 with loss 0.27573\n",
      "------------------------------------------\n",
      "L1 loss: 0.06481759995222092\n",
      "Training batch 33 with loss 0.26293\n",
      "------------------------------------------\n",
      "L1 loss: 0.07593760639429092\n",
      "Training batch 34 with loss 0.31121\n",
      "------------------------------------------\n",
      "L1 loss: 0.09605370461940765\n",
      "Training batch 35 with loss 0.32544\n",
      "------------------------------------------\n",
      "L1 loss: 0.08644769340753555\n",
      "Training batch 36 with loss 0.28229\n",
      "------------------------------------------\n",
      "L1 loss: 0.06847701966762543\n",
      "Training batch 37 with loss 0.25795\n",
      "------------------------------------------\n",
      "L1 loss: 0.062140513211488724\n",
      "Training batch 38 with loss 0.22313\n",
      "------------------------------------------\n",
      "L1 loss: 0.06796592473983765\n",
      "Training batch 39 with loss 0.30462\n",
      "------------------------------------------\n",
      "L1 loss: 0.06861554086208344\n",
      "Training batch 40 with loss 0.24960\n",
      "------------------------------------------\n",
      "L1 loss: 0.05490579828619957\n",
      "Training batch 41 with loss 0.30090\n",
      "------------------------------------------\n",
      "L1 loss: 0.07928680628538132\n",
      "Training batch 42 with loss 0.27345\n",
      "------------------------------------------\n",
      "L1 loss: 0.0699210911989212\n",
      "Training batch 43 with loss 0.27470\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.057951733469963074\n",
      "Training batch 44 with loss 0.26949\n",
      "------------------------------------------\n",
      "L1 loss: 0.09252643585205078\n",
      "Training batch 45 with loss 0.35538\n",
      "------------------------------------------\n",
      "L1 loss: 0.07441068440675735\n",
      "Training batch 46 with loss 0.26775\n",
      "------------------------------------------\n",
      "L1 loss: 0.11006880551576614\n",
      "Training batch 47 with loss 0.36583\n",
      "------------------------------------------\n",
      "L1 loss: 0.04558936879038811\n",
      "Training batch 48 with loss 0.23020\n",
      "------------------------------------------\n",
      "L1 loss: 0.07619661092758179\n",
      "Training batch 49 with loss 0.34445\n",
      "------------------------------------------\n",
      "L1 loss: 0.07504964619874954\n",
      "Training batch 50 with loss 0.29569\n",
      "------------------------------------------\n",
      "L1 loss: 0.07669635862112045\n",
      "Training batch 51 with loss 0.33250\n",
      "------------------------------------------\n",
      "L1 loss: 0.09092653542757034\n",
      "Training batch 52 with loss 0.29815\n",
      "------------------------------------------\n",
      "L1 loss: 0.07836068421602249\n",
      "Training batch 53 with loss 0.27981\n",
      "------------------------------------------\n",
      "L1 loss: 0.07324516028165817\n",
      "Training batch 54 with loss 0.26932\n",
      "------------------------------------------\n",
      "L1 loss: 0.0611114576458931\n",
      "Training batch 55 with loss 0.29306\n",
      "------------------------------------------\n",
      "L1 loss: 0.054600540548563004\n",
      "Training batch 56 with loss 0.24168\n",
      "------------------------------------------\n",
      "L1 loss: 0.06954532861709595\n",
      "Training batch 57 with loss 0.26510\n",
      "------------------------------------------\n",
      "L1 loss: 0.0617278590798378\n",
      "Training batch 58 with loss 0.28232\n",
      "------------------------------------------\n",
      "L1 loss: 0.0784313827753067\n",
      "Training batch 59 with loss 0.28372\n",
      "------------------------------------------\n",
      "L1 loss: 0.06550230830907822\n",
      "Training batch 60 with loss 0.27189\n",
      "------------------------------------------\n",
      "L1 loss: 0.09950295090675354\n",
      "Training batch 61 with loss 0.35039\n",
      "------------------------------------------\n",
      "L1 loss: 0.05514371767640114\n",
      "Training batch 62 with loss 0.25486\n",
      "------------------------------------------\n",
      "L1 loss: 0.06416782736778259\n",
      "Training batch 63 with loss 0.26007\n",
      "------------------------------------------\n",
      "L1 loss: 0.07211614400148392\n",
      "Training batch 64 with loss 0.28038\n",
      "------------------------------------------\n",
      "L1 loss: 0.06066272407770157\n",
      "Training batch 65 with loss 0.22733\n",
      "------------------------------------------\n",
      "L1 loss: 0.06806911528110504\n",
      "Training batch 66 with loss 0.28950\n",
      "------------------------------------------\n",
      "L1 loss: 0.06547959893941879\n",
      "Training batch 67 with loss 0.26360\n",
      "------------------------------------------\n",
      "L1 loss: 0.07553644478321075\n",
      "Training batch 68 with loss 0.29538\n",
      "------------------------------------------\n",
      "L1 loss: 0.049143560230731964\n",
      "Training batch 69 with loss 0.25239\n",
      "------------------------------------------\n",
      "L1 loss: 0.07669734209775925\n",
      "Training batch 70 with loss 0.30293\n",
      "------------------------------------------\n",
      "L1 loss: 0.07179415971040726\n",
      "Training batch 71 with loss 0.30386\n",
      "------------------------------------------\n",
      "L1 loss: 0.08055535703897476\n",
      "Training batch 72 with loss 0.28804\n",
      "------------------------------------------\n",
      "L1 loss: 0.07128501683473587\n",
      "Training batch 73 with loss 0.30685\n",
      "------------------------------------------\n",
      "L1 loss: 0.09824137389659882\n",
      "Training batch 74 with loss 0.31721\n",
      "------------------------------------------\n",
      "L1 loss: 0.07547856867313385\n",
      "Training batch 75 with loss 0.29141\n",
      "------------------------------------------\n",
      "L1 loss: 0.08570598810911179\n",
      "Training batch 76 with loss 0.29893\n",
      "------------------------------------------\n",
      "L1 loss: 0.07998447865247726\n",
      "Training batch 77 with loss 0.26311\n",
      "------------------------------------------\n",
      "L1 loss: 0.08132254332304001\n",
      "Training batch 78 with loss 0.30496\n",
      "------------------------------------------\n",
      "L1 loss: 0.10775578767061234\n",
      "Training batch 79 with loss 0.39244\n",
      "------------------------------------------\n",
      "L1 loss: 0.07488111406564713\n",
      "Training batch 80 with loss 0.28224\n",
      "------------------------------------------\n",
      "L1 loss: 0.06996970623731613\n",
      "Training batch 81 with loss 0.26126\n",
      "------------------------------------------\n",
      "L1 loss: 0.08355499058961868\n",
      "Training batch 82 with loss 0.35411\n",
      "------------------------------------------\n",
      "L1 loss: 0.07879374176263809\n",
      "Training batch 83 with loss 0.28060\n",
      "------------------------------------------\n",
      "L1 loss: 0.07305020838975906\n",
      "Training batch 84 with loss 0.31314\n",
      "------------------------------------------\n",
      "L1 loss: 0.06303086876869202\n",
      "Training batch 85 with loss 0.27167\n",
      "------------------------------------------\n",
      "L1 loss: 0.08467963337898254\n",
      "Training batch 86 with loss 0.30109\n",
      "------------------------------------------\n",
      "L1 loss: 0.10174813866615295\n",
      "Training batch 87 with loss 0.38028\n",
      "------------------------------------------\n",
      "L1 loss: 0.07738111913204193\n",
      "Training batch 88 with loss 0.31899\n",
      "------------------------------------------\n",
      "L1 loss: 0.06969965994358063\n",
      "Training batch 89 with loss 0.25761\n",
      "------------------------------------------\n",
      "L1 loss: 0.07145613431930542\n",
      "Training batch 90 with loss 0.28515\n",
      "------------------------------------------\n",
      "L1 loss: 0.06369498372077942\n",
      "Training batch 91 with loss 0.25486\n",
      "------------------------------------------\n",
      "L1 loss: 0.09138359129428864\n",
      "Training batch 92 with loss 0.29592\n",
      "------------------------------------------\n",
      "L1 loss: 0.07049121707677841\n",
      "Training batch 93 with loss 0.26723\n",
      "------------------------------------------\n",
      "L1 loss: 0.06824048608541489\n",
      "Training batch 94 with loss 0.50015\n",
      "------------------------------------------\n",
      "L1 loss: 0.06975486129522324\n",
      "Training batch 95 with loss 0.24538\n",
      "------------------------------------------\n",
      "L1 loss: 0.06998037546873093\n",
      "Training batch 96 with loss 0.29455\n",
      "------------------------------------------\n",
      "L1 loss: 0.08663120120763779\n",
      "Training batch 97 with loss 0.36183\n",
      "------------------------------------------\n",
      "L1 loss: 0.05668514966964722\n",
      "Training batch 98 with loss 0.24364\n",
      "------------------------------------------\n",
      "L1 loss: 0.06554856896400452\n",
      "Training batch 99 with loss 0.27957\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29124185636639593\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4873\n",
      "------------------------------------------\n",
      "L1 loss: 0.06208149716258049\n",
      "Training batch 0 with loss 0.25561\n",
      "------------------------------------------\n",
      "L1 loss: 0.08489363640546799\n",
      "Training batch 1 with loss 0.27372\n",
      "------------------------------------------\n",
      "L1 loss: 0.09448761492967606\n",
      "Training batch 2 with loss 0.31954\n",
      "------------------------------------------\n",
      "L1 loss: 0.07096992433071136\n",
      "Training batch 3 with loss 0.26581\n",
      "------------------------------------------\n",
      "L1 loss: 0.08410587906837463\n",
      "Training batch 4 with loss 0.31572\n",
      "------------------------------------------\n",
      "L1 loss: 0.06533554196357727\n",
      "Training batch 5 with loss 0.32921\n",
      "------------------------------------------\n",
      "L1 loss: 0.08078332990407944\n",
      "Training batch 6 with loss 0.26898\n",
      "------------------------------------------\n",
      "L1 loss: 0.08382487297058105\n",
      "Training batch 7 with loss 0.33663\n",
      "------------------------------------------\n",
      "L1 loss: 0.07798627018928528\n",
      "Training batch 8 with loss 0.30437\n",
      "------------------------------------------\n",
      "L1 loss: 0.05889880657196045\n",
      "Training batch 9 with loss 0.24730\n",
      "------------------------------------------\n",
      "L1 loss: 0.08409562706947327\n",
      "Training batch 10 with loss 0.28803\n",
      "------------------------------------------\n",
      "L1 loss: 0.06089048087596893\n",
      "Training batch 11 with loss 0.29016\n",
      "------------------------------------------\n",
      "L1 loss: 0.07680300623178482\n",
      "Training batch 12 with loss 0.31551\n",
      "------------------------------------------\n",
      "L1 loss: 0.06896521151065826\n",
      "Training batch 13 with loss 0.25218\n",
      "------------------------------------------\n",
      "L1 loss: 0.0528365783393383\n",
      "Training batch 14 with loss 0.27428\n",
      "------------------------------------------\n",
      "L1 loss: 0.08573661744594574\n",
      "Training batch 15 with loss 0.27451\n",
      "------------------------------------------\n",
      "L1 loss: 0.0621895007789135\n",
      "Training batch 16 with loss 0.27070\n",
      "------------------------------------------\n",
      "L1 loss: 0.08781161904335022\n",
      "Training batch 17 with loss 0.34507\n",
      "------------------------------------------\n",
      "L1 loss: 0.0808100774884224\n",
      "Training batch 18 with loss 0.29004\n",
      "------------------------------------------\n",
      "L1 loss: 0.06374488025903702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 19 with loss 0.25298\n",
      "------------------------------------------\n",
      "L1 loss: 0.07354554533958435\n",
      "Training batch 20 with loss 0.31205\n",
      "------------------------------------------\n",
      "L1 loss: 0.068508580327034\n",
      "Training batch 21 with loss 0.28829\n",
      "------------------------------------------\n",
      "L1 loss: 0.05307798087596893\n",
      "Training batch 22 with loss 0.30049\n",
      "------------------------------------------\n",
      "L1 loss: 0.08832534402608871\n",
      "Training batch 23 with loss 0.38004\n",
      "------------------------------------------\n",
      "L1 loss: 0.06915999203920364\n",
      "Training batch 24 with loss 0.29188\n",
      "------------------------------------------\n",
      "L1 loss: 0.07358796894550323\n",
      "Training batch 25 with loss 0.28677\n",
      "------------------------------------------\n",
      "L1 loss: 0.07707207649946213\n",
      "Training batch 26 with loss 0.27840\n",
      "------------------------------------------\n",
      "L1 loss: 0.054908860474824905\n",
      "Training batch 27 with loss 0.28370\n",
      "------------------------------------------\n",
      "L1 loss: 0.05652156099677086\n",
      "Training batch 28 with loss 0.28027\n",
      "------------------------------------------\n",
      "L1 loss: 0.054366711527109146\n",
      "Training batch 29 with loss 0.23995\n",
      "------------------------------------------\n",
      "L1 loss: 0.05562806874513626\n",
      "Training batch 30 with loss 0.22904\n",
      "------------------------------------------\n",
      "L1 loss: 0.06392963230609894\n",
      "Training batch 31 with loss 0.29995\n",
      "------------------------------------------\n",
      "L1 loss: 0.07014455646276474\n",
      "Training batch 32 with loss 0.29989\n",
      "------------------------------------------\n",
      "L1 loss: 0.06622619926929474\n",
      "Training batch 33 with loss 0.26481\n",
      "------------------------------------------\n",
      "L1 loss: 0.0744834914803505\n",
      "Training batch 34 with loss 0.30906\n",
      "------------------------------------------\n",
      "L1 loss: 0.09639932215213776\n",
      "Training batch 35 with loss 0.33435\n",
      "------------------------------------------\n",
      "L1 loss: 0.08624535799026489\n",
      "Training batch 36 with loss 0.28716\n",
      "------------------------------------------\n",
      "L1 loss: 0.06991543620824814\n",
      "Training batch 37 with loss 0.27319\n",
      "------------------------------------------\n",
      "L1 loss: 0.06356898695230484\n",
      "Training batch 38 with loss 0.22199\n",
      "------------------------------------------\n",
      "L1 loss: 0.06852660328149796\n",
      "Training batch 39 with loss 0.28807\n",
      "------------------------------------------\n",
      "L1 loss: 0.06804021447896957\n",
      "Training batch 40 with loss 0.26219\n",
      "------------------------------------------\n",
      "L1 loss: 0.059697944670915604\n",
      "Training batch 41 with loss 0.28185\n",
      "------------------------------------------\n",
      "L1 loss: 0.08094210177659988\n",
      "Training batch 42 with loss 0.27732\n",
      "------------------------------------------\n",
      "L1 loss: 0.07047858834266663\n",
      "Training batch 43 with loss 0.28294\n",
      "------------------------------------------\n",
      "L1 loss: 0.04742009565234184\n",
      "Training batch 44 with loss 0.29732\n",
      "------------------------------------------\n",
      "L1 loss: 0.08014468848705292\n",
      "Training batch 45 with loss 0.67108\n",
      "------------------------------------------\n",
      "L1 loss: 0.07103990018367767\n",
      "Training batch 46 with loss 0.26486\n",
      "------------------------------------------\n",
      "L1 loss: 0.11066742241382599\n",
      "Training batch 47 with loss 0.36236\n",
      "------------------------------------------\n",
      "L1 loss: 0.04200328141450882\n",
      "Training batch 48 with loss 0.23017\n",
      "------------------------------------------\n",
      "L1 loss: 0.08244328200817108\n",
      "Training batch 49 with loss 0.36914\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018600404262543\n",
      "Training batch 50 with loss 0.29520\n",
      "------------------------------------------\n",
      "L1 loss: 0.08326687663793564\n",
      "Training batch 51 with loss 0.35220\n",
      "------------------------------------------\n",
      "L1 loss: 0.0897681787610054\n",
      "Training batch 52 with loss 0.29592\n",
      "------------------------------------------\n",
      "L1 loss: 0.07878173142671585\n",
      "Training batch 53 with loss 0.29107\n",
      "------------------------------------------\n",
      "L1 loss: 0.07742871344089508\n",
      "Training batch 54 with loss 0.25941\n",
      "------------------------------------------\n",
      "L1 loss: 0.0625411793589592\n",
      "Training batch 55 with loss 0.33175\n",
      "------------------------------------------\n",
      "L1 loss: 0.05812624841928482\n",
      "Training batch 56 with loss 0.25999\n",
      "------------------------------------------\n",
      "L1 loss: 0.06791042536497116\n",
      "Training batch 57 with loss 0.27755\n",
      "------------------------------------------\n",
      "L1 loss: 0.0646972730755806\n",
      "Training batch 58 with loss 0.24562\n",
      "------------------------------------------\n",
      "L1 loss: 0.08194717019796371\n",
      "Training batch 59 with loss 0.28101\n",
      "------------------------------------------\n",
      "L1 loss: 0.064205601811409\n",
      "Training batch 60 with loss 0.27685\n",
      "------------------------------------------\n",
      "L1 loss: 0.09905534237623215\n",
      "Training batch 61 with loss 0.36723\n",
      "------------------------------------------\n",
      "L1 loss: 0.05769042670726776\n",
      "Training batch 62 with loss 0.24795\n",
      "------------------------------------------\n",
      "L1 loss: 0.06143856793642044\n",
      "Training batch 63 with loss 0.26617\n",
      "------------------------------------------\n",
      "L1 loss: 0.07080774009227753\n",
      "Training batch 64 with loss 0.28151\n",
      "------------------------------------------\n",
      "L1 loss: 0.06224220246076584\n",
      "Training batch 65 with loss 0.23913\n",
      "------------------------------------------\n",
      "L1 loss: 0.067075714468956\n",
      "Training batch 66 with loss 0.28559\n",
      "------------------------------------------\n",
      "L1 loss: 0.06565459817647934\n",
      "Training batch 67 with loss 0.26384\n",
      "------------------------------------------\n",
      "L1 loss: 0.07099708914756775\n",
      "Training batch 68 with loss 0.29401\n",
      "------------------------------------------\n",
      "L1 loss: 0.04948807507753372\n",
      "Training batch 69 with loss 0.25356\n",
      "------------------------------------------\n",
      "L1 loss: 0.08368619531393051\n",
      "Training batch 70 with loss 0.31294\n",
      "------------------------------------------\n",
      "L1 loss: 0.07242555916309357\n",
      "Training batch 71 with loss 0.29792\n",
      "------------------------------------------\n",
      "L1 loss: 0.08342522382736206\n",
      "Training batch 72 with loss 0.29146\n",
      "------------------------------------------\n",
      "L1 loss: 0.0738336518406868\n",
      "Training batch 73 with loss 0.30168\n",
      "------------------------------------------\n",
      "L1 loss: 0.09751015156507492\n",
      "Training batch 74 with loss 0.29270\n",
      "------------------------------------------\n",
      "L1 loss: 0.0762912854552269\n",
      "Training batch 75 with loss 0.28039\n",
      "------------------------------------------\n",
      "L1 loss: 0.08657655864953995\n",
      "Training batch 76 with loss 0.29046\n",
      "------------------------------------------\n",
      "L1 loss: 0.07890675961971283\n",
      "Training batch 77 with loss 0.25763\n",
      "------------------------------------------\n",
      "L1 loss: 0.08449982106685638\n",
      "Training batch 78 with loss 0.30671\n",
      "------------------------------------------\n",
      "L1 loss: 0.10871032625436783\n",
      "Training batch 79 with loss 0.36518\n",
      "------------------------------------------\n",
      "L1 loss: 0.07562841475009918\n",
      "Training batch 80 with loss 0.27853\n",
      "------------------------------------------\n",
      "L1 loss: 0.08196266740560532\n",
      "Training batch 81 with loss 0.30060\n",
      "------------------------------------------\n",
      "L1 loss: 0.08213549852371216\n",
      "Training batch 82 with loss 0.36238\n",
      "------------------------------------------\n",
      "L1 loss: 0.07446090131998062\n",
      "Training batch 83 with loss 0.27949\n",
      "------------------------------------------\n",
      "L1 loss: 0.06695600599050522\n",
      "Training batch 84 with loss 0.29588\n",
      "------------------------------------------\n",
      "L1 loss: 0.06289042532444\n",
      "Training batch 85 with loss 0.28361\n",
      "------------------------------------------\n",
      "L1 loss: 0.06471462547779083\n",
      "Training batch 86 with loss 0.23868\n",
      "------------------------------------------\n",
      "L1 loss: 0.09744901210069656\n",
      "Training batch 87 with loss 0.59185\n",
      "------------------------------------------\n",
      "L1 loss: 0.07786845415830612\n",
      "Training batch 88 with loss 0.31404\n",
      "------------------------------------------\n",
      "L1 loss: 0.07061467319726944\n",
      "Training batch 89 with loss 0.25869\n",
      "------------------------------------------\n",
      "L1 loss: 0.07688027620315552\n",
      "Training batch 90 with loss 0.27973\n",
      "------------------------------------------\n",
      "L1 loss: 0.06384329497814178\n",
      "Training batch 91 with loss 0.25723\n",
      "------------------------------------------\n",
      "L1 loss: 0.08904918283224106\n",
      "Training batch 92 with loss 0.28148\n",
      "------------------------------------------\n",
      "L1 loss: 0.05676650255918503\n",
      "Training batch 93 with loss 0.23668\n",
      "------------------------------------------\n",
      "L1 loss: 0.08392099291086197\n",
      "Training batch 94 with loss 0.30726\n",
      "------------------------------------------\n",
      "L1 loss: 0.06992024928331375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 95 with loss 0.25036\n",
      "------------------------------------------\n",
      "L1 loss: 0.061210744082927704\n",
      "Training batch 96 with loss 0.29104\n",
      "------------------------------------------\n",
      "L1 loss: 0.07664800435304642\n",
      "Training batch 97 with loss 0.54793\n",
      "------------------------------------------\n",
      "L1 loss: 0.055649202316999435\n",
      "Training batch 98 with loss 0.31114\n",
      "------------------------------------------\n",
      "L1 loss: 0.05606831610202789\n",
      "Training batch 99 with loss 0.50205\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29950226172804834\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4874\n",
      "------------------------------------------\n",
      "L1 loss: 0.0649869367480278\n",
      "Training batch 0 with loss 0.25840\n",
      "------------------------------------------\n",
      "L1 loss: 0.08554636687040329\n",
      "Training batch 1 with loss 0.29940\n",
      "------------------------------------------\n",
      "L1 loss: 0.09461480379104614\n",
      "Training batch 2 with loss 0.32867\n",
      "------------------------------------------\n",
      "L1 loss: 0.08220002800226212\n",
      "Training batch 3 with loss 0.27124\n",
      "------------------------------------------\n",
      "L1 loss: 0.0829898789525032\n",
      "Training batch 4 with loss 0.33434\n",
      "------------------------------------------\n",
      "L1 loss: 0.06690988689661026\n",
      "Training batch 5 with loss 0.33771\n",
      "------------------------------------------\n",
      "L1 loss: 0.07933434844017029\n",
      "Training batch 6 with loss 0.27128\n",
      "------------------------------------------\n",
      "L1 loss: 0.08309253305196762\n",
      "Training batch 7 with loss 0.32543\n",
      "------------------------------------------\n",
      "L1 loss: 0.08020787686109543\n",
      "Training batch 8 with loss 0.31927\n",
      "------------------------------------------\n",
      "L1 loss: 0.05894492194056511\n",
      "Training batch 9 with loss 0.24493\n",
      "------------------------------------------\n",
      "L1 loss: 0.08340223133563995\n",
      "Training batch 10 with loss 0.29252\n",
      "------------------------------------------\n",
      "L1 loss: 0.0458618625998497\n",
      "Training batch 11 with loss 0.38480\n",
      "------------------------------------------\n",
      "L1 loss: 0.07185030728578568\n",
      "Training batch 12 with loss 0.30914\n",
      "------------------------------------------\n",
      "L1 loss: 0.06783808022737503\n",
      "Training batch 13 with loss 0.23731\n",
      "------------------------------------------\n",
      "L1 loss: 0.05283190682530403\n",
      "Training batch 14 with loss 0.24403\n",
      "------------------------------------------\n",
      "L1 loss: 0.0846007689833641\n",
      "Training batch 15 with loss 0.27288\n",
      "------------------------------------------\n",
      "L1 loss: 0.062462709844112396\n",
      "Training batch 16 with loss 0.27005\n",
      "------------------------------------------\n",
      "L1 loss: 0.07175983488559723\n",
      "Training batch 17 with loss 0.50391\n",
      "------------------------------------------\n",
      "L1 loss: 0.08005010336637497\n",
      "Training batch 18 with loss 0.29402\n",
      "------------------------------------------\n",
      "L1 loss: 0.06228514015674591\n",
      "Training batch 19 with loss 0.25089\n",
      "------------------------------------------\n",
      "L1 loss: 0.07400388270616531\n",
      "Training batch 20 with loss 0.31723\n",
      "------------------------------------------\n",
      "L1 loss: 0.07125144451856613\n",
      "Training batch 21 with loss 0.26771\n",
      "------------------------------------------\n",
      "L1 loss: 0.06804286688566208\n",
      "Training batch 22 with loss 0.28848\n",
      "------------------------------------------\n",
      "L1 loss: 0.08244381844997406\n",
      "Training batch 23 with loss 0.36910\n",
      "------------------------------------------\n",
      "L1 loss: 0.06681791692972183\n",
      "Training batch 24 with loss 0.31427\n",
      "------------------------------------------\n",
      "L1 loss: 0.0738820880651474\n",
      "Training batch 25 with loss 0.28945\n",
      "------------------------------------------\n",
      "L1 loss: 0.07691993564367294\n",
      "Training batch 26 with loss 0.26806\n",
      "------------------------------------------\n",
      "L1 loss: 0.05770613253116608\n",
      "Training batch 27 with loss 0.30517\n",
      "------------------------------------------\n",
      "L1 loss: 0.057259634137153625\n",
      "Training batch 28 with loss 0.27209\n",
      "------------------------------------------\n",
      "L1 loss: 0.05432368069887161\n",
      "Training batch 29 with loss 0.24187\n",
      "------------------------------------------\n",
      "L1 loss: 0.06209973990917206\n",
      "Training batch 30 with loss 0.26976\n",
      "------------------------------------------\n",
      "L1 loss: 0.06157178059220314\n",
      "Training batch 31 with loss 0.28737\n",
      "------------------------------------------\n",
      "L1 loss: 0.0701090395450592\n",
      "Training batch 32 with loss 0.29774\n",
      "------------------------------------------\n",
      "L1 loss: 0.0641053095459938\n",
      "Training batch 33 with loss 0.26146\n",
      "------------------------------------------\n",
      "L1 loss: 0.07397652417421341\n",
      "Training batch 34 with loss 0.30644\n",
      "------------------------------------------\n",
      "L1 loss: 0.09165562689304352\n",
      "Training batch 35 with loss 0.34126\n",
      "------------------------------------------\n",
      "L1 loss: 0.08327635377645493\n",
      "Training batch 36 with loss 0.27704\n",
      "------------------------------------------\n",
      "L1 loss: 0.06495977938175201\n",
      "Training batch 37 with loss 0.26170\n",
      "------------------------------------------\n",
      "L1 loss: 0.06332272291183472\n",
      "Training batch 38 with loss 0.22347\n",
      "------------------------------------------\n",
      "L1 loss: 0.05072678625583649\n",
      "Training batch 39 with loss 0.45296\n",
      "------------------------------------------\n",
      "L1 loss: 0.06875738501548767\n",
      "Training batch 40 with loss 0.25929\n",
      "------------------------------------------\n",
      "L1 loss: 0.05989162251353264\n",
      "Training batch 41 with loss 0.28227\n",
      "------------------------------------------\n",
      "L1 loss: 0.07811612635850906\n",
      "Training batch 42 with loss 0.27523\n",
      "------------------------------------------\n",
      "L1 loss: 0.06750305742025375\n",
      "Training batch 43 with loss 0.26972\n",
      "------------------------------------------\n",
      "L1 loss: 0.057935476303100586\n",
      "Training batch 44 with loss 0.25637\n",
      "------------------------------------------\n",
      "L1 loss: 0.09214405715465546\n",
      "Training batch 45 with loss 0.35950\n",
      "------------------------------------------\n",
      "L1 loss: 0.07127528637647629\n",
      "Training batch 46 with loss 0.26563\n",
      "------------------------------------------\n",
      "L1 loss: 0.11252511292695999\n",
      "Training batch 47 with loss 0.38803\n",
      "------------------------------------------\n",
      "L1 loss: 0.048763055354356766\n",
      "Training batch 48 with loss 0.24798\n",
      "------------------------------------------\n",
      "L1 loss: 0.07766798883676529\n",
      "Training batch 49 with loss 0.35536\n",
      "------------------------------------------\n",
      "L1 loss: 0.07254046201705933\n",
      "Training batch 50 with loss 0.27170\n",
      "------------------------------------------\n",
      "L1 loss: 0.08429235965013504\n",
      "Training batch 51 with loss 0.34776\n",
      "------------------------------------------\n",
      "L1 loss: 0.09074465185403824\n",
      "Training batch 52 with loss 0.29553\n",
      "------------------------------------------\n",
      "L1 loss: 0.07761619240045547\n",
      "Training batch 53 with loss 0.31295\n",
      "------------------------------------------\n",
      "L1 loss: 0.07762688398361206\n",
      "Training batch 54 with loss 0.27256\n",
      "------------------------------------------\n",
      "L1 loss: 0.06328661739826202\n",
      "Training batch 55 with loss 0.31013\n",
      "------------------------------------------\n",
      "L1 loss: 0.0517708919942379\n",
      "Training batch 56 with loss 0.23620\n",
      "------------------------------------------\n",
      "L1 loss: 0.06601385027170181\n",
      "Training batch 57 with loss 0.26723\n",
      "------------------------------------------\n",
      "L1 loss: 0.06312106549739838\n",
      "Training batch 58 with loss 0.27806\n",
      "------------------------------------------\n",
      "L1 loss: 0.07876599580049515\n",
      "Training batch 59 with loss 0.29330\n",
      "------------------------------------------\n",
      "L1 loss: 0.0632866695523262\n",
      "Training batch 60 with loss 0.27453\n",
      "------------------------------------------\n",
      "L1 loss: 0.10413174331188202\n",
      "Training batch 61 with loss 0.37491\n",
      "------------------------------------------\n",
      "L1 loss: 0.0559694841504097\n",
      "Training batch 62 with loss 0.25760\n",
      "------------------------------------------\n",
      "L1 loss: 0.06162719801068306\n",
      "Training batch 63 with loss 0.25223\n",
      "------------------------------------------\n",
      "L1 loss: 0.07030832767486572\n",
      "Training batch 64 with loss 0.28407\n",
      "------------------------------------------\n",
      "L1 loss: 0.06290076673030853\n",
      "Training batch 65 with loss 0.23624\n",
      "------------------------------------------\n",
      "L1 loss: 0.0661180391907692\n",
      "Training batch 66 with loss 0.26493\n",
      "------------------------------------------\n",
      "L1 loss: 0.06511688977479935\n",
      "Training batch 67 with loss 0.25790\n",
      "------------------------------------------\n",
      "L1 loss: 0.07262461632490158\n",
      "Training batch 68 with loss 0.29663\n",
      "------------------------------------------\n",
      "L1 loss: 0.05156692862510681\n",
      "Training batch 69 with loss 0.25833\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07586486637592316\n",
      "Training batch 70 with loss 0.30037\n",
      "------------------------------------------\n",
      "L1 loss: 0.0725032240152359\n",
      "Training batch 71 with loss 0.29330\n",
      "------------------------------------------\n",
      "L1 loss: 0.0852447971701622\n",
      "Training batch 72 with loss 0.30851\n",
      "------------------------------------------\n",
      "L1 loss: 0.07070714235305786\n",
      "Training batch 73 with loss 0.28227\n",
      "------------------------------------------\n",
      "L1 loss: 0.09831245243549347\n",
      "Training batch 74 with loss 0.31881\n",
      "------------------------------------------\n",
      "L1 loss: 0.07447353005409241\n",
      "Training batch 75 with loss 0.27152\n",
      "------------------------------------------\n",
      "L1 loss: 0.08625491708517075\n",
      "Training batch 76 with loss 0.30603\n",
      "------------------------------------------\n",
      "L1 loss: 0.08008719980716705\n",
      "Training batch 77 with loss 0.26313\n",
      "------------------------------------------\n",
      "L1 loss: 0.08359455317258835\n",
      "Training batch 78 with loss 0.31537\n",
      "------------------------------------------\n",
      "L1 loss: 0.10694622248411179\n",
      "Training batch 79 with loss 0.36772\n",
      "------------------------------------------\n",
      "L1 loss: 0.07588452845811844\n",
      "Training batch 80 with loss 0.28962\n",
      "------------------------------------------\n",
      "L1 loss: 0.07399333268404007\n",
      "Training batch 81 with loss 0.28413\n",
      "------------------------------------------\n",
      "L1 loss: 0.08545388281345367\n",
      "Training batch 82 with loss 0.35365\n",
      "------------------------------------------\n",
      "L1 loss: 0.07481087744235992\n",
      "Training batch 83 with loss 0.27954\n",
      "------------------------------------------\n",
      "L1 loss: 0.06979066133499146\n",
      "Training batch 84 with loss 0.29324\n",
      "------------------------------------------\n",
      "L1 loss: 0.06481456011533737\n",
      "Training batch 85 with loss 0.29913\n",
      "------------------------------------------\n",
      "L1 loss: 0.08428612351417542\n",
      "Training batch 86 with loss 0.29503\n",
      "------------------------------------------\n",
      "L1 loss: 0.10156038403511047\n",
      "Training batch 87 with loss 0.38247\n",
      "------------------------------------------\n",
      "L1 loss: 0.07732740044593811\n",
      "Training batch 88 with loss 0.31137\n",
      "------------------------------------------\n",
      "L1 loss: 0.06955119222402573\n",
      "Training batch 89 with loss 0.26167\n",
      "------------------------------------------\n",
      "L1 loss: 0.07108750194311142\n",
      "Training batch 90 with loss 0.28656\n",
      "------------------------------------------\n",
      "L1 loss: 0.05859612673521042\n",
      "Training batch 91 with loss 0.25268\n",
      "------------------------------------------\n",
      "L1 loss: 0.09283366054296494\n",
      "Training batch 92 with loss 0.30085\n",
      "------------------------------------------\n",
      "L1 loss: 0.06871247291564941\n",
      "Training batch 93 with loss 0.27205\n",
      "------------------------------------------\n",
      "L1 loss: 0.08337119221687317\n",
      "Training batch 94 with loss 0.29862\n",
      "------------------------------------------\n",
      "L1 loss: 0.07228904217481613\n",
      "Training batch 95 with loss 0.25887\n",
      "------------------------------------------\n",
      "L1 loss: 0.0609259270131588\n",
      "Training batch 96 with loss 0.27641\n",
      "------------------------------------------\n",
      "L1 loss: 0.08550866693258286\n",
      "Training batch 97 with loss 0.35782\n",
      "------------------------------------------\n",
      "L1 loss: 0.05802730470895767\n",
      "Training batch 98 with loss 0.23745\n",
      "------------------------------------------\n",
      "L1 loss: 0.06527789682149887\n",
      "Training batch 99 with loss 0.27457\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2945782092213631\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4875\n",
      "------------------------------------------\n",
      "L1 loss: 0.05940676108002663\n",
      "Training batch 0 with loss 0.26128\n",
      "------------------------------------------\n",
      "L1 loss: 0.08755184710025787\n",
      "Training batch 1 with loss 0.30016\n",
      "------------------------------------------\n",
      "L1 loss: 0.08655387163162231\n",
      "Training batch 2 with loss 0.51569\n",
      "------------------------------------------\n",
      "L1 loss: 0.07271155714988708\n",
      "Training batch 3 with loss 0.25813\n",
      "------------------------------------------\n",
      "L1 loss: 0.08428596705198288\n",
      "Training batch 4 with loss 0.33216\n",
      "------------------------------------------\n",
      "L1 loss: 0.06665538996458054\n",
      "Training batch 5 with loss 0.32661\n",
      "------------------------------------------\n",
      "L1 loss: 0.07849665731191635\n",
      "Training batch 6 with loss 0.27270\n",
      "------------------------------------------\n",
      "L1 loss: 0.08375205099582672\n",
      "Training batch 7 with loss 0.33753\n",
      "------------------------------------------\n",
      "L1 loss: 0.07866311818361282\n",
      "Training batch 8 with loss 0.31881\n",
      "------------------------------------------\n",
      "L1 loss: 0.06253129243850708\n",
      "Training batch 9 with loss 0.25567\n",
      "------------------------------------------\n",
      "L1 loss: 0.08381064981222153\n",
      "Training batch 10 with loss 0.29223\n",
      "------------------------------------------\n",
      "L1 loss: 0.057320427149534225\n",
      "Training batch 11 with loss 0.28801\n",
      "------------------------------------------\n",
      "L1 loss: 0.07666755467653275\n",
      "Training batch 12 with loss 0.31250\n",
      "------------------------------------------\n",
      "L1 loss: 0.06926583498716354\n",
      "Training batch 13 with loss 0.23996\n",
      "------------------------------------------\n",
      "L1 loss: 0.052036598324775696\n",
      "Training batch 14 with loss 0.24994\n",
      "------------------------------------------\n",
      "L1 loss: 0.08474989235401154\n",
      "Training batch 15 with loss 0.27247\n",
      "------------------------------------------\n",
      "L1 loss: 0.06260313838720322\n",
      "Training batch 16 with loss 0.25859\n",
      "------------------------------------------\n",
      "L1 loss: 0.08826630562543869\n",
      "Training batch 17 with loss 0.34136\n",
      "------------------------------------------\n",
      "L1 loss: 0.08026865124702454\n",
      "Training batch 18 with loss 0.29553\n",
      "------------------------------------------\n",
      "L1 loss: 0.060332994908094406\n",
      "Training batch 19 with loss 0.22155\n",
      "------------------------------------------\n",
      "L1 loss: 0.07345813512802124\n",
      "Training batch 20 with loss 0.31664\n",
      "------------------------------------------\n",
      "L1 loss: 0.07185304909944534\n",
      "Training batch 21 with loss 0.28853\n",
      "------------------------------------------\n",
      "L1 loss: 0.06874407082796097\n",
      "Training batch 22 with loss 0.30152\n",
      "------------------------------------------\n",
      "L1 loss: 0.09038602560758591\n",
      "Training batch 23 with loss 0.35428\n",
      "------------------------------------------\n",
      "L1 loss: 0.06730035692453384\n",
      "Training batch 24 with loss 0.31127\n",
      "------------------------------------------\n",
      "L1 loss: 0.07594946771860123\n",
      "Training batch 25 with loss 0.30910\n",
      "------------------------------------------\n",
      "L1 loss: 0.07572613656520844\n",
      "Training batch 26 with loss 0.28226\n",
      "------------------------------------------\n",
      "L1 loss: 0.056568630039691925\n",
      "Training batch 27 with loss 0.29686\n",
      "------------------------------------------\n",
      "L1 loss: 0.05730568617582321\n",
      "Training batch 28 with loss 0.28981\n",
      "------------------------------------------\n",
      "L1 loss: 0.054062362760305405\n",
      "Training batch 29 with loss 0.23409\n",
      "------------------------------------------\n",
      "L1 loss: 0.05874588340520859\n",
      "Training batch 30 with loss 0.27903\n",
      "------------------------------------------\n",
      "L1 loss: 0.06517665833234787\n",
      "Training batch 31 with loss 0.29367\n",
      "------------------------------------------\n",
      "L1 loss: 0.07003972679376602\n",
      "Training batch 32 with loss 0.27738\n",
      "------------------------------------------\n",
      "L1 loss: 0.06628554314374924\n",
      "Training batch 33 with loss 0.28131\n",
      "------------------------------------------\n",
      "L1 loss: 0.07752399146556854\n",
      "Training batch 34 with loss 0.31560\n",
      "------------------------------------------\n",
      "L1 loss: 0.09412961453199387\n",
      "Training batch 35 with loss 0.32866\n",
      "------------------------------------------\n",
      "L1 loss: 0.08178681880235672\n",
      "Training batch 36 with loss 0.27216\n",
      "------------------------------------------\n",
      "L1 loss: 0.07177621126174927\n",
      "Training batch 37 with loss 0.30947\n",
      "------------------------------------------\n",
      "L1 loss: 0.061788056045770645\n",
      "Training batch 38 with loss 0.21827\n",
      "------------------------------------------\n",
      "L1 loss: 0.06816690415143967\n",
      "Training batch 39 with loss 0.29810\n",
      "------------------------------------------\n",
      "L1 loss: 0.0701010450720787\n",
      "Training batch 40 with loss 0.26667\n",
      "------------------------------------------\n",
      "L1 loss: 0.05672746151685715\n",
      "Training batch 41 with loss 0.27667\n",
      "------------------------------------------\n",
      "L1 loss: 0.07730777561664581\n",
      "Training batch 42 with loss 0.27756\n",
      "------------------------------------------\n",
      "L1 loss: 0.07107924669981003\n",
      "Training batch 43 with loss 0.28650\n",
      "------------------------------------------\n",
      "L1 loss: 0.0571482889354229\n",
      "Training batch 44 with loss 0.25737\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.09240339696407318\n",
      "Training batch 45 with loss 0.35750\n",
      "------------------------------------------\n",
      "L1 loss: 0.07099977135658264\n",
      "Training batch 46 with loss 0.27554\n",
      "------------------------------------------\n",
      "L1 loss: 0.11168263107538223\n",
      "Training batch 47 with loss 0.36086\n",
      "------------------------------------------\n",
      "L1 loss: 0.04721876606345177\n",
      "Training batch 48 with loss 0.25998\n",
      "------------------------------------------\n",
      "L1 loss: 0.07850990444421768\n",
      "Training batch 49 with loss 0.34814\n",
      "------------------------------------------\n",
      "L1 loss: 0.07212536036968231\n",
      "Training batch 50 with loss 0.28198\n",
      "------------------------------------------\n",
      "L1 loss: 0.054774362593889236\n",
      "Training batch 51 with loss 0.26367\n",
      "------------------------------------------\n",
      "L1 loss: 0.09068646281957626\n",
      "Training batch 52 with loss 0.28914\n",
      "------------------------------------------\n",
      "L1 loss: 0.08081568032503128\n",
      "Training batch 53 with loss 0.28970\n",
      "------------------------------------------\n",
      "L1 loss: 0.06026259809732437\n",
      "Training batch 54 with loss 0.36711\n",
      "------------------------------------------\n",
      "L1 loss: 0.06327670067548752\n",
      "Training batch 55 with loss 0.30757\n",
      "------------------------------------------\n",
      "L1 loss: 0.054125331342220306\n",
      "Training batch 56 with loss 0.24452\n",
      "------------------------------------------\n",
      "L1 loss: 0.06546563655138016\n",
      "Training batch 57 with loss 0.26759\n",
      "------------------------------------------\n",
      "L1 loss: 0.06950422376394272\n",
      "Training batch 58 with loss 0.26463\n",
      "------------------------------------------\n",
      "L1 loss: 0.08020514994859695\n",
      "Training batch 59 with loss 0.28884\n",
      "------------------------------------------\n",
      "L1 loss: 0.06395592540502548\n",
      "Training batch 60 with loss 0.27574\n",
      "------------------------------------------\n",
      "L1 loss: 0.09365678578615189\n",
      "Training batch 61 with loss 0.33926\n",
      "------------------------------------------\n",
      "L1 loss: 0.05781480669975281\n",
      "Training batch 62 with loss 0.26211\n",
      "------------------------------------------\n",
      "L1 loss: 0.0627954974770546\n",
      "Training batch 63 with loss 0.26128\n",
      "------------------------------------------\n",
      "L1 loss: 0.05631715804338455\n",
      "Training batch 64 with loss 0.26886\n",
      "------------------------------------------\n",
      "L1 loss: 0.06436999887228012\n",
      "Training batch 65 with loss 0.24161\n",
      "------------------------------------------\n",
      "L1 loss: 0.06614940613508224\n",
      "Training batch 66 with loss 0.27307\n",
      "------------------------------------------\n",
      "L1 loss: 0.0664207860827446\n",
      "Training batch 67 with loss 0.26125\n",
      "------------------------------------------\n",
      "L1 loss: 0.07262703031301498\n",
      "Training batch 68 with loss 0.31212\n",
      "------------------------------------------\n",
      "L1 loss: 0.050387799739837646\n",
      "Training batch 69 with loss 0.26258\n",
      "------------------------------------------\n",
      "L1 loss: 0.07854045927524567\n",
      "Training batch 70 with loss 0.28820\n",
      "------------------------------------------\n",
      "L1 loss: 0.07144395262002945\n",
      "Training batch 71 with loss 0.29665\n",
      "------------------------------------------\n",
      "L1 loss: 0.08032459765672684\n",
      "Training batch 72 with loss 0.29856\n",
      "------------------------------------------\n",
      "L1 loss: 0.07180378586053848\n",
      "Training batch 73 with loss 0.28578\n",
      "------------------------------------------\n",
      "L1 loss: 0.09821587055921555\n",
      "Training batch 74 with loss 0.32135\n",
      "------------------------------------------\n",
      "L1 loss: 0.07385271787643433\n",
      "Training batch 75 with loss 0.29024\n",
      "------------------------------------------\n",
      "L1 loss: 0.08617676794528961\n",
      "Training batch 76 with loss 0.29808\n",
      "------------------------------------------\n",
      "L1 loss: 0.0765736922621727\n",
      "Training batch 77 with loss 0.27315\n",
      "------------------------------------------\n",
      "L1 loss: 0.07919984310865402\n",
      "Training batch 78 with loss 0.30680\n",
      "------------------------------------------\n",
      "L1 loss: 0.10646399110555649\n",
      "Training batch 79 with loss 0.37532\n",
      "------------------------------------------\n",
      "L1 loss: 0.07731467485427856\n",
      "Training batch 80 with loss 0.26999\n",
      "------------------------------------------\n",
      "L1 loss: 0.06609099358320236\n",
      "Training batch 81 with loss 0.47520\n",
      "------------------------------------------\n",
      "L1 loss: 0.08035793900489807\n",
      "Training batch 82 with loss 0.36044\n",
      "------------------------------------------\n",
      "L1 loss: 0.0759427398443222\n",
      "Training batch 83 with loss 0.27509\n",
      "------------------------------------------\n",
      "L1 loss: 0.06831709295511246\n",
      "Training batch 84 with loss 0.30078\n",
      "------------------------------------------\n",
      "L1 loss: 0.06845913082361221\n",
      "Training batch 85 with loss 0.29089\n",
      "------------------------------------------\n",
      "L1 loss: 0.08233527094125748\n",
      "Training batch 86 with loss 0.30053\n",
      "------------------------------------------\n",
      "L1 loss: 0.10122041404247284\n",
      "Training batch 87 with loss 0.40361\n",
      "------------------------------------------\n",
      "L1 loss: 0.07781804352998734\n",
      "Training batch 88 with loss 0.32078\n",
      "------------------------------------------\n",
      "L1 loss: 0.06956750154495239\n",
      "Training batch 89 with loss 0.26875\n",
      "------------------------------------------\n",
      "L1 loss: 0.07469406723976135\n",
      "Training batch 90 with loss 0.27628\n",
      "------------------------------------------\n",
      "L1 loss: 0.06516952812671661\n",
      "Training batch 91 with loss 0.26090\n",
      "------------------------------------------\n",
      "L1 loss: 0.0934765636920929\n",
      "Training batch 92 with loss 0.29781\n",
      "------------------------------------------\n",
      "L1 loss: 0.07322200387716293\n",
      "Training batch 93 with loss 0.26732\n",
      "------------------------------------------\n",
      "L1 loss: 0.08465038239955902\n",
      "Training batch 94 with loss 0.29968\n",
      "------------------------------------------\n",
      "L1 loss: 0.07114531844854355\n",
      "Training batch 95 with loss 0.25114\n",
      "------------------------------------------\n",
      "L1 loss: 0.06443050503730774\n",
      "Training batch 96 with loss 0.28297\n",
      "------------------------------------------\n",
      "L1 loss: 0.08789918571710587\n",
      "Training batch 97 with loss 0.37257\n",
      "------------------------------------------\n",
      "L1 loss: 0.05591607838869095\n",
      "Training batch 98 with loss 0.23904\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394932419061661\n",
      "Training batch 99 with loss 0.26429\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29510545149445533\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4876\n",
      "------------------------------------------\n",
      "L1 loss: 0.06421853601932526\n",
      "Training batch 0 with loss 0.25125\n",
      "------------------------------------------\n",
      "L1 loss: 0.08190005272626877\n",
      "Training batch 1 with loss 0.27764\n",
      "------------------------------------------\n",
      "L1 loss: 0.07318629324436188\n",
      "Training batch 2 with loss 0.36039\n",
      "------------------------------------------\n",
      "L1 loss: 0.07052775472402573\n",
      "Training batch 3 with loss 0.40976\n",
      "------------------------------------------\n",
      "L1 loss: 0.08473765105009079\n",
      "Training batch 4 with loss 0.32572\n",
      "------------------------------------------\n",
      "L1 loss: 0.06877893209457397\n",
      "Training batch 5 with loss 0.35121\n",
      "------------------------------------------\n",
      "L1 loss: 0.07966459542512894\n",
      "Training batch 6 with loss 0.26908\n",
      "------------------------------------------\n",
      "L1 loss: 0.08348816633224487\n",
      "Training batch 7 with loss 0.35702\n",
      "------------------------------------------\n",
      "L1 loss: 0.08072181791067123\n",
      "Training batch 8 with loss 0.33190\n",
      "------------------------------------------\n",
      "L1 loss: 0.059356849640607834\n",
      "Training batch 9 with loss 0.25038\n",
      "------------------------------------------\n",
      "L1 loss: 0.08240529149770737\n",
      "Training batch 10 with loss 0.30753\n",
      "------------------------------------------\n",
      "L1 loss: 0.06266162544488907\n",
      "Training batch 11 with loss 0.31340\n",
      "------------------------------------------\n",
      "L1 loss: 0.07286499440670013\n",
      "Training batch 12 with loss 0.31262\n",
      "------------------------------------------\n",
      "L1 loss: 0.06398553401231766\n",
      "Training batch 13 with loss 0.70482\n",
      "------------------------------------------\n",
      "L1 loss: 0.052802179008722305\n",
      "Training batch 14 with loss 0.26082\n",
      "------------------------------------------\n",
      "L1 loss: 0.08249029517173767\n",
      "Training batch 15 with loss 0.27334\n",
      "------------------------------------------\n",
      "L1 loss: 0.06313172727823257\n",
      "Training batch 16 with loss 0.26796\n",
      "------------------------------------------\n",
      "L1 loss: 0.08755325525999069\n",
      "Training batch 17 with loss 0.35550\n",
      "------------------------------------------\n",
      "L1 loss: 0.07974743098020554\n",
      "Training batch 18 with loss 0.30782\n",
      "------------------------------------------\n",
      "L1 loss: 0.06091870367527008\n",
      "Training batch 19 with loss 0.22873\n",
      "------------------------------------------\n",
      "L1 loss: 0.07449935376644135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 20 with loss 0.31506\n",
      "------------------------------------------\n",
      "L1 loss: 0.0737987607717514\n",
      "Training batch 21 with loss 0.28108\n",
      "------------------------------------------\n",
      "L1 loss: 0.0676349326968193\n",
      "Training batch 22 with loss 0.30101\n",
      "------------------------------------------\n",
      "L1 loss: 0.08888933807611465\n",
      "Training batch 23 with loss 0.36716\n",
      "------------------------------------------\n",
      "L1 loss: 0.06810833513736725\n",
      "Training batch 24 with loss 0.30504\n",
      "------------------------------------------\n",
      "L1 loss: 0.0733218714594841\n",
      "Training batch 25 with loss 0.29050\n",
      "------------------------------------------\n",
      "L1 loss: 0.07616733014583588\n",
      "Training batch 26 with loss 0.28760\n",
      "------------------------------------------\n",
      "L1 loss: 0.05719597265124321\n",
      "Training batch 27 with loss 0.29479\n",
      "------------------------------------------\n",
      "L1 loss: 0.05769931152462959\n",
      "Training batch 28 with loss 0.27674\n",
      "------------------------------------------\n",
      "L1 loss: 0.05295393988490105\n",
      "Training batch 29 with loss 0.24078\n",
      "------------------------------------------\n",
      "L1 loss: 0.06228811666369438\n",
      "Training batch 30 with loss 0.26204\n",
      "------------------------------------------\n",
      "L1 loss: 0.06526350975036621\n",
      "Training batch 31 with loss 0.29731\n",
      "------------------------------------------\n",
      "L1 loss: 0.06993013620376587\n",
      "Training batch 32 with loss 0.27326\n",
      "------------------------------------------\n",
      "L1 loss: 0.060138314962387085\n",
      "Training batch 33 with loss 0.24772\n",
      "------------------------------------------\n",
      "L1 loss: 0.0767425075173378\n",
      "Training batch 34 with loss 0.34839\n",
      "------------------------------------------\n",
      "L1 loss: 0.09604524821043015\n",
      "Training batch 35 with loss 0.33968\n",
      "------------------------------------------\n",
      "L1 loss: 0.08654898405075073\n",
      "Training batch 36 with loss 0.28379\n",
      "------------------------------------------\n",
      "L1 loss: 0.06688840687274933\n",
      "Training batch 37 with loss 0.24948\n",
      "------------------------------------------\n",
      "L1 loss: 0.061838727444410324\n",
      "Training batch 38 with loss 0.22071\n",
      "------------------------------------------\n",
      "L1 loss: 0.06596503406763077\n",
      "Training batch 39 with loss 0.28311\n",
      "------------------------------------------\n",
      "L1 loss: 0.06976023316383362\n",
      "Training batch 40 with loss 0.25740\n",
      "------------------------------------------\n",
      "L1 loss: 0.05744773522019386\n",
      "Training batch 41 with loss 0.27888\n",
      "------------------------------------------\n",
      "L1 loss: 0.08358915150165558\n",
      "Training batch 42 with loss 0.28427\n",
      "------------------------------------------\n",
      "L1 loss: 0.069691002368927\n",
      "Training batch 43 with loss 0.26742\n",
      "------------------------------------------\n",
      "L1 loss: 0.05817640572786331\n",
      "Training batch 44 with loss 0.26307\n",
      "------------------------------------------\n",
      "L1 loss: 0.09310305863618851\n",
      "Training batch 45 with loss 0.35235\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990113854408264\n",
      "Training batch 46 with loss 0.27437\n",
      "------------------------------------------\n",
      "L1 loss: 0.1103585734963417\n",
      "Training batch 47 with loss 0.36253\n",
      "------------------------------------------\n",
      "L1 loss: 0.045458074659109116\n",
      "Training batch 48 with loss 0.22979\n",
      "------------------------------------------\n",
      "L1 loss: 0.08266019821166992\n",
      "Training batch 49 with loss 0.34434\n",
      "------------------------------------------\n",
      "L1 loss: 0.07327986508607864\n",
      "Training batch 50 with loss 0.28551\n",
      "------------------------------------------\n",
      "L1 loss: 0.08119232952594757\n",
      "Training batch 51 with loss 0.34591\n",
      "------------------------------------------\n",
      "L1 loss: 0.08975410461425781\n",
      "Training batch 52 with loss 0.29561\n",
      "------------------------------------------\n",
      "L1 loss: 0.08075088262557983\n",
      "Training batch 53 with loss 0.28366\n",
      "------------------------------------------\n",
      "L1 loss: 0.07574000954627991\n",
      "Training batch 54 with loss 0.26713\n",
      "------------------------------------------\n",
      "L1 loss: 0.06326209008693695\n",
      "Training batch 55 with loss 0.29716\n",
      "------------------------------------------\n",
      "L1 loss: 0.05771651118993759\n",
      "Training batch 56 with loss 0.25507\n",
      "------------------------------------------\n",
      "L1 loss: 0.07106869667768478\n",
      "Training batch 57 with loss 0.28483\n",
      "------------------------------------------\n",
      "L1 loss: 0.06399564445018768\n",
      "Training batch 58 with loss 0.25117\n",
      "------------------------------------------\n",
      "L1 loss: 0.07899980992078781\n",
      "Training batch 59 with loss 0.30776\n",
      "------------------------------------------\n",
      "L1 loss: 0.06314414739608765\n",
      "Training batch 60 with loss 0.29191\n",
      "------------------------------------------\n",
      "L1 loss: 0.09169204533100128\n",
      "Training batch 61 with loss 0.34638\n",
      "------------------------------------------\n",
      "L1 loss: 0.048689767718315125\n",
      "Training batch 62 with loss 0.35303\n",
      "------------------------------------------\n",
      "L1 loss: 0.06157049536705017\n",
      "Training batch 63 with loss 0.24641\n",
      "------------------------------------------\n",
      "L1 loss: 0.07275382429361343\n",
      "Training batch 64 with loss 0.28515\n",
      "------------------------------------------\n",
      "L1 loss: 0.06379584968090057\n",
      "Training batch 65 with loss 0.23425\n",
      "------------------------------------------\n",
      "L1 loss: 0.06856359541416168\n",
      "Training batch 66 with loss 0.27605\n",
      "------------------------------------------\n",
      "L1 loss: 0.06531144678592682\n",
      "Training batch 67 with loss 0.26309\n",
      "------------------------------------------\n",
      "L1 loss: 0.07274317741394043\n",
      "Training batch 68 with loss 0.29802\n",
      "------------------------------------------\n",
      "L1 loss: 0.03700774163007736\n",
      "Training batch 69 with loss 0.22124\n",
      "------------------------------------------\n",
      "L1 loss: 0.07965114712715149\n",
      "Training batch 70 with loss 0.30909\n",
      "------------------------------------------\n",
      "L1 loss: 0.07265395671129227\n",
      "Training batch 71 with loss 0.28807\n",
      "------------------------------------------\n",
      "L1 loss: 0.08247746527194977\n",
      "Training batch 72 with loss 0.29436\n",
      "------------------------------------------\n",
      "L1 loss: 0.07290935516357422\n",
      "Training batch 73 with loss 0.29514\n",
      "------------------------------------------\n",
      "L1 loss: 0.09896863251924515\n",
      "Training batch 74 with loss 0.32941\n",
      "------------------------------------------\n",
      "L1 loss: 0.07501525431871414\n",
      "Training batch 75 with loss 0.27661\n",
      "------------------------------------------\n",
      "L1 loss: 0.08597537130117416\n",
      "Training batch 76 with loss 0.30184\n",
      "------------------------------------------\n",
      "L1 loss: 0.07619389146566391\n",
      "Training batch 77 with loss 0.26922\n",
      "------------------------------------------\n",
      "L1 loss: 0.0708688423037529\n",
      "Training batch 78 with loss 0.41591\n",
      "------------------------------------------\n",
      "L1 loss: 0.10465393960475922\n",
      "Training batch 79 with loss 0.36453\n",
      "------------------------------------------\n",
      "L1 loss: 0.0758177638053894\n",
      "Training batch 80 with loss 0.28755\n",
      "------------------------------------------\n",
      "L1 loss: 0.07209113985300064\n",
      "Training batch 81 with loss 0.27580\n",
      "------------------------------------------\n",
      "L1 loss: 0.08388295769691467\n",
      "Training batch 82 with loss 0.34256\n",
      "------------------------------------------\n",
      "L1 loss: 0.07654494047164917\n",
      "Training batch 83 with loss 0.28207\n",
      "------------------------------------------\n",
      "L1 loss: 0.07133462280035019\n",
      "Training batch 84 with loss 0.30885\n",
      "------------------------------------------\n",
      "L1 loss: 0.06377647817134857\n",
      "Training batch 85 with loss 0.28302\n",
      "------------------------------------------\n",
      "L1 loss: 0.07347739487886429\n",
      "Training batch 86 with loss 0.56286\n",
      "------------------------------------------\n",
      "L1 loss: 0.10242029279470444\n",
      "Training batch 87 with loss 0.40782\n",
      "------------------------------------------\n",
      "L1 loss: 0.07422293722629547\n",
      "Training batch 88 with loss 0.32092\n",
      "------------------------------------------\n",
      "L1 loss: 0.0708499550819397\n",
      "Training batch 89 with loss 0.26686\n",
      "------------------------------------------\n",
      "L1 loss: 0.07559502869844437\n",
      "Training batch 90 with loss 0.28269\n",
      "------------------------------------------\n",
      "L1 loss: 0.06391222029924393\n",
      "Training batch 91 with loss 0.25004\n",
      "------------------------------------------\n",
      "L1 loss: 0.09436427056789398\n",
      "Training batch 92 with loss 0.30192\n",
      "------------------------------------------\n",
      "L1 loss: 0.0690469816327095\n",
      "Training batch 93 with loss 0.26855\n",
      "------------------------------------------\n",
      "L1 loss: 0.08452615886926651\n",
      "Training batch 94 with loss 0.29161\n",
      "------------------------------------------\n",
      "L1 loss: 0.06991596519947052\n",
      "Training batch 95 with loss 0.24968\n",
      "------------------------------------------\n",
      "L1 loss: 0.06750840693712234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 96 with loss 0.30369\n",
      "------------------------------------------\n",
      "L1 loss: 0.08534207940101624\n",
      "Training batch 97 with loss 0.34935\n",
      "------------------------------------------\n",
      "L1 loss: 0.05742540583014488\n",
      "Training batch 98 with loss 0.24583\n",
      "------------------------------------------\n",
      "L1 loss: 0.06751610338687897\n",
      "Training batch 99 with loss 0.27854\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.3019025757908821\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4877\n",
      "------------------------------------------\n",
      "L1 loss: 0.05985381454229355\n",
      "Training batch 0 with loss 0.25042\n",
      "------------------------------------------\n",
      "L1 loss: 0.08458725363016129\n",
      "Training batch 1 with loss 0.28433\n",
      "------------------------------------------\n",
      "L1 loss: 0.09202732890844345\n",
      "Training batch 2 with loss 0.32723\n",
      "------------------------------------------\n",
      "L1 loss: 0.0725204274058342\n",
      "Training batch 3 with loss 0.25329\n",
      "------------------------------------------\n",
      "L1 loss: 0.08628679066896439\n",
      "Training batch 4 with loss 0.32469\n",
      "------------------------------------------\n",
      "L1 loss: 0.06340697407722473\n",
      "Training batch 5 with loss 0.32839\n",
      "------------------------------------------\n",
      "L1 loss: 0.08092465996742249\n",
      "Training batch 6 with loss 0.26795\n",
      "------------------------------------------\n",
      "L1 loss: 0.08415284007787704\n",
      "Training batch 7 with loss 0.32047\n",
      "------------------------------------------\n",
      "L1 loss: 0.07941589504480362\n",
      "Training batch 8 with loss 0.32093\n",
      "------------------------------------------\n",
      "L1 loss: 0.061653245240449905\n",
      "Training batch 9 with loss 0.25589\n",
      "------------------------------------------\n",
      "L1 loss: 0.08052276819944382\n",
      "Training batch 10 with loss 0.30300\n",
      "------------------------------------------\n",
      "L1 loss: 0.061505936086177826\n",
      "Training batch 11 with loss 0.28677\n",
      "------------------------------------------\n",
      "L1 loss: 0.07373996078968048\n",
      "Training batch 12 with loss 0.30234\n",
      "------------------------------------------\n",
      "L1 loss: 0.06910792738199234\n",
      "Training batch 13 with loss 0.24869\n",
      "------------------------------------------\n",
      "L1 loss: 0.05281393602490425\n",
      "Training batch 14 with loss 0.23632\n",
      "------------------------------------------\n",
      "L1 loss: 0.085971400141716\n",
      "Training batch 15 with loss 0.27067\n",
      "------------------------------------------\n",
      "L1 loss: 0.05734692141413689\n",
      "Training batch 16 with loss 0.35921\n",
      "------------------------------------------\n",
      "L1 loss: 0.08195808529853821\n",
      "Training batch 17 with loss 0.32248\n",
      "------------------------------------------\n",
      "L1 loss: 0.0813458263874054\n",
      "Training batch 18 with loss 0.29633\n",
      "------------------------------------------\n",
      "L1 loss: 0.06121993064880371\n",
      "Training batch 19 with loss 0.23623\n",
      "------------------------------------------\n",
      "L1 loss: 0.07326357811689377\n",
      "Training batch 20 with loss 0.33026\n",
      "------------------------------------------\n",
      "L1 loss: 0.07099266350269318\n",
      "Training batch 21 with loss 0.28127\n",
      "------------------------------------------\n",
      "L1 loss: 0.06892016530036926\n",
      "Training batch 22 with loss 0.28568\n",
      "------------------------------------------\n",
      "L1 loss: 0.08689175546169281\n",
      "Training batch 23 with loss 0.35597\n",
      "------------------------------------------\n",
      "L1 loss: 0.0691053718328476\n",
      "Training batch 24 with loss 0.29262\n",
      "------------------------------------------\n",
      "L1 loss: 0.07547415047883987\n",
      "Training batch 25 with loss 0.30738\n",
      "------------------------------------------\n",
      "L1 loss: 0.07686058431863785\n",
      "Training batch 26 with loss 0.28941\n",
      "------------------------------------------\n",
      "L1 loss: 0.057165540754795074\n",
      "Training batch 27 with loss 0.28191\n",
      "------------------------------------------\n",
      "L1 loss: 0.05724313482642174\n",
      "Training batch 28 with loss 0.28697\n",
      "------------------------------------------\n",
      "L1 loss: 0.055468108505010605\n",
      "Training batch 29 with loss 0.23113\n",
      "------------------------------------------\n",
      "L1 loss: 0.05996463820338249\n",
      "Training batch 30 with loss 0.26963\n",
      "------------------------------------------\n",
      "L1 loss: 0.05669284611940384\n",
      "Training batch 31 with loss 0.31414\n",
      "------------------------------------------\n",
      "L1 loss: 0.06919702142477036\n",
      "Training batch 32 with loss 0.27745\n",
      "------------------------------------------\n",
      "L1 loss: 0.06466024369001389\n",
      "Training batch 33 with loss 0.26881\n",
      "------------------------------------------\n",
      "L1 loss: 0.07473006844520569\n",
      "Training batch 34 with loss 0.29334\n",
      "------------------------------------------\n",
      "L1 loss: 0.09682357311248779\n",
      "Training batch 35 with loss 0.33488\n",
      "------------------------------------------\n",
      "L1 loss: 0.07629023492336273\n",
      "Training batch 36 with loss 0.38216\n",
      "------------------------------------------\n",
      "L1 loss: 0.06804827600717545\n",
      "Training batch 37 with loss 0.25958\n",
      "------------------------------------------\n",
      "L1 loss: 0.0640602707862854\n",
      "Training batch 38 with loss 0.21552\n",
      "------------------------------------------\n",
      "L1 loss: 0.06793317943811417\n",
      "Training batch 39 with loss 0.29881\n",
      "------------------------------------------\n",
      "L1 loss: 0.06955781579017639\n",
      "Training batch 40 with loss 0.25886\n",
      "------------------------------------------\n",
      "L1 loss: 0.05907721817493439\n",
      "Training batch 41 with loss 0.29708\n",
      "------------------------------------------\n",
      "L1 loss: 0.06346592307090759\n",
      "Training batch 42 with loss 0.41573\n",
      "------------------------------------------\n",
      "L1 loss: 0.06896203756332397\n",
      "Training batch 43 with loss 0.27210\n",
      "------------------------------------------\n",
      "L1 loss: 0.05758628249168396\n",
      "Training batch 44 with loss 0.27640\n",
      "------------------------------------------\n",
      "L1 loss: 0.09261639416217804\n",
      "Training batch 45 with loss 0.35395\n",
      "------------------------------------------\n",
      "L1 loss: 0.07482881098985672\n",
      "Training batch 46 with loss 0.26879\n",
      "------------------------------------------\n",
      "L1 loss: 0.11114927381277084\n",
      "Training batch 47 with loss 0.35967\n",
      "------------------------------------------\n",
      "L1 loss: 0.047614485025405884\n",
      "Training batch 48 with loss 0.23531\n",
      "------------------------------------------\n",
      "L1 loss: 0.08193562924861908\n",
      "Training batch 49 with loss 0.36665\n",
      "------------------------------------------\n",
      "L1 loss: 0.07163427025079727\n",
      "Training batch 50 with loss 0.30110\n",
      "------------------------------------------\n",
      "L1 loss: 0.08122401684522629\n",
      "Training batch 51 with loss 0.32590\n",
      "------------------------------------------\n",
      "L1 loss: 0.08971748501062393\n",
      "Training batch 52 with loss 0.28239\n",
      "------------------------------------------\n",
      "L1 loss: 0.07987106591463089\n",
      "Training batch 53 with loss 0.29988\n",
      "------------------------------------------\n",
      "L1 loss: 0.07834197580814362\n",
      "Training batch 54 with loss 0.26904\n",
      "------------------------------------------\n",
      "L1 loss: 0.05870811268687248\n",
      "Training batch 55 with loss 0.29860\n",
      "------------------------------------------\n",
      "L1 loss: 0.054285336285829544\n",
      "Training batch 56 with loss 0.23562\n",
      "------------------------------------------\n",
      "L1 loss: 0.0680670365691185\n",
      "Training batch 57 with loss 0.27385\n",
      "------------------------------------------\n",
      "L1 loss: 0.06368966400623322\n",
      "Training batch 58 with loss 0.26679\n",
      "------------------------------------------\n",
      "L1 loss: 0.08213172107934952\n",
      "Training batch 59 with loss 0.29214\n",
      "------------------------------------------\n",
      "L1 loss: 0.04949185624718666\n",
      "Training batch 60 with loss 0.28678\n",
      "------------------------------------------\n",
      "L1 loss: 0.10359331220388412\n",
      "Training batch 61 with loss 0.36623\n",
      "------------------------------------------\n",
      "L1 loss: 0.055888477712869644\n",
      "Training batch 62 with loss 0.25871\n",
      "------------------------------------------\n",
      "L1 loss: 0.06203527748584747\n",
      "Training batch 63 with loss 0.26183\n",
      "------------------------------------------\n",
      "L1 loss: 0.0710553228855133\n",
      "Training batch 64 with loss 0.28368\n",
      "------------------------------------------\n",
      "L1 loss: 0.058550119400024414\n",
      "Training batch 65 with loss 0.21948\n",
      "------------------------------------------\n",
      "L1 loss: 0.06748774647712708\n",
      "Training batch 66 with loss 0.26395\n",
      "------------------------------------------\n",
      "L1 loss: 0.06538958847522736\n",
      "Training batch 67 with loss 0.25913\n",
      "------------------------------------------\n",
      "L1 loss: 0.07510290294885635\n",
      "Training batch 68 with loss 0.30162\n",
      "------------------------------------------\n",
      "L1 loss: 0.05126771330833435\n",
      "Training batch 69 with loss 0.26275\n",
      "------------------------------------------\n",
      "L1 loss: 0.0730103850364685\n",
      "Training batch 70 with loss 0.29901\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06090029701590538\n",
      "Training batch 71 with loss 0.38395\n",
      "------------------------------------------\n",
      "L1 loss: 0.08405362069606781\n",
      "Training batch 72 with loss 0.30358\n",
      "------------------------------------------\n",
      "L1 loss: 0.07182852178812027\n",
      "Training batch 73 with loss 0.29147\n",
      "------------------------------------------\n",
      "L1 loss: 0.09767179936170578\n",
      "Training batch 74 with loss 0.31508\n",
      "------------------------------------------\n",
      "L1 loss: 0.07346818596124649\n",
      "Training batch 75 with loss 0.27327\n",
      "------------------------------------------\n",
      "L1 loss: 0.08580749481916428\n",
      "Training batch 76 with loss 0.30100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08059221506118774\n",
      "Training batch 77 with loss 0.28120\n",
      "------------------------------------------\n",
      "L1 loss: 0.0803413912653923\n",
      "Training batch 78 with loss 0.30903\n",
      "------------------------------------------\n",
      "L1 loss: 0.10453182458877563\n",
      "Training batch 79 with loss 0.35872\n",
      "------------------------------------------\n",
      "L1 loss: 0.07637285441160202\n",
      "Training batch 80 with loss 0.27612\n",
      "------------------------------------------\n",
      "L1 loss: 0.08297942578792572\n",
      "Training batch 81 with loss 0.31351\n",
      "------------------------------------------\n",
      "L1 loss: 0.08581441640853882\n",
      "Training batch 82 with loss 0.34999\n",
      "------------------------------------------\n",
      "L1 loss: 0.07817097753286362\n",
      "Training batch 83 with loss 0.27574\n",
      "------------------------------------------\n",
      "L1 loss: 0.07136226445436478\n",
      "Training batch 84 with loss 0.31798\n",
      "------------------------------------------\n",
      "L1 loss: 0.06287205964326859\n",
      "Training batch 85 with loss 0.26463\n",
      "------------------------------------------\n",
      "L1 loss: 0.08352179825305939\n",
      "Training batch 86 with loss 0.31949\n",
      "------------------------------------------\n",
      "L1 loss: 0.10226589441299438\n",
      "Training batch 87 with loss 0.38659\n",
      "------------------------------------------\n",
      "L1 loss: 0.06602661311626434\n",
      "Training batch 88 with loss 0.31777\n",
      "------------------------------------------\n",
      "L1 loss: 0.06936077028512955\n",
      "Training batch 89 with loss 0.27625\n",
      "------------------------------------------\n",
      "L1 loss: 0.07239120453596115\n",
      "Training batch 90 with loss 0.27448\n",
      "------------------------------------------\n",
      "L1 loss: 0.06225759536027908\n",
      "Training batch 91 with loss 0.25936\n",
      "------------------------------------------\n",
      "L1 loss: 0.09483188390731812\n",
      "Training batch 92 with loss 0.30543\n",
      "------------------------------------------\n",
      "L1 loss: 0.05715521052479744\n",
      "Training batch 93 with loss 0.23610\n",
      "------------------------------------------\n",
      "L1 loss: 0.08242171257734299\n",
      "Training batch 94 with loss 0.29342\n",
      "------------------------------------------\n",
      "L1 loss: 0.07092151045799255\n",
      "Training batch 95 with loss 0.25276\n",
      "------------------------------------------\n",
      "L1 loss: 0.06604212522506714\n",
      "Training batch 96 with loss 0.28902\n",
      "------------------------------------------\n",
      "L1 loss: 0.08285559713840485\n",
      "Training batch 97 with loss 0.36156\n",
      "------------------------------------------\n",
      "L1 loss: 0.056487709283828735\n",
      "Training batch 98 with loss 0.23408\n",
      "------------------------------------------\n",
      "L1 loss: 0.06550537049770355\n",
      "Training batch 99 with loss 0.26747\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29354640513658525\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4878\n",
      "------------------------------------------\n",
      "L1 loss: 0.06048822030425072\n",
      "Training batch 0 with loss 0.24395\n",
      "------------------------------------------\n",
      "L1 loss: 0.08516057580709457\n",
      "Training batch 1 with loss 0.28773\n",
      "------------------------------------------\n",
      "L1 loss: 0.09014661610126495\n",
      "Training batch 2 with loss 0.32176\n",
      "------------------------------------------\n",
      "L1 loss: 0.07110593467950821\n",
      "Training batch 3 with loss 0.26559\n",
      "------------------------------------------\n",
      "L1 loss: 0.08483925461769104\n",
      "Training batch 4 with loss 0.31875\n",
      "------------------------------------------\n",
      "L1 loss: 0.06733399629592896\n",
      "Training batch 5 with loss 0.32235\n",
      "------------------------------------------\n",
      "L1 loss: 0.07871533930301666\n",
      "Training batch 6 with loss 0.26806\n",
      "------------------------------------------\n",
      "L1 loss: 0.07526865601539612\n",
      "Training batch 7 with loss 0.44291\n",
      "------------------------------------------\n",
      "L1 loss: 0.07973825186491013\n",
      "Training batch 8 with loss 0.30387\n",
      "------------------------------------------\n",
      "L1 loss: 0.0609138049185276\n",
      "Training batch 9 with loss 0.24594\n",
      "------------------------------------------\n",
      "L1 loss: 0.08067633211612701\n",
      "Training batch 10 with loss 0.29955\n",
      "------------------------------------------\n",
      "L1 loss: 0.06054873764514923\n",
      "Training batch 11 with loss 0.31987\n",
      "------------------------------------------\n",
      "L1 loss: 0.0770992785692215\n",
      "Training batch 12 with loss 0.31842\n",
      "------------------------------------------\n",
      "L1 loss: 0.06954368948936462\n",
      "Training batch 13 with loss 0.25251\n",
      "------------------------------------------\n",
      "L1 loss: 0.05234038457274437\n",
      "Training batch 14 with loss 0.25335\n",
      "------------------------------------------\n",
      "L1 loss: 0.08614093065261841\n",
      "Training batch 15 with loss 0.26545\n",
      "------------------------------------------\n",
      "L1 loss: 0.06347879022359848\n",
      "Training batch 16 with loss 0.25713\n",
      "------------------------------------------\n",
      "L1 loss: 0.08254138380289078\n",
      "Training batch 17 with loss 0.32684\n",
      "------------------------------------------\n",
      "L1 loss: 0.0809606984257698\n",
      "Training batch 18 with loss 0.29740\n",
      "------------------------------------------\n",
      "L1 loss: 0.054669201374053955\n",
      "Training batch 19 with loss 0.22561\n",
      "------------------------------------------\n",
      "L1 loss: 0.07274571806192398\n",
      "Training batch 20 with loss 0.31262\n",
      "------------------------------------------\n",
      "L1 loss: 0.07307084649801254\n",
      "Training batch 21 with loss 0.28267\n",
      "------------------------------------------\n",
      "L1 loss: 0.06937079131603241\n",
      "Training batch 22 with loss 0.28467\n",
      "------------------------------------------\n",
      "L1 loss: 0.0886324942111969\n",
      "Training batch 23 with loss 0.37043\n",
      "------------------------------------------\n",
      "L1 loss: 0.06806159764528275\n",
      "Training batch 24 with loss 0.29407\n",
      "------------------------------------------\n",
      "L1 loss: 0.07867012917995453\n",
      "Training batch 25 with loss 0.30715\n",
      "------------------------------------------\n",
      "L1 loss: 0.07488872110843658\n",
      "Training batch 26 with loss 0.28078\n",
      "------------------------------------------\n",
      "L1 loss: 0.058448996394872665\n",
      "Training batch 27 with loss 0.31510\n",
      "------------------------------------------\n",
      "L1 loss: 0.057811636477708817\n",
      "Training batch 28 with loss 0.29202\n",
      "------------------------------------------\n",
      "L1 loss: 0.05959625542163849\n",
      "Training batch 29 with loss 0.25905\n",
      "------------------------------------------\n",
      "L1 loss: 0.05882607772946358\n",
      "Training batch 30 with loss 0.26871\n",
      "------------------------------------------\n",
      "L1 loss: 0.061980511993169785\n",
      "Training batch 31 with loss 0.29698\n",
      "------------------------------------------\n",
      "L1 loss: 0.07154890149831772\n",
      "Training batch 32 with loss 0.29441\n",
      "------------------------------------------\n",
      "L1 loss: 0.06652156263589859\n",
      "Training batch 33 with loss 0.25804\n",
      "------------------------------------------\n",
      "L1 loss: 0.06563784182071686\n",
      "Training batch 34 with loss 0.43527\n",
      "------------------------------------------\n",
      "L1 loss: 0.09293040633201599\n",
      "Training batch 35 with loss 0.31942\n",
      "------------------------------------------\n",
      "L1 loss: 0.08727408200502396\n",
      "Training batch 36 with loss 0.27998\n",
      "------------------------------------------\n",
      "L1 loss: 0.06390481442213058\n",
      "Training batch 37 with loss 0.47578\n",
      "------------------------------------------\n",
      "L1 loss: 0.06273876130580902\n",
      "Training batch 38 with loss 0.22526\n",
      "------------------------------------------\n",
      "L1 loss: 0.06814227253198624\n",
      "Training batch 39 with loss 0.29814\n",
      "------------------------------------------\n",
      "L1 loss: 0.07413466274738312\n",
      "Training batch 40 with loss 0.26501\n",
      "------------------------------------------\n",
      "L1 loss: 0.06143151596188545\n",
      "Training batch 41 with loss 0.29467\n",
      "------------------------------------------\n",
      "L1 loss: 0.07863838970661163\n",
      "Training batch 42 with loss 0.26974\n",
      "------------------------------------------\n",
      "L1 loss: 0.07279159873723984\n",
      "Training batch 43 with loss 0.30971\n",
      "------------------------------------------\n",
      "L1 loss: 0.05821685492992401\n",
      "Training batch 44 with loss 0.26475\n",
      "------------------------------------------\n",
      "L1 loss: 0.09165159612894058\n",
      "Training batch 45 with loss 0.36579\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07477472722530365\n",
      "Training batch 46 with loss 0.27187\n",
      "------------------------------------------\n",
      "L1 loss: 0.11059019714593887\n",
      "Training batch 47 with loss 0.37234\n",
      "------------------------------------------\n",
      "L1 loss: 0.04952223226428032\n",
      "Training batch 48 with loss 0.24812\n",
      "------------------------------------------\n",
      "L1 loss: 0.0832357406616211\n",
      "Training batch 49 with loss 0.36210\n",
      "------------------------------------------\n",
      "L1 loss: 0.07313370704650879\n",
      "Training batch 50 with loss 0.28155\n",
      "------------------------------------------\n",
      "L1 loss: 0.08164706081151962\n",
      "Training batch 51 with loss 0.33522\n",
      "------------------------------------------\n",
      "L1 loss: 0.09131937474012375\n",
      "Training batch 52 with loss 0.29657\n",
      "------------------------------------------\n",
      "L1 loss: 0.07604672014713287\n",
      "Training batch 53 with loss 0.29318\n",
      "------------------------------------------\n",
      "L1 loss: 0.07306718081235886\n",
      "Training batch 54 with loss 0.25352\n",
      "------------------------------------------\n",
      "L1 loss: 0.05901831388473511\n",
      "Training batch 55 with loss 0.29804\n",
      "------------------------------------------\n",
      "L1 loss: 0.058099616318941116\n",
      "Training batch 56 with loss 0.25941\n",
      "------------------------------------------\n",
      "L1 loss: 0.06640408933162689\n",
      "Training batch 57 with loss 0.28381\n",
      "------------------------------------------\n",
      "L1 loss: 0.06492304056882858\n",
      "Training batch 58 with loss 0.25614\n",
      "------------------------------------------\n",
      "L1 loss: 0.08055377006530762\n",
      "Training batch 59 with loss 0.28312\n",
      "------------------------------------------\n",
      "L1 loss: 0.06253179162740707\n",
      "Training batch 60 with loss 0.27008\n",
      "------------------------------------------\n",
      "L1 loss: 0.09290999174118042\n",
      "Training batch 61 with loss 0.34667\n",
      "------------------------------------------\n",
      "L1 loss: 0.05712205916643143\n",
      "Training batch 62 with loss 0.25565\n",
      "------------------------------------------\n",
      "L1 loss: 0.06249215081334114\n",
      "Training batch 63 with loss 0.25009\n",
      "------------------------------------------\n",
      "L1 loss: 0.06887271255254745\n",
      "Training batch 64 with loss 0.28813\n",
      "------------------------------------------\n",
      "L1 loss: 0.05912254378199577\n",
      "Training batch 65 with loss 0.21314\n",
      "------------------------------------------\n",
      "L1 loss: 0.06505635380744934\n",
      "Training batch 66 with loss 0.27135\n",
      "------------------------------------------\n",
      "L1 loss: 0.06435753405094147\n",
      "Training batch 67 with loss 0.26203\n",
      "------------------------------------------\n",
      "L1 loss: 0.07273311913013458\n",
      "Training batch 68 with loss 0.30207\n",
      "------------------------------------------\n",
      "L1 loss: 0.05141978710889816\n",
      "Training batch 69 with loss 0.26966\n",
      "------------------------------------------\n",
      "L1 loss: 0.08428321033716202\n",
      "Training batch 70 with loss 0.31129\n",
      "------------------------------------------\n",
      "L1 loss: 0.07243603467941284\n",
      "Training batch 71 with loss 0.31188\n",
      "------------------------------------------\n",
      "L1 loss: 0.08519808202981949\n",
      "Training batch 72 with loss 0.29526\n",
      "------------------------------------------\n",
      "L1 loss: 0.06994379311800003\n",
      "Training batch 73 with loss 0.27571\n",
      "------------------------------------------\n",
      "L1 loss: 0.09687536954879761\n",
      "Training batch 74 with loss 0.30130\n",
      "------------------------------------------\n",
      "L1 loss: 0.07308218628168106\n",
      "Training batch 75 with loss 0.28320\n",
      "------------------------------------------\n",
      "L1 loss: 0.08668326586484909\n",
      "Training batch 76 with loss 0.29571\n",
      "------------------------------------------\n",
      "L1 loss: 0.07674144208431244\n",
      "Training batch 77 with loss 0.26422\n",
      "------------------------------------------\n",
      "L1 loss: 0.07990264147520065\n",
      "Training batch 78 with loss 0.30555\n",
      "------------------------------------------\n",
      "L1 loss: 0.10268042981624603\n",
      "Training batch 79 with loss 0.36729\n",
      "------------------------------------------\n",
      "L1 loss: 0.07692857831716537\n",
      "Training batch 80 with loss 0.28343\n",
      "------------------------------------------\n",
      "L1 loss: 0.08366673439741135\n",
      "Training batch 81 with loss 0.30138\n",
      "------------------------------------------\n",
      "L1 loss: 0.08777035772800446\n",
      "Training batch 82 with loss 0.35677\n",
      "------------------------------------------\n",
      "L1 loss: 0.06020885705947876\n",
      "Training batch 83 with loss 0.42278\n",
      "------------------------------------------\n",
      "L1 loss: 0.0726294070482254\n",
      "Training batch 84 with loss 0.30668\n",
      "------------------------------------------\n",
      "L1 loss: 0.06324806809425354\n",
      "Training batch 85 with loss 0.28286\n",
      "------------------------------------------\n",
      "L1 loss: 0.06437088549137115\n",
      "Training batch 86 with loss 0.24429\n",
      "------------------------------------------\n",
      "L1 loss: 0.1037890836596489\n",
      "Training batch 87 with loss 0.38208\n",
      "------------------------------------------\n",
      "L1 loss: 0.07822443544864655\n",
      "Training batch 88 with loss 0.31630\n",
      "------------------------------------------\n",
      "L1 loss: 0.0704239010810852\n",
      "Training batch 89 with loss 0.27564\n",
      "------------------------------------------\n",
      "L1 loss: 0.0707421749830246\n",
      "Training batch 90 with loss 0.27464\n",
      "------------------------------------------\n",
      "L1 loss: 0.06309325993061066\n",
      "Training batch 91 with loss 0.25314\n",
      "------------------------------------------\n",
      "L1 loss: 0.09616304934024811\n",
      "Training batch 92 with loss 0.32476\n",
      "------------------------------------------\n",
      "L1 loss: 0.07158657163381577\n",
      "Training batch 93 with loss 0.27242\n",
      "------------------------------------------\n",
      "L1 loss: 0.08548443764448166\n",
      "Training batch 94 with loss 0.29552\n",
      "------------------------------------------\n",
      "L1 loss: 0.07275500148534775\n",
      "Training batch 95 with loss 0.26101\n",
      "------------------------------------------\n",
      "L1 loss: 0.0659574642777443\n",
      "Training batch 96 with loss 0.29502\n",
      "------------------------------------------\n",
      "L1 loss: 0.08578921854496002\n",
      "Training batch 97 with loss 0.36503\n",
      "------------------------------------------\n",
      "L1 loss: 0.05655783414840698\n",
      "Training batch 98 with loss 0.23639\n",
      "------------------------------------------\n",
      "L1 loss: 0.06165425479412079\n",
      "Training batch 99 with loss 0.25347\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2959614932537079\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4879\n",
      "------------------------------------------\n",
      "L1 loss: 0.065445177257061\n",
      "Training batch 0 with loss 0.25183\n",
      "------------------------------------------\n",
      "L1 loss: 0.08291067183017731\n",
      "Training batch 1 with loss 0.28261\n",
      "------------------------------------------\n",
      "L1 loss: 0.09483598917722702\n",
      "Training batch 2 with loss 0.33263\n",
      "------------------------------------------\n",
      "L1 loss: 0.071470245718956\n",
      "Training batch 3 with loss 0.25847\n",
      "------------------------------------------\n",
      "L1 loss: 0.08395691961050034\n",
      "Training batch 4 with loss 0.33602\n",
      "------------------------------------------\n",
      "L1 loss: 0.07018823176622391\n",
      "Training batch 5 with loss 0.33444\n",
      "------------------------------------------\n",
      "L1 loss: 0.07894707471132278\n",
      "Training batch 6 with loss 0.26740\n",
      "------------------------------------------\n",
      "L1 loss: 0.08344605565071106\n",
      "Training batch 7 with loss 0.34000\n",
      "------------------------------------------\n",
      "L1 loss: 0.08705686777830124\n",
      "Training batch 8 with loss 0.56191\n",
      "------------------------------------------\n",
      "L1 loss: 0.06389392912387848\n",
      "Training batch 9 with loss 0.23955\n",
      "------------------------------------------\n",
      "L1 loss: 0.08263657242059708\n",
      "Training batch 10 with loss 0.28061\n",
      "------------------------------------------\n",
      "L1 loss: 0.05916285514831543\n",
      "Training batch 11 with loss 0.28903\n",
      "------------------------------------------\n",
      "L1 loss: 0.07409996539354324\n",
      "Training batch 12 with loss 0.29812\n",
      "------------------------------------------\n",
      "L1 loss: 0.07023877650499344\n",
      "Training batch 13 with loss 0.25531\n",
      "------------------------------------------\n",
      "L1 loss: 0.051164329051971436\n",
      "Training batch 14 with loss 0.24257\n",
      "------------------------------------------\n",
      "L1 loss: 0.08340355008840561\n",
      "Training batch 15 with loss 0.26089\n",
      "------------------------------------------\n",
      "L1 loss: 0.06236862763762474\n",
      "Training batch 16 with loss 0.26251\n",
      "------------------------------------------\n",
      "L1 loss: 0.08527711778879166\n",
      "Training batch 17 with loss 0.34998\n",
      "------------------------------------------\n",
      "L1 loss: 0.0813165009021759\n",
      "Training batch 18 with loss 0.31961\n",
      "------------------------------------------\n",
      "L1 loss: 0.06285586208105087\n",
      "Training batch 19 with loss 0.23454\n",
      "------------------------------------------\n",
      "L1 loss: 0.07417193055152893\n",
      "Training batch 20 with loss 0.35951\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07126251608133316\n",
      "Training batch 21 with loss 0.26704\n",
      "------------------------------------------\n",
      "L1 loss: 0.06815744191408157\n",
      "Training batch 22 with loss 0.26715\n",
      "------------------------------------------\n",
      "L1 loss: 0.08635275810956955\n",
      "Training batch 23 with loss 0.38216\n",
      "------------------------------------------\n",
      "L1 loss: 0.06656074523925781\n",
      "Training batch 24 with loss 0.30043\n",
      "------------------------------------------\n",
      "L1 loss: 0.07411926239728928\n",
      "Training batch 25 with loss 0.29441\n",
      "------------------------------------------\n",
      "L1 loss: 0.07569058984518051\n",
      "Training batch 26 with loss 0.28487\n",
      "------------------------------------------\n",
      "L1 loss: 0.05628247931599617\n",
      "Training batch 27 with loss 0.28354\n",
      "------------------------------------------\n",
      "L1 loss: 0.05820488929748535\n",
      "Training batch 28 with loss 0.29210\n",
      "------------------------------------------\n",
      "L1 loss: 0.05774666741490364\n",
      "Training batch 29 with loss 0.25123\n",
      "------------------------------------------\n",
      "L1 loss: 0.06485234200954437\n",
      "Training batch 30 with loss 0.28189\n",
      "------------------------------------------\n",
      "L1 loss: 0.0628088042140007\n",
      "Training batch 31 with loss 0.29569\n",
      "------------------------------------------\n",
      "L1 loss: 0.06941865384578705\n",
      "Training batch 32 with loss 0.27115\n",
      "------------------------------------------\n",
      "L1 loss: 0.06452790647745132\n",
      "Training batch 33 with loss 0.25841\n",
      "------------------------------------------\n",
      "L1 loss: 0.06674734503030777\n",
      "Training batch 34 with loss 0.41279\n",
      "------------------------------------------\n",
      "L1 loss: 0.09610127657651901\n",
      "Training batch 35 with loss 0.32969\n",
      "------------------------------------------\n",
      "L1 loss: 0.08223139494657516\n",
      "Training batch 36 with loss 0.29287\n",
      "------------------------------------------\n",
      "L1 loss: 0.06668825447559357\n",
      "Training batch 37 with loss 0.25803\n",
      "------------------------------------------\n",
      "L1 loss: 0.06375645101070404\n",
      "Training batch 38 with loss 0.23006\n",
      "------------------------------------------\n",
      "L1 loss: 0.06736860424280167\n",
      "Training batch 39 with loss 0.31653\n",
      "------------------------------------------\n",
      "L1 loss: 0.0692266970872879\n",
      "Training batch 40 with loss 0.25959\n",
      "------------------------------------------\n",
      "L1 loss: 0.05940781906247139\n",
      "Training batch 41 with loss 0.28410\n",
      "------------------------------------------\n",
      "L1 loss: 0.0759064182639122\n",
      "Training batch 42 with loss 0.28202\n",
      "------------------------------------------\n",
      "L1 loss: 0.06622149050235748\n",
      "Training batch 43 with loss 0.28144\n",
      "------------------------------------------\n",
      "L1 loss: 0.059173621237277985\n",
      "Training batch 44 with loss 0.28816\n",
      "------------------------------------------\n",
      "L1 loss: 0.09249049425125122\n",
      "Training batch 45 with loss 0.34989\n",
      "------------------------------------------\n",
      "L1 loss: 0.07493191212415695\n",
      "Training batch 46 with loss 0.26960\n",
      "------------------------------------------\n",
      "L1 loss: 0.09546612203121185\n",
      "Training batch 47 with loss 0.48106\n",
      "------------------------------------------\n",
      "L1 loss: 0.04724453017115593\n",
      "Training batch 48 with loss 0.23760\n",
      "------------------------------------------\n",
      "L1 loss: 0.07719244807958603\n",
      "Training batch 49 with loss 0.35060\n",
      "------------------------------------------\n",
      "L1 loss: 0.0715714767575264\n",
      "Training batch 50 with loss 0.29281\n",
      "------------------------------------------\n",
      "L1 loss: 0.08157182484865189\n",
      "Training batch 51 with loss 0.33987\n",
      "------------------------------------------\n",
      "L1 loss: 0.08880793303251266\n",
      "Training batch 52 with loss 0.28980\n",
      "------------------------------------------\n",
      "L1 loss: 0.0783831924200058\n",
      "Training batch 53 with loss 0.28019\n",
      "------------------------------------------\n",
      "L1 loss: 0.0734909325838089\n",
      "Training batch 54 with loss 0.26523\n",
      "------------------------------------------\n",
      "L1 loss: 0.0615970641374588\n",
      "Training batch 55 with loss 0.30562\n",
      "------------------------------------------\n",
      "L1 loss: 0.05260127782821655\n",
      "Training batch 56 with loss 0.24609\n",
      "------------------------------------------\n",
      "L1 loss: 0.06724148243665695\n",
      "Training batch 57 with loss 0.28352\n",
      "------------------------------------------\n",
      "L1 loss: 0.06446345150470734\n",
      "Training batch 58 with loss 0.27247\n",
      "------------------------------------------\n",
      "L1 loss: 0.08248981833457947\n",
      "Training batch 59 with loss 0.64954\n",
      "------------------------------------------\n",
      "L1 loss: 0.06351875513792038\n",
      "Training batch 60 with loss 0.27187\n",
      "------------------------------------------\n",
      "L1 loss: 0.09465409815311432\n",
      "Training batch 61 with loss 0.35696\n",
      "------------------------------------------\n",
      "L1 loss: 0.05652878060936928\n",
      "Training batch 62 with loss 0.26397\n",
      "------------------------------------------\n",
      "L1 loss: 0.06057070568203926\n",
      "Training batch 63 with loss 0.25113\n",
      "------------------------------------------\n",
      "L1 loss: 0.06784756481647491\n",
      "Training batch 64 with loss 0.29840\n",
      "------------------------------------------\n",
      "L1 loss: 0.06154799833893776\n",
      "Training batch 65 with loss 0.24129\n",
      "------------------------------------------\n",
      "L1 loss: 0.0633314922451973\n",
      "Training batch 66 with loss 0.26434\n",
      "------------------------------------------\n",
      "L1 loss: 0.06678333878517151\n",
      "Training batch 67 with loss 0.26291\n",
      "------------------------------------------\n",
      "L1 loss: 0.06581244617700577\n",
      "Training batch 68 with loss 0.38352\n",
      "------------------------------------------\n",
      "L1 loss: 0.05285445973277092\n",
      "Training batch 69 with loss 0.27282\n",
      "------------------------------------------\n",
      "L1 loss: 0.07635927200317383\n",
      "Training batch 70 with loss 0.28685\n",
      "------------------------------------------\n",
      "L1 loss: 0.07267840206623077\n",
      "Training batch 71 with loss 0.28709\n",
      "------------------------------------------\n",
      "L1 loss: 0.08413482457399368\n",
      "Training batch 72 with loss 0.29370\n",
      "------------------------------------------\n",
      "L1 loss: 0.07185228168964386\n",
      "Training batch 73 with loss 0.28802\n",
      "------------------------------------------\n",
      "L1 loss: 0.09732653200626373\n",
      "Training batch 74 with loss 0.30409\n",
      "------------------------------------------\n",
      "L1 loss: 0.07469772547483444\n",
      "Training batch 75 with loss 0.28256\n",
      "------------------------------------------\n",
      "L1 loss: 0.08570494502782822\n",
      "Training batch 76 with loss 0.31354\n",
      "------------------------------------------\n",
      "L1 loss: 0.08098998665809631\n",
      "Training batch 77 with loss 0.26901\n",
      "------------------------------------------\n",
      "L1 loss: 0.08069419860839844\n",
      "Training batch 78 with loss 0.29737\n",
      "------------------------------------------\n",
      "L1 loss: 0.1060948371887207\n",
      "Training batch 79 with loss 0.35856\n",
      "------------------------------------------\n",
      "L1 loss: 0.07722487300634384\n",
      "Training batch 80 with loss 0.27975\n",
      "------------------------------------------\n",
      "L1 loss: 0.06957964599132538\n",
      "Training batch 81 with loss 0.41401\n",
      "------------------------------------------\n",
      "L1 loss: 0.08743463456630707\n",
      "Training batch 82 with loss 0.35443\n",
      "------------------------------------------\n",
      "L1 loss: 0.07747507095336914\n",
      "Training batch 83 with loss 0.27563\n",
      "------------------------------------------\n",
      "L1 loss: 0.06644286215305328\n",
      "Training batch 84 with loss 0.29288\n",
      "------------------------------------------\n",
      "L1 loss: 0.06186777353286743\n",
      "Training batch 85 with loss 0.26669\n",
      "------------------------------------------\n",
      "L1 loss: 0.0815703421831131\n",
      "Training batch 86 with loss 0.28330\n",
      "------------------------------------------\n",
      "L1 loss: 0.10220225900411606\n",
      "Training batch 87 with loss 0.39308\n",
      "------------------------------------------\n",
      "L1 loss: 0.075595423579216\n",
      "Training batch 88 with loss 0.32172\n",
      "------------------------------------------\n",
      "L1 loss: 0.06928513199090958\n",
      "Training batch 89 with loss 0.25834\n",
      "------------------------------------------\n",
      "L1 loss: 0.07467169314622879\n",
      "Training batch 90 with loss 0.28340\n",
      "------------------------------------------\n",
      "L1 loss: 0.05119697377085686\n",
      "Training batch 91 with loss 0.20983\n",
      "------------------------------------------\n",
      "L1 loss: 0.0940309464931488\n",
      "Training batch 92 with loss 0.30512\n",
      "------------------------------------------\n",
      "L1 loss: 0.07236132025718689\n",
      "Training batch 93 with loss 0.27758\n",
      "------------------------------------------\n",
      "L1 loss: 0.08602188527584076\n",
      "Training batch 94 with loss 0.29499\n",
      "------------------------------------------\n",
      "L1 loss: 0.07344687730073929\n",
      "Training batch 95 with loss 0.25101\n",
      "------------------------------------------\n",
      "L1 loss: 0.06350840628147125\n",
      "Training batch 96 with loss 0.32561\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08673084527254105\n",
      "Training batch 97 with loss 0.36132\n",
      "------------------------------------------\n",
      "L1 loss: 0.05506863445043564\n",
      "Training batch 98 with loss 0.23696\n",
      "------------------------------------------\n",
      "L1 loss: 0.06552066653966904\n",
      "Training batch 99 with loss 0.24964\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.30020091027021406\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4880\n",
      "------------------------------------------\n",
      "L1 loss: 0.060425352305173874\n",
      "Training batch 0 with loss 0.24384\n",
      "------------------------------------------\n",
      "L1 loss: 0.08586636185646057\n",
      "Training batch 1 with loss 0.29847\n",
      "------------------------------------------\n",
      "L1 loss: 0.08116298913955688\n",
      "Training batch 2 with loss 0.31709\n",
      "------------------------------------------\n",
      "L1 loss: 0.07765759527683258\n",
      "Training batch 3 with loss 0.26639\n",
      "------------------------------------------\n",
      "L1 loss: 0.0824277326464653\n",
      "Training batch 4 with loss 0.33122\n",
      "------------------------------------------\n",
      "L1 loss: 0.06673748046159744\n",
      "Training batch 5 with loss 0.31158\n",
      "------------------------------------------\n",
      "L1 loss: 0.081192247569561\n",
      "Training batch 6 with loss 0.26404\n",
      "------------------------------------------\n",
      "L1 loss: 0.08319763094186783\n",
      "Training batch 7 with loss 0.33918\n",
      "------------------------------------------\n",
      "L1 loss: 0.07795683294534683\n",
      "Training batch 8 with loss 0.31825\n",
      "------------------------------------------\n",
      "L1 loss: 0.061009347438812256\n",
      "Training batch 9 with loss 0.25024\n",
      "------------------------------------------\n",
      "L1 loss: 0.08277811110019684\n",
      "Training batch 10 with loss 0.30384\n",
      "------------------------------------------\n",
      "L1 loss: 0.06257610768079758\n",
      "Training batch 11 with loss 0.28983\n",
      "------------------------------------------\n",
      "L1 loss: 0.07638688385486603\n",
      "Training batch 12 with loss 0.31182\n",
      "------------------------------------------\n",
      "L1 loss: 0.06394887715578079\n",
      "Training batch 13 with loss 0.23397\n",
      "------------------------------------------\n",
      "L1 loss: 0.05315621569752693\n",
      "Training batch 14 with loss 0.24530\n",
      "------------------------------------------\n",
      "L1 loss: 0.08140774071216583\n",
      "Training batch 15 with loss 0.27997\n",
      "------------------------------------------\n",
      "L1 loss: 0.06106484308838844\n",
      "Training batch 16 with loss 0.25994\n",
      "------------------------------------------\n",
      "L1 loss: 0.08661089837551117\n",
      "Training batch 17 with loss 0.34209\n",
      "------------------------------------------\n",
      "L1 loss: 0.08015826344490051\n",
      "Training batch 18 with loss 0.30247\n",
      "------------------------------------------\n",
      "L1 loss: 0.05888672173023224\n",
      "Training batch 19 with loss 0.22767\n",
      "------------------------------------------\n",
      "L1 loss: 0.07289646565914154\n",
      "Training batch 20 with loss 0.31813\n",
      "------------------------------------------\n",
      "L1 loss: 0.0697687640786171\n",
      "Training batch 21 with loss 0.26977\n",
      "------------------------------------------\n",
      "L1 loss: 0.06843101233243942\n",
      "Training batch 22 with loss 0.29261\n",
      "------------------------------------------\n",
      "L1 loss: 0.0908232256770134\n",
      "Training batch 23 with loss 0.37774\n",
      "------------------------------------------\n",
      "L1 loss: 0.06850762665271759\n",
      "Training batch 24 with loss 0.30197\n",
      "------------------------------------------\n",
      "L1 loss: 0.07328281551599503\n",
      "Training batch 25 with loss 0.29373\n",
      "------------------------------------------\n",
      "L1 loss: 0.0807289257645607\n",
      "Training batch 26 with loss 0.28318\n",
      "------------------------------------------\n",
      "L1 loss: 0.056563299149274826\n",
      "Training batch 27 with loss 0.29240\n",
      "------------------------------------------\n",
      "L1 loss: 0.046926066279411316\n",
      "Training batch 28 with loss 0.39357\n",
      "------------------------------------------\n",
      "L1 loss: 0.05689835548400879\n",
      "Training batch 29 with loss 0.23777\n",
      "------------------------------------------\n",
      "L1 loss: 0.06283167749643326\n",
      "Training batch 30 with loss 0.27418\n",
      "------------------------------------------\n",
      "L1 loss: 0.05165278539061546\n",
      "Training batch 31 with loss 0.46022\n",
      "------------------------------------------\n",
      "L1 loss: 0.0695275366306305\n",
      "Training batch 32 with loss 0.28453\n",
      "------------------------------------------\n",
      "L1 loss: 0.06513562798500061\n",
      "Training batch 33 with loss 0.26441\n",
      "------------------------------------------\n",
      "L1 loss: 0.07465139031410217\n",
      "Training batch 34 with loss 0.31492\n",
      "------------------------------------------\n",
      "L1 loss: 0.0938805341720581\n",
      "Training batch 35 with loss 0.33508\n",
      "------------------------------------------\n",
      "L1 loss: 0.08647928386926651\n",
      "Training batch 36 with loss 0.28775\n",
      "------------------------------------------\n",
      "L1 loss: 0.07155857980251312\n",
      "Training batch 37 with loss 0.26989\n",
      "------------------------------------------\n",
      "L1 loss: 0.06063050031661987\n",
      "Training batch 38 with loss 0.21569\n",
      "------------------------------------------\n",
      "L1 loss: 0.06907611340284348\n",
      "Training batch 39 with loss 0.30072\n",
      "------------------------------------------\n",
      "L1 loss: 0.07314242422580719\n",
      "Training batch 40 with loss 0.26087\n",
      "------------------------------------------\n",
      "L1 loss: 0.061729662120342255\n",
      "Training batch 41 with loss 0.30660\n",
      "------------------------------------------\n",
      "L1 loss: 0.07829541712999344\n",
      "Training batch 42 with loss 0.27445\n",
      "------------------------------------------\n",
      "L1 loss: 0.07134383916854858\n",
      "Training batch 43 with loss 0.53041\n",
      "------------------------------------------\n",
      "L1 loss: 0.05456557497382164\n",
      "Training batch 44 with loss 0.26018\n",
      "------------------------------------------\n",
      "L1 loss: 0.09237167984247208\n",
      "Training batch 45 with loss 0.35961\n",
      "------------------------------------------\n",
      "L1 loss: 0.0743117704987526\n",
      "Training batch 46 with loss 0.28005\n",
      "------------------------------------------\n",
      "L1 loss: 0.11202269792556763\n",
      "Training batch 47 with loss 0.37617\n",
      "------------------------------------------\n",
      "L1 loss: 0.0445711687207222\n",
      "Training batch 48 with loss 0.25809\n",
      "------------------------------------------\n",
      "L1 loss: 0.08179602771997452\n",
      "Training batch 49 with loss 0.35113\n",
      "------------------------------------------\n",
      "L1 loss: 0.07325207442045212\n",
      "Training batch 50 with loss 0.29960\n",
      "------------------------------------------\n",
      "L1 loss: 0.08407958596944809\n",
      "Training batch 51 with loss 0.36118\n",
      "------------------------------------------\n",
      "L1 loss: 0.08849988132715225\n",
      "Training batch 52 with loss 0.29682\n",
      "------------------------------------------\n",
      "L1 loss: 0.07882801443338394\n",
      "Training batch 53 with loss 0.28071\n",
      "------------------------------------------\n",
      "L1 loss: 0.07825841009616852\n",
      "Training batch 54 with loss 0.28491\n",
      "------------------------------------------\n",
      "L1 loss: 0.060157835483551025\n",
      "Training batch 55 with loss 0.31376\n",
      "------------------------------------------\n",
      "L1 loss: 0.0576481893658638\n",
      "Training batch 56 with loss 0.25452\n",
      "------------------------------------------\n",
      "L1 loss: 0.06662381440401077\n",
      "Training batch 57 with loss 0.26900\n",
      "------------------------------------------\n",
      "L1 loss: 0.06602098792791367\n",
      "Training batch 58 with loss 0.24800\n",
      "------------------------------------------\n",
      "L1 loss: 0.08318046480417252\n",
      "Training batch 59 with loss 0.28326\n",
      "------------------------------------------\n",
      "L1 loss: 0.06475916504859924\n",
      "Training batch 60 with loss 0.27289\n",
      "------------------------------------------\n",
      "L1 loss: 0.08902312070131302\n",
      "Training batch 61 with loss 0.33246\n",
      "------------------------------------------\n",
      "L1 loss: 0.05561838299036026\n",
      "Training batch 62 with loss 0.25445\n",
      "------------------------------------------\n",
      "L1 loss: 0.0632416382431984\n",
      "Training batch 63 with loss 0.26868\n",
      "------------------------------------------\n",
      "L1 loss: 0.0692586898803711\n",
      "Training batch 64 with loss 0.27205\n",
      "------------------------------------------\n",
      "L1 loss: 0.05859530717134476\n",
      "Training batch 65 with loss 0.22612\n",
      "------------------------------------------\n",
      "L1 loss: 0.06557975709438324\n",
      "Training batch 66 with loss 0.25686\n",
      "------------------------------------------\n",
      "L1 loss: 0.06559153646230698\n",
      "Training batch 67 with loss 0.25322\n",
      "------------------------------------------\n",
      "L1 loss: 0.07639121264219284\n",
      "Training batch 68 with loss 0.31816\n",
      "------------------------------------------\n",
      "L1 loss: 0.05017782002687454\n",
      "Training batch 69 with loss 0.25603\n",
      "------------------------------------------\n",
      "L1 loss: 0.07808319479227066\n",
      "Training batch 70 with loss 0.29695\n",
      "------------------------------------------\n",
      "L1 loss: 0.07200945168733597\n",
      "Training batch 71 with loss 0.28901\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08354460448026657\n",
      "Training batch 72 with loss 0.30063\n",
      "------------------------------------------\n",
      "L1 loss: 0.07550045102834702\n",
      "Training batch 73 with loss 0.30264\n",
      "------------------------------------------\n",
      "L1 loss: 0.09821686148643494\n",
      "Training batch 74 with loss 0.31103\n",
      "------------------------------------------\n",
      "L1 loss: 0.07527989149093628\n",
      "Training batch 75 with loss 0.27583\n",
      "------------------------------------------\n",
      "L1 loss: 0.08626355230808258\n",
      "Training batch 76 with loss 0.29352\n",
      "------------------------------------------\n",
      "L1 loss: 0.0786813274025917\n",
      "Training batch 77 with loss 0.26897\n",
      "------------------------------------------\n",
      "L1 loss: 0.07691480219364166\n",
      "Training batch 78 with loss 0.31547\n",
      "------------------------------------------\n",
      "L1 loss: 0.10601864755153656\n",
      "Training batch 79 with loss 0.37211\n",
      "------------------------------------------\n",
      "L1 loss: 0.07623165845870972\n",
      "Training batch 80 with loss 0.27299\n",
      "------------------------------------------\n",
      "L1 loss: 0.07214362919330597\n",
      "Training batch 81 with loss 0.26362\n",
      "------------------------------------------\n",
      "L1 loss: 0.08194342255592346\n",
      "Training batch 82 with loss 0.38423\n",
      "------------------------------------------\n",
      "L1 loss: 0.07605128735303879\n",
      "Training batch 83 with loss 0.28791\n",
      "------------------------------------------\n",
      "L1 loss: 0.07153890281915665\n",
      "Training batch 84 with loss 0.30417\n",
      "------------------------------------------\n",
      "L1 loss: 0.06442797929048538\n",
      "Training batch 85 with loss 0.27622\n",
      "------------------------------------------\n",
      "L1 loss: 0.08059792220592499\n",
      "Training batch 86 with loss 0.28086\n",
      "------------------------------------------\n",
      "L1 loss: 0.10098475217819214\n",
      "Training batch 87 with loss 0.39927\n",
      "------------------------------------------\n",
      "L1 loss: 0.07885248959064484\n",
      "Training batch 88 with loss 0.31818\n",
      "------------------------------------------\n",
      "L1 loss: 0.07035950571298599\n",
      "Training batch 89 with loss 0.25045\n",
      "------------------------------------------\n",
      "L1 loss: 0.07261954993009567\n",
      "Training batch 90 with loss 0.27660\n",
      "------------------------------------------\n",
      "L1 loss: 0.06237495690584183\n",
      "Training batch 91 with loss 0.26909\n",
      "------------------------------------------\n",
      "L1 loss: 0.0899893119931221\n",
      "Training batch 92 with loss 0.29385\n",
      "------------------------------------------\n",
      "L1 loss: 0.06343650072813034\n",
      "Training batch 93 with loss 0.24996\n",
      "------------------------------------------\n",
      "L1 loss: 0.08300872892141342\n",
      "Training batch 94 with loss 0.29971\n",
      "------------------------------------------\n",
      "L1 loss: 0.06955370306968689\n",
      "Training batch 95 with loss 0.25063\n",
      "------------------------------------------\n",
      "L1 loss: 0.06668742001056671\n",
      "Training batch 96 with loss 0.29217\n",
      "------------------------------------------\n",
      "L1 loss: 0.08275726437568665\n",
      "Training batch 97 with loss 0.34371\n",
      "------------------------------------------\n",
      "L1 loss: 0.056999798864126205\n",
      "Training batch 98 with loss 0.26458\n",
      "------------------------------------------\n",
      "L1 loss: 0.0653097927570343\n",
      "Training batch 99 with loss 0.27727\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2954730720818043\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4881\n",
      "------------------------------------------\n",
      "L1 loss: 0.05581815540790558\n",
      "Training batch 0 with loss 0.22194\n",
      "------------------------------------------\n",
      "L1 loss: 0.07815100252628326\n",
      "Training batch 1 with loss 0.40727\n",
      "------------------------------------------\n",
      "L1 loss: 0.08958883583545685\n",
      "Training batch 2 with loss 0.32753\n",
      "------------------------------------------\n",
      "L1 loss: 0.06522355228662491\n",
      "Training batch 3 with loss 0.25345\n",
      "------------------------------------------\n",
      "L1 loss: 0.08267253637313843\n",
      "Training batch 4 with loss 0.32647\n",
      "------------------------------------------\n",
      "L1 loss: 0.06506004929542542\n",
      "Training batch 5 with loss 0.30982\n",
      "------------------------------------------\n",
      "L1 loss: 0.07148831337690353\n",
      "Training batch 6 with loss 0.41562\n",
      "------------------------------------------\n",
      "L1 loss: 0.08302172273397446\n",
      "Training batch 7 with loss 0.32707\n",
      "------------------------------------------\n",
      "L1 loss: 0.08226513117551804\n",
      "Training batch 8 with loss 0.31920\n",
      "------------------------------------------\n",
      "L1 loss: 0.06326874345541\n",
      "Training batch 9 with loss 0.25948\n",
      "------------------------------------------\n",
      "L1 loss: 0.08250478655099869\n",
      "Training batch 10 with loss 0.30545\n",
      "------------------------------------------\n",
      "L1 loss: 0.06340938061475754\n",
      "Training batch 11 with loss 0.30462\n",
      "------------------------------------------\n",
      "L1 loss: 0.07523094117641449\n",
      "Training batch 12 with loss 0.31411\n",
      "------------------------------------------\n",
      "L1 loss: 0.06632554531097412\n",
      "Training batch 13 with loss 0.26086\n",
      "------------------------------------------\n",
      "L1 loss: 0.0519404374063015\n",
      "Training batch 14 with loss 0.24111\n",
      "------------------------------------------\n",
      "L1 loss: 0.0839787945151329\n",
      "Training batch 15 with loss 0.26263\n",
      "------------------------------------------\n",
      "L1 loss: 0.06144190579652786\n",
      "Training batch 16 with loss 0.26768\n",
      "------------------------------------------\n",
      "L1 loss: 0.08867353200912476\n",
      "Training batch 17 with loss 0.36639\n",
      "------------------------------------------\n",
      "L1 loss: 0.08124551922082901\n",
      "Training batch 18 with loss 0.29244\n",
      "------------------------------------------\n",
      "L1 loss: 0.05250273272395134\n",
      "Training batch 19 with loss 0.25086\n",
      "------------------------------------------\n",
      "L1 loss: 0.07219823449850082\n",
      "Training batch 20 with loss 0.31404\n",
      "------------------------------------------\n",
      "L1 loss: 0.07078424096107483\n",
      "Training batch 21 with loss 0.29730\n",
      "------------------------------------------\n",
      "L1 loss: 0.06658248603343964\n",
      "Training batch 22 with loss 0.29021\n",
      "------------------------------------------\n",
      "L1 loss: 0.08899844437837601\n",
      "Training batch 23 with loss 0.35534\n",
      "------------------------------------------\n",
      "L1 loss: 0.06781034916639328\n",
      "Training batch 24 with loss 0.29000\n",
      "------------------------------------------\n",
      "L1 loss: 0.0807914212346077\n",
      "Training batch 25 with loss 0.48839\n",
      "------------------------------------------\n",
      "L1 loss: 0.07599242776632309\n",
      "Training batch 26 with loss 0.28120\n",
      "------------------------------------------\n",
      "L1 loss: 0.05763300880789757\n",
      "Training batch 27 with loss 0.29920\n",
      "------------------------------------------\n",
      "L1 loss: 0.05781020224094391\n",
      "Training batch 28 with loss 0.29124\n",
      "------------------------------------------\n",
      "L1 loss: 0.059078119695186615\n",
      "Training batch 29 with loss 0.25003\n",
      "------------------------------------------\n",
      "L1 loss: 0.05997918173670769\n",
      "Training batch 30 with loss 0.27251\n",
      "------------------------------------------\n",
      "L1 loss: 0.06487973034381866\n",
      "Training batch 31 with loss 0.30461\n",
      "------------------------------------------\n",
      "L1 loss: 0.06850053369998932\n",
      "Training batch 32 with loss 0.26952\n",
      "------------------------------------------\n",
      "L1 loss: 0.061725545674562454\n",
      "Training batch 33 with loss 0.24576\n",
      "------------------------------------------\n",
      "L1 loss: 0.07518857717514038\n",
      "Training batch 34 with loss 0.31821\n",
      "------------------------------------------\n",
      "L1 loss: 0.09335779398679733\n",
      "Training batch 35 with loss 0.33140\n",
      "------------------------------------------\n",
      "L1 loss: 0.08431462198495865\n",
      "Training batch 36 with loss 0.28022\n",
      "------------------------------------------\n",
      "L1 loss: 0.06761065125465393\n",
      "Training batch 37 with loss 0.25688\n",
      "------------------------------------------\n",
      "L1 loss: 0.06211800128221512\n",
      "Training batch 38 with loss 0.22355\n",
      "------------------------------------------\n",
      "L1 loss: 0.06886306405067444\n",
      "Training batch 39 with loss 0.30018\n",
      "------------------------------------------\n",
      "L1 loss: 0.07063484191894531\n",
      "Training batch 40 with loss 0.24980\n",
      "------------------------------------------\n",
      "L1 loss: 0.055502645671367645\n",
      "Training batch 41 with loss 0.27932\n",
      "------------------------------------------\n",
      "L1 loss: 0.07841459661722183\n",
      "Training batch 42 with loss 0.26764\n",
      "------------------------------------------\n",
      "L1 loss: 0.07181838899850845\n",
      "Training batch 43 with loss 0.29396\n",
      "------------------------------------------\n",
      "L1 loss: 0.05549135059118271\n",
      "Training batch 44 with loss 0.26864\n",
      "------------------------------------------\n",
      "L1 loss: 0.09199189394712448\n",
      "Training batch 45 with loss 0.36989\n",
      "------------------------------------------\n",
      "L1 loss: 0.07330203056335449\n",
      "Training batch 46 with loss 0.26231\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.11081695556640625\n",
      "Training batch 47 with loss 0.39697\n",
      "------------------------------------------\n",
      "L1 loss: 0.04722124710679054\n",
      "Training batch 48 with loss 0.23520\n",
      "------------------------------------------\n",
      "L1 loss: 0.08356037735939026\n",
      "Training batch 49 with loss 0.35393\n",
      "------------------------------------------\n",
      "L1 loss: 0.07093971967697144\n",
      "Training batch 50 with loss 0.29051\n",
      "------------------------------------------\n",
      "L1 loss: 0.07894843071699142\n",
      "Training batch 51 with loss 0.33051\n",
      "------------------------------------------\n",
      "L1 loss: 0.09458938986063004\n",
      "Training batch 52 with loss 0.44901\n",
      "------------------------------------------\n",
      "L1 loss: 0.07882864773273468\n",
      "Training batch 53 with loss 0.28187\n",
      "------------------------------------------\n",
      "L1 loss: 0.07536695897579193\n",
      "Training batch 54 with loss 0.26675\n",
      "------------------------------------------\n",
      "L1 loss: 0.05917123705148697\n",
      "Training batch 55 with loss 0.31542\n",
      "------------------------------------------\n",
      "L1 loss: 0.053007982671260834\n",
      "Training batch 56 with loss 0.25204\n",
      "------------------------------------------\n",
      "L1 loss: 0.07017060369253159\n",
      "Training batch 57 with loss 0.28098\n",
      "------------------------------------------\n",
      "L1 loss: 0.05986764281988144\n",
      "Training batch 58 with loss 0.58649\n",
      "------------------------------------------\n",
      "L1 loss: 0.07899712771177292\n",
      "Training batch 59 with loss 0.29783\n",
      "------------------------------------------\n",
      "L1 loss: 0.06549836695194244\n",
      "Training batch 60 with loss 0.27211\n",
      "------------------------------------------\n",
      "L1 loss: 0.09594936668872833\n",
      "Training batch 61 with loss 0.33867\n",
      "------------------------------------------\n",
      "L1 loss: 0.05690188705921173\n",
      "Training batch 62 with loss 0.26152\n",
      "------------------------------------------\n",
      "L1 loss: 0.0611206516623497\n",
      "Training batch 63 with loss 0.25746\n",
      "------------------------------------------\n",
      "L1 loss: 0.07123716920614243\n",
      "Training batch 64 with loss 0.27856\n",
      "------------------------------------------\n",
      "L1 loss: 0.06156810000538826\n",
      "Training batch 65 with loss 0.22893\n",
      "------------------------------------------\n",
      "L1 loss: 0.0667230486869812\n",
      "Training batch 66 with loss 0.26971\n",
      "------------------------------------------\n",
      "L1 loss: 0.06391320377588272\n",
      "Training batch 67 with loss 0.26515\n",
      "------------------------------------------\n",
      "L1 loss: 0.07646266371011734\n",
      "Training batch 68 with loss 0.30882\n",
      "------------------------------------------\n",
      "L1 loss: 0.05137552693486214\n",
      "Training batch 69 with loss 0.26183\n",
      "------------------------------------------\n",
      "L1 loss: 0.08066575974225998\n",
      "Training batch 70 with loss 0.30226\n",
      "------------------------------------------\n",
      "L1 loss: 0.07236019521951675\n",
      "Training batch 71 with loss 0.29553\n",
      "------------------------------------------\n",
      "L1 loss: 0.08146077394485474\n",
      "Training batch 72 with loss 0.29122\n",
      "------------------------------------------\n",
      "L1 loss: 0.07543843239545822\n",
      "Training batch 73 with loss 0.31207\n",
      "------------------------------------------\n",
      "L1 loss: 0.09918290376663208\n",
      "Training batch 74 with loss 0.31727\n",
      "------------------------------------------\n",
      "L1 loss: 0.07464709877967834\n",
      "Training batch 75 with loss 0.28895\n",
      "------------------------------------------\n",
      "L1 loss: 0.08651331812143326\n",
      "Training batch 76 with loss 0.28563\n",
      "------------------------------------------\n",
      "L1 loss: 0.07918663322925568\n",
      "Training batch 77 with loss 0.25525\n",
      "------------------------------------------\n",
      "L1 loss: 0.0776882916688919\n",
      "Training batch 78 with loss 0.29757\n",
      "------------------------------------------\n",
      "L1 loss: 0.1055213063955307\n",
      "Training batch 79 with loss 0.38050\n",
      "------------------------------------------\n",
      "L1 loss: 0.0770404264330864\n",
      "Training batch 80 with loss 0.27832\n",
      "------------------------------------------\n",
      "L1 loss: 0.07447366416454315\n",
      "Training batch 81 with loss 0.28371\n",
      "------------------------------------------\n",
      "L1 loss: 0.08177720010280609\n",
      "Training batch 82 with loss 0.34725\n",
      "------------------------------------------\n",
      "L1 loss: 0.07366468012332916\n",
      "Training batch 83 with loss 0.25987\n",
      "------------------------------------------\n",
      "L1 loss: 0.06884387135505676\n",
      "Training batch 84 with loss 0.28145\n",
      "------------------------------------------\n",
      "L1 loss: 0.061489373445510864\n",
      "Training batch 85 with loss 0.27215\n",
      "------------------------------------------\n",
      "L1 loss: 0.08117786049842834\n",
      "Training batch 86 with loss 0.28367\n",
      "------------------------------------------\n",
      "L1 loss: 0.10117176175117493\n",
      "Training batch 87 with loss 0.39608\n",
      "------------------------------------------\n",
      "L1 loss: 0.07782743871212006\n",
      "Training batch 88 with loss 0.31443\n",
      "------------------------------------------\n",
      "L1 loss: 0.06857941299676895\n",
      "Training batch 89 with loss 0.25876\n",
      "------------------------------------------\n",
      "L1 loss: 0.06240231916308403\n",
      "Training batch 90 with loss 0.47202\n",
      "------------------------------------------\n",
      "L1 loss: 0.06455327570438385\n",
      "Training batch 91 with loss 0.25555\n",
      "------------------------------------------\n",
      "L1 loss: 0.09681662917137146\n",
      "Training batch 92 with loss 0.31117\n",
      "------------------------------------------\n",
      "L1 loss: 0.06159801036119461\n",
      "Training batch 93 with loss 0.24514\n",
      "------------------------------------------\n",
      "L1 loss: 0.08654431253671646\n",
      "Training batch 94 with loss 0.30977\n",
      "------------------------------------------\n",
      "L1 loss: 0.07260564714670181\n",
      "Training batch 95 with loss 0.25495\n",
      "------------------------------------------\n",
      "L1 loss: 0.06629456579685211\n",
      "Training batch 96 with loss 0.27251\n",
      "------------------------------------------\n",
      "L1 loss: 0.0885666012763977\n",
      "Training batch 97 with loss 0.38267\n",
      "------------------------------------------\n",
      "L1 loss: 0.049646202474832535\n",
      "Training batch 98 with loss 0.38993\n",
      "------------------------------------------\n",
      "L1 loss: 0.05955817922949791\n",
      "Training batch 99 with loss 0.38942\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.30350629553198816\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4882\n",
      "------------------------------------------\n",
      "L1 loss: 0.0625382736325264\n",
      "Training batch 0 with loss 0.25566\n",
      "------------------------------------------\n",
      "L1 loss: 0.08365467190742493\n",
      "Training batch 1 with loss 0.28335\n",
      "------------------------------------------\n",
      "L1 loss: 0.09137030690908432\n",
      "Training batch 2 with loss 0.31973\n",
      "------------------------------------------\n",
      "L1 loss: 0.074449822306633\n",
      "Training batch 3 with loss 0.26542\n",
      "------------------------------------------\n",
      "L1 loss: 0.08347172290086746\n",
      "Training batch 4 with loss 0.32189\n",
      "------------------------------------------\n",
      "L1 loss: 0.06651102006435394\n",
      "Training batch 5 with loss 0.31961\n",
      "------------------------------------------\n",
      "L1 loss: 0.0797106996178627\n",
      "Training batch 6 with loss 0.26779\n",
      "------------------------------------------\n",
      "L1 loss: 0.08408264070749283\n",
      "Training batch 7 with loss 0.32073\n",
      "------------------------------------------\n",
      "L1 loss: 0.0827299952507019\n",
      "Training batch 8 with loss 0.33000\n",
      "------------------------------------------\n",
      "L1 loss: 0.06295198202133179\n",
      "Training batch 9 with loss 0.26109\n",
      "------------------------------------------\n",
      "L1 loss: 0.08325608819723129\n",
      "Training batch 10 with loss 0.29250\n",
      "------------------------------------------\n",
      "L1 loss: 0.05918838828802109\n",
      "Training batch 11 with loss 0.28593\n",
      "------------------------------------------\n",
      "L1 loss: 0.0720793679356575\n",
      "Training batch 12 with loss 0.29669\n",
      "------------------------------------------\n",
      "L1 loss: 0.06990838795900345\n",
      "Training batch 13 with loss 0.23676\n",
      "------------------------------------------\n",
      "L1 loss: 0.05180530995130539\n",
      "Training batch 14 with loss 0.23122\n",
      "------------------------------------------\n",
      "L1 loss: 0.08345915377140045\n",
      "Training batch 15 with loss 0.27221\n",
      "------------------------------------------\n",
      "L1 loss: 0.06279855966567993\n",
      "Training batch 16 with loss 0.26962\n",
      "------------------------------------------\n",
      "L1 loss: 0.08879807591438293\n",
      "Training batch 17 with loss 0.34517\n",
      "------------------------------------------\n",
      "L1 loss: 0.07970688492059708\n",
      "Training batch 18 with loss 0.30117\n",
      "------------------------------------------\n",
      "L1 loss: 0.05913432314991951\n",
      "Training batch 19 with loss 0.22564\n",
      "------------------------------------------\n",
      "L1 loss: 0.07293745130300522\n",
      "Training batch 20 with loss 0.32027\n",
      "------------------------------------------\n",
      "L1 loss: 0.06882254034280777\n",
      "Training batch 21 with loss 0.27414\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.06805704534053802\n",
      "Training batch 22 with loss 0.29756\n",
      "------------------------------------------\n",
      "L1 loss: 0.08156292885541916\n",
      "Training batch 23 with loss 0.36712\n",
      "------------------------------------------\n",
      "L1 loss: 0.06961101293563843\n",
      "Training batch 24 with loss 0.29742\n",
      "------------------------------------------\n",
      "L1 loss: 0.07563508301973343\n",
      "Training batch 25 with loss 0.30839\n",
      "------------------------------------------\n",
      "L1 loss: 0.07542230188846588\n",
      "Training batch 26 with loss 0.27711\n",
      "------------------------------------------\n",
      "L1 loss: 0.056470707058906555\n",
      "Training batch 27 with loss 0.29793\n",
      "------------------------------------------\n",
      "L1 loss: 0.056939225643873215\n",
      "Training batch 28 with loss 0.29389\n",
      "------------------------------------------\n",
      "L1 loss: 0.05590330436825752\n",
      "Training batch 29 with loss 0.23030\n",
      "------------------------------------------\n",
      "L1 loss: 0.05920841172337532\n",
      "Training batch 30 with loss 0.26853\n",
      "------------------------------------------\n",
      "L1 loss: 0.06540235877037048\n",
      "Training batch 31 with loss 0.31035\n",
      "------------------------------------------\n",
      "L1 loss: 0.0702538937330246\n",
      "Training batch 32 with loss 0.27606\n",
      "------------------------------------------\n",
      "L1 loss: 0.060782112181186676\n",
      "Training batch 33 with loss 0.24662\n",
      "------------------------------------------\n",
      "L1 loss: 0.07435659319162369\n",
      "Training batch 34 with loss 0.32145\n",
      "------------------------------------------\n",
      "L1 loss: 0.09548662602901459\n",
      "Training batch 35 with loss 0.32944\n",
      "------------------------------------------\n",
      "L1 loss: 0.08420522511005402\n",
      "Training batch 36 with loss 0.27553\n",
      "------------------------------------------\n",
      "L1 loss: 0.06757987290620804\n",
      "Training batch 37 with loss 0.24794\n",
      "------------------------------------------\n",
      "L1 loss: 0.060668472200632095\n",
      "Training batch 38 with loss 0.21519\n",
      "------------------------------------------\n",
      "L1 loss: 0.06686030328273773\n",
      "Training batch 39 with loss 0.29593\n",
      "------------------------------------------\n",
      "L1 loss: 0.06946924328804016\n",
      "Training batch 40 with loss 0.25679\n",
      "------------------------------------------\n",
      "L1 loss: 0.053914159536361694\n",
      "Training batch 41 with loss 0.27920\n",
      "------------------------------------------\n",
      "L1 loss: 0.08093705028295517\n",
      "Training batch 42 with loss 0.26729\n",
      "------------------------------------------\n",
      "L1 loss: 0.06967353820800781\n",
      "Training batch 43 with loss 0.28293\n",
      "------------------------------------------\n",
      "L1 loss: 0.05911984294652939\n",
      "Training batch 44 with loss 0.27202\n",
      "------------------------------------------\n",
      "L1 loss: 0.0928313285112381\n",
      "Training batch 45 with loss 0.35934\n",
      "------------------------------------------\n",
      "L1 loss: 0.07451444119215012\n",
      "Training batch 46 with loss 0.26275\n",
      "------------------------------------------\n",
      "L1 loss: 0.10911891609430313\n",
      "Training batch 47 with loss 0.37104\n",
      "------------------------------------------\n",
      "L1 loss: 0.050571322441101074\n",
      "Training batch 48 with loss 0.25722\n",
      "------------------------------------------\n",
      "L1 loss: 0.07871659100055695\n",
      "Training batch 49 with loss 0.35441\n",
      "------------------------------------------\n",
      "L1 loss: 0.07288634032011032\n",
      "Training batch 50 with loss 0.28217\n",
      "------------------------------------------\n",
      "L1 loss: 0.07752913236618042\n",
      "Training batch 51 with loss 0.32398\n",
      "------------------------------------------\n",
      "L1 loss: 0.08855270594358444\n",
      "Training batch 52 with loss 0.28515\n",
      "------------------------------------------\n",
      "L1 loss: 0.07710450142621994\n",
      "Training batch 53 with loss 0.28005\n",
      "------------------------------------------\n",
      "L1 loss: 0.07684624940156937\n",
      "Training batch 54 with loss 0.27376\n",
      "------------------------------------------\n",
      "L1 loss: 0.06291098892688751\n",
      "Training batch 55 with loss 0.31701\n",
      "------------------------------------------\n",
      "L1 loss: 0.04951675608754158\n",
      "Training batch 56 with loss 0.37989\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776992976665497\n",
      "Training batch 57 with loss 0.27189\n",
      "------------------------------------------\n",
      "L1 loss: 0.06578199565410614\n",
      "Training batch 58 with loss 0.27111\n",
      "------------------------------------------\n",
      "L1 loss: 0.08228874206542969\n",
      "Training batch 59 with loss 0.29659\n",
      "------------------------------------------\n",
      "L1 loss: 0.06674414873123169\n",
      "Training batch 60 with loss 0.27892\n",
      "------------------------------------------\n",
      "L1 loss: 0.10438425838947296\n",
      "Training batch 61 with loss 0.36517\n",
      "------------------------------------------\n",
      "L1 loss: 0.05091280862689018\n",
      "Training batch 62 with loss 0.41980\n",
      "------------------------------------------\n",
      "L1 loss: 0.06427206099033356\n",
      "Training batch 63 with loss 0.26523\n",
      "------------------------------------------\n",
      "L1 loss: 0.06891480088233948\n",
      "Training batch 64 with loss 0.28985\n",
      "------------------------------------------\n",
      "L1 loss: 0.05831758305430412\n",
      "Training batch 65 with loss 0.23069\n",
      "------------------------------------------\n",
      "L1 loss: 0.05666483938694\n",
      "Training batch 66 with loss 0.35070\n",
      "------------------------------------------\n",
      "L1 loss: 0.06276337802410126\n",
      "Training batch 67 with loss 0.26070\n",
      "------------------------------------------\n",
      "L1 loss: 0.07461127638816833\n",
      "Training batch 68 with loss 0.29535\n",
      "------------------------------------------\n",
      "L1 loss: 0.05247804522514343\n",
      "Training batch 69 with loss 0.25964\n",
      "------------------------------------------\n",
      "L1 loss: 0.07925231754779816\n",
      "Training batch 70 with loss 0.29183\n",
      "------------------------------------------\n",
      "L1 loss: 0.06262067705392838\n",
      "Training batch 71 with loss 0.40663\n",
      "------------------------------------------\n",
      "L1 loss: 0.08348680287599564\n",
      "Training batch 72 with loss 0.28975\n",
      "------------------------------------------\n",
      "L1 loss: 0.07518826425075531\n",
      "Training batch 73 with loss 0.30431\n",
      "------------------------------------------\n",
      "L1 loss: 0.09573187679052353\n",
      "Training batch 74 with loss 0.31167\n",
      "------------------------------------------\n",
      "L1 loss: 0.07350795716047287\n",
      "Training batch 75 with loss 0.27517\n",
      "------------------------------------------\n",
      "L1 loss: 0.08716943860054016\n",
      "Training batch 76 with loss 0.29596\n",
      "------------------------------------------\n",
      "L1 loss: 0.07880714535713196\n",
      "Training batch 77 with loss 0.26483\n",
      "------------------------------------------\n",
      "L1 loss: 0.07744952291250229\n",
      "Training batch 78 with loss 0.28946\n",
      "------------------------------------------\n",
      "L1 loss: 0.1075279638171196\n",
      "Training batch 79 with loss 0.36749\n",
      "------------------------------------------\n",
      "L1 loss: 0.07655439525842667\n",
      "Training batch 80 with loss 0.28415\n",
      "------------------------------------------\n",
      "L1 loss: 0.08427176624536514\n",
      "Training batch 81 with loss 0.31301\n",
      "------------------------------------------\n",
      "L1 loss: 0.08387304097414017\n",
      "Training batch 82 with loss 0.36182\n",
      "------------------------------------------\n",
      "L1 loss: 0.07892321050167084\n",
      "Training batch 83 with loss 0.27956\n",
      "------------------------------------------\n",
      "L1 loss: 0.06776644289493561\n",
      "Training batch 84 with loss 0.28944\n",
      "------------------------------------------\n",
      "L1 loss: 0.06434258073568344\n",
      "Training batch 85 with loss 0.26907\n",
      "------------------------------------------\n",
      "L1 loss: 0.08059581369161606\n",
      "Training batch 86 with loss 0.32160\n",
      "------------------------------------------\n",
      "L1 loss: 0.10362058877944946\n",
      "Training batch 87 with loss 0.37812\n",
      "------------------------------------------\n",
      "L1 loss: 0.07745211571455002\n",
      "Training batch 88 with loss 0.32326\n",
      "------------------------------------------\n",
      "L1 loss: 0.0706278383731842\n",
      "Training batch 89 with loss 0.28136\n",
      "------------------------------------------\n",
      "L1 loss: 0.07740017026662827\n",
      "Training batch 90 with loss 0.27126\n",
      "------------------------------------------\n",
      "L1 loss: 0.06619198620319366\n",
      "Training batch 91 with loss 0.26418\n",
      "------------------------------------------\n",
      "L1 loss: 0.0952746793627739\n",
      "Training batch 92 with loss 0.31493\n",
      "------------------------------------------\n",
      "L1 loss: 0.07175667583942413\n",
      "Training batch 93 with loss 0.27217\n",
      "------------------------------------------\n",
      "L1 loss: 0.0866171270608902\n",
      "Training batch 94 with loss 0.30671\n",
      "------------------------------------------\n",
      "L1 loss: 0.0716514065861702\n",
      "Training batch 95 with loss 0.26374\n",
      "------------------------------------------\n",
      "L1 loss: 0.059846896678209305\n",
      "Training batch 96 with loss 0.29603\n",
      "------------------------------------------\n",
      "L1 loss: 0.08895253390073776\n",
      "Training batch 97 with loss 0.37481\n",
      "------------------------------------------\n",
      "L1 loss: 0.05571063235402107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 98 with loss 0.23282\n",
      "------------------------------------------\n",
      "L1 loss: 0.06366975605487823\n",
      "Training batch 99 with loss 0.26268\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.2947176739573479\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4883\n",
      "------------------------------------------\n",
      "L1 loss: 0.06158497929573059\n",
      "Training batch 0 with loss 0.24512\n",
      "------------------------------------------\n",
      "L1 loss: 0.08699523657560349\n",
      "Training batch 1 with loss 0.29291\n",
      "------------------------------------------\n",
      "L1 loss: 0.09432777017354965\n",
      "Training batch 2 with loss 0.31103\n",
      "------------------------------------------\n",
      "L1 loss: 0.06870550662279129\n",
      "Training batch 3 with loss 0.24980\n",
      "------------------------------------------\n",
      "L1 loss: 0.08266852796077728\n",
      "Training batch 4 with loss 0.33053\n",
      "------------------------------------------\n",
      "L1 loss: 0.05663631483912468\n",
      "Training batch 5 with loss 0.47956\n",
      "------------------------------------------\n",
      "L1 loss: 0.08019737899303436\n",
      "Training batch 6 with loss 0.27047\n",
      "------------------------------------------\n",
      "L1 loss: 0.08334550261497498\n",
      "Training batch 7 with loss 0.32742\n",
      "------------------------------------------\n",
      "L1 loss: 0.0803510919213295\n",
      "Training batch 8 with loss 0.33089\n",
      "------------------------------------------\n",
      "L1 loss: 0.06012079864740372\n",
      "Training batch 9 with loss 0.24715\n",
      "------------------------------------------\n",
      "L1 loss: 0.08350986987352371\n",
      "Training batch 10 with loss 0.28688\n",
      "------------------------------------------\n",
      "L1 loss: 0.06093231588602066\n",
      "Training batch 11 with loss 0.30325\n",
      "------------------------------------------\n",
      "L1 loss: 0.07610252499580383\n",
      "Training batch 12 with loss 0.31084\n",
      "------------------------------------------\n",
      "L1 loss: 0.0696062371134758\n",
      "Training batch 13 with loss 0.24129\n",
      "------------------------------------------\n",
      "L1 loss: 0.05212714150547981\n",
      "Training batch 14 with loss 0.25040\n",
      "------------------------------------------\n",
      "L1 loss: 0.08232830464839935\n",
      "Training batch 15 with loss 0.27974\n",
      "------------------------------------------\n",
      "L1 loss: 0.06189801171422005\n",
      "Training batch 16 with loss 0.25011\n",
      "------------------------------------------\n",
      "L1 loss: 0.08621625602245331\n",
      "Training batch 17 with loss 0.33978\n",
      "------------------------------------------\n",
      "L1 loss: 0.07954306900501251\n",
      "Training batch 18 with loss 0.30412\n",
      "------------------------------------------\n",
      "L1 loss: 0.06201782077550888\n",
      "Training batch 19 with loss 0.24337\n",
      "------------------------------------------\n",
      "L1 loss: 0.07203067094087601\n",
      "Training batch 20 with loss 0.32148\n",
      "------------------------------------------\n",
      "L1 loss: 0.07260183990001678\n",
      "Training batch 21 with loss 0.29333\n",
      "------------------------------------------\n",
      "L1 loss: 0.06897684186697006\n",
      "Training batch 22 with loss 0.27846\n",
      "------------------------------------------\n",
      "L1 loss: 0.08230448514223099\n",
      "Training batch 23 with loss 0.35372\n",
      "------------------------------------------\n",
      "L1 loss: 0.06654136627912521\n",
      "Training batch 24 with loss 0.30165\n",
      "------------------------------------------\n",
      "L1 loss: 0.07296513020992279\n",
      "Training batch 25 with loss 0.30057\n",
      "------------------------------------------\n",
      "L1 loss: 0.07879789918661118\n",
      "Training batch 26 with loss 0.27584\n",
      "------------------------------------------\n",
      "L1 loss: 0.055385589599609375\n",
      "Training batch 27 with loss 0.27820\n",
      "------------------------------------------\n",
      "L1 loss: 0.057086195796728134\n",
      "Training batch 28 with loss 0.27872\n",
      "------------------------------------------\n",
      "L1 loss: 0.06330128759145737\n",
      "Training batch 29 with loss 0.26582\n",
      "------------------------------------------\n",
      "L1 loss: 0.06081720069050789\n",
      "Training batch 30 with loss 0.27484\n",
      "------------------------------------------\n",
      "L1 loss: 0.06365302205085754\n",
      "Training batch 31 with loss 0.31266\n",
      "------------------------------------------\n",
      "L1 loss: 0.0700792595744133\n",
      "Training batch 32 with loss 0.29013\n",
      "------------------------------------------\n",
      "L1 loss: 0.06430607289075851\n",
      "Training batch 33 with loss 0.25100\n",
      "------------------------------------------\n",
      "L1 loss: 0.07540281862020493\n",
      "Training batch 34 with loss 0.31313\n",
      "------------------------------------------\n",
      "L1 loss: 0.09579482674598694\n",
      "Training batch 35 with loss 0.34254\n",
      "------------------------------------------\n",
      "L1 loss: 0.08542965352535248\n",
      "Training batch 36 with loss 0.28712\n",
      "------------------------------------------\n",
      "L1 loss: 0.06643348932266235\n",
      "Training batch 37 with loss 0.25916\n",
      "------------------------------------------\n",
      "L1 loss: 0.06279852986335754\n",
      "Training batch 38 with loss 0.21879\n",
      "------------------------------------------\n",
      "L1 loss: 0.06896625459194183\n",
      "Training batch 39 with loss 0.30528\n",
      "------------------------------------------\n",
      "L1 loss: 0.07221078872680664\n",
      "Training batch 40 with loss 0.25341\n",
      "------------------------------------------\n",
      "L1 loss: 0.06014307215809822\n",
      "Training batch 41 with loss 0.28858\n",
      "------------------------------------------\n",
      "L1 loss: 0.08282972127199173\n",
      "Training batch 42 with loss 0.28625\n",
      "------------------------------------------\n",
      "L1 loss: 0.06779910624027252\n",
      "Training batch 43 with loss 0.26790\n",
      "------------------------------------------\n",
      "L1 loss: 0.05935010686516762\n",
      "Training batch 44 with loss 0.27709\n",
      "------------------------------------------\n",
      "L1 loss: 0.09127706289291382\n",
      "Training batch 45 with loss 0.36954\n",
      "------------------------------------------\n",
      "L1 loss: 0.07521957904100418\n",
      "Training batch 46 with loss 0.27080\n",
      "------------------------------------------\n",
      "L1 loss: 0.11189232766628265\n",
      "Training batch 47 with loss 0.35853\n",
      "------------------------------------------\n",
      "L1 loss: 0.04447341710329056\n",
      "Training batch 48 with loss 0.22415\n",
      "------------------------------------------\n",
      "L1 loss: 0.0762038603425026\n",
      "Training batch 49 with loss 0.34115\n",
      "------------------------------------------\n",
      "L1 loss: 0.06899626553058624\n",
      "Training batch 50 with loss 0.45328\n",
      "------------------------------------------\n",
      "L1 loss: 0.07788343727588654\n",
      "Training batch 51 with loss 0.33348\n",
      "------------------------------------------\n",
      "L1 loss: 0.08211679011583328\n",
      "Training batch 52 with loss 0.32437\n",
      "------------------------------------------\n",
      "L1 loss: 0.08068202435970306\n",
      "Training batch 53 with loss 0.29263\n",
      "------------------------------------------\n",
      "L1 loss: 0.07891658693552017\n",
      "Training batch 54 with loss 0.28116\n",
      "------------------------------------------\n",
      "L1 loss: 0.06239280849695206\n",
      "Training batch 55 with loss 0.30764\n",
      "------------------------------------------\n",
      "L1 loss: 0.05215618014335632\n",
      "Training batch 56 with loss 0.24632\n",
      "------------------------------------------\n",
      "L1 loss: 0.06865717470645905\n",
      "Training batch 57 with loss 0.27505\n",
      "------------------------------------------\n",
      "L1 loss: 0.06419272720813751\n",
      "Training batch 58 with loss 0.25334\n",
      "------------------------------------------\n",
      "L1 loss: 0.07953613996505737\n",
      "Training batch 59 with loss 0.28725\n",
      "------------------------------------------\n",
      "L1 loss: 0.06289013475179672\n",
      "Training batch 60 with loss 0.27596\n",
      "------------------------------------------\n",
      "L1 loss: 0.09282473474740982\n",
      "Training batch 61 with loss 0.32612\n",
      "------------------------------------------\n",
      "L1 loss: 0.05597544088959694\n",
      "Training batch 62 with loss 0.24494\n",
      "------------------------------------------\n",
      "L1 loss: 0.06072566285729408\n",
      "Training batch 63 with loss 0.24869\n",
      "------------------------------------------\n",
      "L1 loss: 0.07169238477945328\n",
      "Training batch 64 with loss 0.26810\n",
      "------------------------------------------\n",
      "L1 loss: 0.057395678013563156\n",
      "Training batch 65 with loss 0.23413\n",
      "------------------------------------------\n",
      "L1 loss: 0.06633523851633072\n",
      "Training batch 66 with loss 0.27920\n",
      "------------------------------------------\n",
      "L1 loss: 0.06593287736177444\n",
      "Training batch 67 with loss 0.26519\n",
      "------------------------------------------\n",
      "L1 loss: 0.0730382576584816\n",
      "Training batch 68 with loss 0.30417\n",
      "------------------------------------------\n",
      "L1 loss: 0.048125408589839935\n",
      "Training batch 69 with loss 0.24100\n",
      "------------------------------------------\n",
      "L1 loss: 0.08156267553567886\n",
      "Training batch 70 with loss 0.31632\n",
      "------------------------------------------\n",
      "L1 loss: 0.07257325947284698\n",
      "Training batch 71 with loss 0.29258\n",
      "------------------------------------------\n",
      "L1 loss: 0.08524170517921448\n",
      "Training batch 72 with loss 0.30498\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.07161451876163483\n",
      "Training batch 73 with loss 0.29414\n",
      "------------------------------------------\n",
      "L1 loss: 0.09541783481836319\n",
      "Training batch 74 with loss 0.31125\n",
      "------------------------------------------\n",
      "L1 loss: 0.0739651769399643\n",
      "Training batch 75 with loss 0.27250\n",
      "------------------------------------------\n",
      "L1 loss: 0.08575651794672012\n",
      "Training batch 76 with loss 0.31876\n",
      "------------------------------------------\n",
      "L1 loss: 0.08094947785139084\n",
      "Training batch 77 with loss 0.26881\n",
      "------------------------------------------\n",
      "L1 loss: 0.08580086380243301\n",
      "Training batch 78 with loss 0.30610\n",
      "------------------------------------------\n",
      "L1 loss: 0.10631946474313736\n",
      "Training batch 79 with loss 0.36531\n",
      "------------------------------------------\n",
      "L1 loss: 0.07688559591770172\n",
      "Training batch 80 with loss 0.28607\n",
      "------------------------------------------\n",
      "L1 loss: 0.07973382622003555\n",
      "Training batch 81 with loss 0.30174\n",
      "------------------------------------------\n",
      "L1 loss: 0.08368333429098129\n",
      "Training batch 82 with loss 0.34833\n",
      "------------------------------------------\n",
      "L1 loss: 0.07408986240625381\n",
      "Training batch 83 with loss 0.28272\n",
      "------------------------------------------\n",
      "L1 loss: 0.06630206108093262\n",
      "Training batch 84 with loss 0.29062\n",
      "------------------------------------------\n",
      "L1 loss: 0.06521495431661606\n",
      "Training batch 85 with loss 0.28693\n",
      "------------------------------------------\n",
      "L1 loss: 0.08338231593370438\n",
      "Training batch 86 with loss 0.32036\n",
      "------------------------------------------\n",
      "L1 loss: 0.10116040706634521\n",
      "Training batch 87 with loss 0.40207\n",
      "------------------------------------------\n",
      "L1 loss: 0.07484786957502365\n",
      "Training batch 88 with loss 0.29768\n",
      "------------------------------------------\n",
      "L1 loss: 0.07016336917877197\n",
      "Training batch 89 with loss 0.26244\n",
      "------------------------------------------\n",
      "L1 loss: 0.07314921915531158\n",
      "Training batch 90 with loss 0.27071\n",
      "------------------------------------------\n",
      "L1 loss: 0.05770503357052803\n",
      "Training batch 91 with loss 0.25080\n",
      "------------------------------------------\n",
      "L1 loss: 0.09521950781345367\n",
      "Training batch 92 with loss 0.29619\n",
      "------------------------------------------\n",
      "L1 loss: 0.056190792471170425\n",
      "Training batch 93 with loss 0.22698\n",
      "------------------------------------------\n",
      "L1 loss: 0.08581272512674332\n",
      "Training batch 94 with loss 0.30699\n",
      "------------------------------------------\n",
      "L1 loss: 0.07362093031406403\n",
      "Training batch 95 with loss 0.26812\n",
      "------------------------------------------\n",
      "L1 loss: 0.06415750831365585\n",
      "Training batch 96 with loss 0.28424\n",
      "------------------------------------------\n",
      "L1 loss: 0.08415225148200989\n",
      "Training batch 97 with loss 0.33610\n",
      "------------------------------------------\n",
      "L1 loss: 0.055353447794914246\n",
      "Training batch 98 with loss 0.23752\n",
      "------------------------------------------\n",
      "L1 loss: 0.06343476474285126\n",
      "Training batch 99 with loss 0.25724\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29241119518876074\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4884\n",
      "------------------------------------------\n",
      "L1 loss: 0.059911202639341354\n",
      "Training batch 0 with loss 0.24583\n",
      "------------------------------------------\n",
      "L1 loss: 0.08498666435480118\n",
      "Training batch 1 with loss 0.27879\n",
      "------------------------------------------\n",
      "L1 loss: 0.09282050281763077\n",
      "Training batch 2 with loss 0.31849\n",
      "------------------------------------------\n",
      "L1 loss: 0.06268323212862015\n",
      "Training batch 3 with loss 0.47871\n",
      "------------------------------------------\n",
      "L1 loss: 0.0850660651922226\n",
      "Training batch 4 with loss 0.33070\n",
      "------------------------------------------\n",
      "L1 loss: 0.06703878939151764\n",
      "Training batch 5 with loss 0.34793\n",
      "------------------------------------------\n",
      "L1 loss: 0.07955139875411987\n",
      "Training batch 6 with loss 0.26693\n",
      "------------------------------------------\n",
      "L1 loss: 0.08358148485422134\n",
      "Training batch 7 with loss 0.34455\n",
      "------------------------------------------\n",
      "L1 loss: 0.07865102589130402\n",
      "Training batch 8 with loss 0.30457\n",
      "------------------------------------------\n",
      "L1 loss: 0.06305406242609024\n",
      "Training batch 9 with loss 0.24759\n",
      "------------------------------------------\n",
      "L1 loss: 0.08423738181591034\n",
      "Training batch 10 with loss 0.29751\n",
      "------------------------------------------\n",
      "L1 loss: 0.06418302655220032\n",
      "Training batch 11 with loss 0.29070\n",
      "------------------------------------------\n",
      "L1 loss: 0.07353316247463226\n",
      "Training batch 12 with loss 0.30502\n",
      "------------------------------------------\n",
      "L1 loss: 0.06830407679080963\n",
      "Training batch 13 with loss 0.24154\n",
      "------------------------------------------\n",
      "L1 loss: 0.053776148706674576\n",
      "Training batch 14 with loss 0.28501\n",
      "------------------------------------------\n",
      "L1 loss: 0.08420734107494354\n",
      "Training batch 15 with loss 0.27272\n",
      "------------------------------------------\n",
      "L1 loss: 0.06326252967119217\n",
      "Training batch 16 with loss 0.26732\n",
      "------------------------------------------\n",
      "L1 loss: 0.08674898743629456\n",
      "Training batch 17 with loss 0.35451\n",
      "------------------------------------------\n",
      "L1 loss: 0.08028054982423782\n",
      "Training batch 18 with loss 0.29789\n",
      "------------------------------------------\n",
      "L1 loss: 0.0632224828004837\n",
      "Training batch 19 with loss 0.23532\n",
      "------------------------------------------\n",
      "L1 loss: 0.07325749844312668\n",
      "Training batch 20 with loss 0.31354\n",
      "------------------------------------------\n",
      "L1 loss: 0.07038446515798569\n",
      "Training batch 21 with loss 0.28747\n",
      "------------------------------------------\n",
      "L1 loss: 0.06820641458034515\n",
      "Training batch 22 with loss 0.29164\n",
      "------------------------------------------\n",
      "L1 loss: 0.08891919255256653\n",
      "Training batch 23 with loss 0.36469\n",
      "------------------------------------------\n",
      "L1 loss: 0.06747493147850037\n",
      "Training batch 24 with loss 0.31407\n",
      "------------------------------------------\n",
      "L1 loss: 0.07600776106119156\n",
      "Training batch 25 with loss 0.31385\n",
      "------------------------------------------\n",
      "L1 loss: 0.07734178751707077\n",
      "Training batch 26 with loss 0.27048\n",
      "------------------------------------------\n",
      "L1 loss: 0.05692761018872261\n",
      "Training batch 27 with loss 0.28901\n",
      "------------------------------------------\n",
      "L1 loss: 0.057346273213624954\n",
      "Training batch 28 with loss 0.30211\n",
      "------------------------------------------\n",
      "L1 loss: 0.05754651501774788\n",
      "Training batch 29 with loss 0.25031\n",
      "------------------------------------------\n",
      "L1 loss: 0.05961865186691284\n",
      "Training batch 30 with loss 0.26434\n",
      "------------------------------------------\n",
      "L1 loss: 0.06416479498147964\n",
      "Training batch 31 with loss 0.30110\n",
      "------------------------------------------\n",
      "L1 loss: 0.0690125823020935\n",
      "Training batch 32 with loss 0.27582\n",
      "------------------------------------------\n",
      "L1 loss: 0.06350422650575638\n",
      "Training batch 33 with loss 0.25400\n",
      "------------------------------------------\n",
      "L1 loss: 0.0745849758386612\n",
      "Training batch 34 with loss 0.31086\n",
      "------------------------------------------\n",
      "L1 loss: 0.09457118809223175\n",
      "Training batch 35 with loss 0.32681\n",
      "------------------------------------------\n",
      "L1 loss: 0.08535268902778625\n",
      "Training batch 36 with loss 0.28405\n",
      "------------------------------------------\n",
      "L1 loss: 0.0685667097568512\n",
      "Training batch 37 with loss 0.26295\n",
      "------------------------------------------\n",
      "L1 loss: 0.06435898691415787\n",
      "Training batch 38 with loss 0.21956\n",
      "------------------------------------------\n",
      "L1 loss: 0.06814710050821304\n",
      "Training batch 39 with loss 0.30401\n",
      "------------------------------------------\n",
      "L1 loss: 0.06862255930900574\n",
      "Training batch 40 with loss 0.23877\n",
      "------------------------------------------\n",
      "L1 loss: 0.05884220823645592\n",
      "Training batch 41 with loss 0.27690\n",
      "------------------------------------------\n",
      "L1 loss: 0.07861823588609695\n",
      "Training batch 42 with loss 0.28483\n",
      "------------------------------------------\n",
      "L1 loss: 0.07226131856441498\n",
      "Training batch 43 with loss 0.29053\n",
      "------------------------------------------\n",
      "L1 loss: 0.054181069135665894\n",
      "Training batch 44 with loss 0.26227\n",
      "------------------------------------------\n",
      "L1 loss: 0.09287196397781372\n",
      "Training batch 45 with loss 0.36570\n",
      "------------------------------------------\n",
      "L1 loss: 0.07372378557920456\n",
      "Training batch 46 with loss 0.27374\n",
      "------------------------------------------\n",
      "L1 loss: 0.1100955531001091\n",
      "Training batch 47 with loss 0.34629\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.047750551253557205\n",
      "Training batch 48 with loss 0.24409\n",
      "------------------------------------------\n",
      "L1 loss: 0.08075319975614548\n",
      "Training batch 49 with loss 0.34411\n",
      "------------------------------------------\n",
      "L1 loss: 0.07182539254426956\n",
      "Training batch 50 with loss 0.28014\n",
      "------------------------------------------\n",
      "L1 loss: 0.08313894271850586\n",
      "Training batch 51 with loss 0.36947\n",
      "------------------------------------------\n",
      "L1 loss: 0.0906265377998352\n",
      "Training batch 52 with loss 0.29646\n",
      "------------------------------------------\n",
      "L1 loss: 0.07653231173753738\n",
      "Training batch 53 with loss 0.27275\n",
      "------------------------------------------\n",
      "L1 loss: 0.07237233221530914\n",
      "Training batch 54 with loss 0.25938\n",
      "------------------------------------------\n",
      "L1 loss: 0.06285248696804047\n",
      "Training batch 55 with loss 0.29774\n",
      "------------------------------------------\n",
      "L1 loss: 0.05202054977416992\n",
      "Training batch 56 with loss 0.24413\n",
      "------------------------------------------\n",
      "L1 loss: 0.0698215514421463\n",
      "Training batch 57 with loss 0.28234\n",
      "------------------------------------------\n",
      "L1 loss: 0.06943204998970032\n",
      "Training batch 58 with loss 0.27148\n",
      "------------------------------------------\n",
      "L1 loss: 0.0782046690583229\n",
      "Training batch 59 with loss 0.27556\n",
      "------------------------------------------\n",
      "L1 loss: 0.0654921606183052\n",
      "Training batch 60 with loss 0.25927\n",
      "------------------------------------------\n",
      "L1 loss: 0.09250924736261368\n",
      "Training batch 61 with loss 0.33124\n",
      "------------------------------------------\n",
      "L1 loss: 0.056456755846738815\n",
      "Training batch 62 with loss 0.25930\n",
      "------------------------------------------\n",
      "L1 loss: 0.06163737550377846\n",
      "Training batch 63 with loss 0.26390\n",
      "------------------------------------------\n",
      "L1 loss: 0.07097330689430237\n",
      "Training batch 64 with loss 0.27822\n",
      "------------------------------------------\n",
      "L1 loss: 0.059138573706150055\n",
      "Training batch 65 with loss 0.21524\n",
      "------------------------------------------\n",
      "L1 loss: 0.0662631168961525\n",
      "Training batch 66 with loss 0.27667\n",
      "------------------------------------------\n",
      "L1 loss: 0.06495383381843567\n",
      "Training batch 67 with loss 0.26173\n",
      "------------------------------------------\n",
      "L1 loss: 0.07221414148807526\n",
      "Training batch 68 with loss 0.29225\n",
      "------------------------------------------\n",
      "L1 loss: 0.04956353083252907\n",
      "Training batch 69 with loss 0.26404\n",
      "------------------------------------------\n",
      "L1 loss: 0.06559663265943527\n",
      "Training batch 70 with loss 0.37328\n",
      "------------------------------------------\n",
      "L1 loss: 0.07106085866689682\n",
      "Training batch 71 with loss 0.30223\n",
      "------------------------------------------\n",
      "L1 loss: 0.08807523548603058\n",
      "Training batch 72 with loss 0.31980\n",
      "------------------------------------------\n",
      "L1 loss: 0.06880190968513489\n",
      "Training batch 73 with loss 0.28842\n",
      "------------------------------------------\n",
      "L1 loss: 0.10745548456907272\n",
      "Training batch 74 with loss 0.44089\n",
      "------------------------------------------\n",
      "L1 loss: 0.07152073085308075\n",
      "Training batch 75 with loss 0.28546\n",
      "------------------------------------------\n",
      "L1 loss: 0.08664808422327042\n",
      "Training batch 76 with loss 0.29077\n",
      "------------------------------------------\n",
      "L1 loss: 0.07949255406856537\n",
      "Training batch 77 with loss 0.26039\n",
      "------------------------------------------\n",
      "L1 loss: 0.07896875590085983\n",
      "Training batch 78 with loss 0.29719\n",
      "------------------------------------------\n",
      "L1 loss: 0.10702706128358841\n",
      "Training batch 79 with loss 0.36933\n",
      "------------------------------------------\n",
      "L1 loss: 0.07580812275409698\n",
      "Training batch 80 with loss 0.28101\n",
      "------------------------------------------\n",
      "L1 loss: 0.08253263682126999\n",
      "Training batch 81 with loss 0.30312\n",
      "------------------------------------------\n",
      "L1 loss: 0.08384861052036285\n",
      "Training batch 82 with loss 0.35635\n",
      "------------------------------------------\n",
      "L1 loss: 0.07506664842367172\n",
      "Training batch 83 with loss 0.28881\n",
      "------------------------------------------\n",
      "L1 loss: 0.07067658752202988\n",
      "Training batch 84 with loss 0.30250\n",
      "------------------------------------------\n",
      "L1 loss: 0.062212247401475906\n",
      "Training batch 85 with loss 0.27575\n",
      "------------------------------------------\n",
      "L1 loss: 0.08093024790287018\n",
      "Training batch 86 with loss 0.27822\n",
      "------------------------------------------\n",
      "L1 loss: 0.08426560461521149\n",
      "Training batch 87 with loss 0.47450\n",
      "------------------------------------------\n",
      "L1 loss: 0.07569153606891632\n",
      "Training batch 88 with loss 0.30602\n",
      "------------------------------------------\n",
      "L1 loss: 0.0698331668972969\n",
      "Training batch 89 with loss 0.26966\n",
      "------------------------------------------\n",
      "L1 loss: 0.07347287237644196\n",
      "Training batch 90 with loss 0.27773\n",
      "------------------------------------------\n",
      "L1 loss: 0.06288369745016098\n",
      "Training batch 91 with loss 0.25365\n",
      "------------------------------------------\n",
      "L1 loss: 0.09234500676393509\n",
      "Training batch 92 with loss 0.30590\n",
      "------------------------------------------\n",
      "L1 loss: 0.06261082738637924\n",
      "Training batch 93 with loss 0.23731\n",
      "------------------------------------------\n",
      "L1 loss: 0.07313314080238342\n",
      "Training batch 94 with loss 0.26289\n",
      "------------------------------------------\n",
      "L1 loss: 0.07039956748485565\n",
      "Training batch 95 with loss 0.24869\n",
      "------------------------------------------\n",
      "L1 loss: 0.07055702805519104\n",
      "Training batch 96 with loss 0.29932\n",
      "------------------------------------------\n",
      "L1 loss: 0.08635615557432175\n",
      "Training batch 97 with loss 0.36381\n",
      "------------------------------------------\n",
      "L1 loss: 0.05602947995066643\n",
      "Training batch 98 with loss 0.23976\n",
      "------------------------------------------\n",
      "L1 loss: 0.06357478350400925\n",
      "Training batch 99 with loss 0.27892\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.29422484457492826\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4885\n",
      "------------------------------------------\n",
      "L1 loss: 0.06209931522607803\n",
      "Training batch 0 with loss 0.25012\n",
      "------------------------------------------\n",
      "L1 loss: 0.08604930341243744\n",
      "Training batch 1 with loss 0.28255\n",
      "------------------------------------------\n",
      "L1 loss: 0.09124363958835602\n",
      "Training batch 2 with loss 0.32680\n",
      "------------------------------------------\n",
      "L1 loss: 0.0813610851764679\n",
      "Training batch 3 with loss 0.27682\n",
      "------------------------------------------\n",
      "L1 loss: 0.08455532789230347\n",
      "Training batch 4 with loss 0.32887\n",
      "------------------------------------------\n",
      "L1 loss: 0.06664816290140152\n",
      "Training batch 5 with loss 0.31348\n",
      "------------------------------------------\n",
      "L1 loss: 0.06532671302556992\n",
      "Training batch 6 with loss 0.48569\n",
      "------------------------------------------\n",
      "L1 loss: 0.08465519547462463\n",
      "Training batch 7 with loss 0.32251\n",
      "------------------------------------------\n",
      "L1 loss: 0.07983876764774323\n",
      "Training batch 8 with loss 0.33568\n",
      "------------------------------------------\n",
      "L1 loss: 0.06048000231385231\n",
      "Training batch 9 with loss 0.24136\n",
      "------------------------------------------\n",
      "L1 loss: 0.08180533349514008\n",
      "Training batch 10 with loss 0.29495\n",
      "------------------------------------------\n",
      "L1 loss: 0.06073567271232605\n",
      "Training batch 11 with loss 0.28768\n",
      "------------------------------------------\n",
      "L1 loss: 0.0743124932050705\n",
      "Training batch 12 with loss 0.30848\n",
      "------------------------------------------\n",
      "L1 loss: 0.06914428621530533\n",
      "Training batch 13 with loss 0.24004\n",
      "------------------------------------------\n",
      "L1 loss: 0.05237056687474251\n",
      "Training batch 14 with loss 0.25481\n",
      "------------------------------------------\n",
      "L1 loss: 0.08584046363830566\n",
      "Training batch 15 with loss 0.26615\n",
      "------------------------------------------\n",
      "L1 loss: 0.06082246080040932\n",
      "Training batch 16 with loss 0.26800\n",
      "------------------------------------------\n",
      "L1 loss: 0.08650700002908707\n",
      "Training batch 17 with loss 0.34243\n",
      "------------------------------------------\n",
      "L1 loss: 0.08058708906173706\n",
      "Training batch 18 with loss 0.31315\n",
      "------------------------------------------\n",
      "L1 loss: 0.06162538379430771\n",
      "Training batch 19 with loss 0.23400\n",
      "------------------------------------------\n",
      "L1 loss: 0.07297778129577637\n",
      "Training batch 20 with loss 0.31002\n",
      "------------------------------------------\n",
      "L1 loss: 0.07041832059621811\n",
      "Training batch 21 with loss 0.29121\n",
      "------------------------------------------\n",
      "L1 loss: 0.06831055879592896\n",
      "Training batch 22 with loss 0.27716\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.08562527596950531\n",
      "Training batch 23 with loss 0.35255\n",
      "------------------------------------------\n",
      "L1 loss: 0.06722763925790787\n",
      "Training batch 24 with loss 0.29230\n",
      "------------------------------------------\n",
      "L1 loss: 0.07553992420434952\n",
      "Training batch 25 with loss 0.29164\n",
      "------------------------------------------\n",
      "L1 loss: 0.07428298890590668\n",
      "Training batch 26 with loss 0.27543\n",
      "------------------------------------------\n",
      "L1 loss: 0.054532747715711594\n",
      "Training batch 27 with loss 0.28344\n",
      "------------------------------------------\n",
      "L1 loss: 0.05739280581474304\n",
      "Training batch 28 with loss 0.30168\n",
      "------------------------------------------\n",
      "L1 loss: 0.05880534276366234\n",
      "Training batch 29 with loss 0.24595\n",
      "------------------------------------------\n",
      "L1 loss: 0.06360042840242386\n",
      "Training batch 30 with loss 0.27031\n",
      "------------------------------------------\n",
      "L1 loss: 0.0681651160120964\n",
      "Training batch 31 with loss 0.31247\n",
      "------------------------------------------\n",
      "L1 loss: 0.0700438842177391\n",
      "Training batch 32 with loss 0.27108\n",
      "------------------------------------------\n",
      "L1 loss: 0.061779022216796875\n",
      "Training batch 33 with loss 0.27127\n",
      "------------------------------------------\n",
      "L1 loss: 0.07513895630836487\n",
      "Training batch 34 with loss 0.32849\n",
      "------------------------------------------\n",
      "L1 loss: 0.09486730396747589\n",
      "Training batch 35 with loss 0.33438\n",
      "------------------------------------------\n",
      "L1 loss: 0.08402413129806519\n",
      "Training batch 36 with loss 0.27763\n",
      "------------------------------------------\n",
      "L1 loss: 0.06665123999118805\n",
      "Training batch 37 with loss 0.25679\n",
      "------------------------------------------\n",
      "L1 loss: 0.06374629586935043\n",
      "Training batch 38 with loss 0.22177\n",
      "------------------------------------------\n",
      "L1 loss: 0.06823048740625381\n",
      "Training batch 39 with loss 0.29080\n",
      "------------------------------------------\n",
      "L1 loss: 0.06970328837633133\n",
      "Training batch 40 with loss 0.40226\n",
      "------------------------------------------\n",
      "L1 loss: 0.05834503844380379\n",
      "Training batch 41 with loss 0.27857\n",
      "------------------------------------------\n",
      "L1 loss: 0.07780671119689941\n",
      "Training batch 42 with loss 0.30184\n",
      "------------------------------------------\n",
      "L1 loss: 0.06940212100744247\n",
      "Training batch 43 with loss 0.28804\n",
      "------------------------------------------\n",
      "L1 loss: 0.05828022211790085\n",
      "Training batch 44 with loss 0.26371\n",
      "------------------------------------------\n",
      "L1 loss: 0.09243430197238922\n",
      "Training batch 45 with loss 0.36671\n",
      "------------------------------------------\n",
      "L1 loss: 0.07573656737804413\n",
      "Training batch 46 with loss 0.26084\n",
      "------------------------------------------\n",
      "L1 loss: 0.11005580425262451\n",
      "Training batch 47 with loss 0.35681\n",
      "------------------------------------------\n",
      "L1 loss: 0.04534612223505974\n",
      "Training batch 48 with loss 0.25253\n",
      "------------------------------------------\n",
      "L1 loss: 0.0765327513217926\n",
      "Training batch 49 with loss 0.35371\n",
      "------------------------------------------\n",
      "L1 loss: 0.07673775404691696\n",
      "Training batch 50 with loss 0.47756\n",
      "------------------------------------------\n",
      "L1 loss: 0.0788220688700676\n",
      "Training batch 51 with loss 0.31271\n",
      "------------------------------------------\n",
      "L1 loss: 0.0904243066906929\n",
      "Training batch 52 with loss 0.28408\n",
      "------------------------------------------\n",
      "L1 loss: 0.08001235127449036\n",
      "Training batch 53 with loss 0.29355\n",
      "------------------------------------------\n",
      "L1 loss: 0.07764291018247604\n",
      "Training batch 54 with loss 0.27506\n",
      "------------------------------------------\n",
      "L1 loss: 0.0645807608962059\n",
      "Training batch 55 with loss 0.31976\n",
      "------------------------------------------\n",
      "L1 loss: 0.057409483939409256\n",
      "Training batch 56 with loss 0.27581\n",
      "------------------------------------------\n",
      "L1 loss: 0.06779054552316666\n",
      "Training batch 57 with loss 0.26869\n",
      "------------------------------------------\n",
      "L1 loss: 0.06871912628412247\n",
      "Training batch 58 with loss 0.27604\n",
      "------------------------------------------\n",
      "L1 loss: 0.07950791716575623\n",
      "Training batch 59 with loss 0.31030\n",
      "------------------------------------------\n",
      "L1 loss: 0.06764011830091476\n",
      "Training batch 60 with loss 0.28649\n",
      "------------------------------------------\n",
      "L1 loss: 0.10336881130933762\n",
      "Training batch 61 with loss 0.36306\n",
      "------------------------------------------\n",
      "L1 loss: 0.05631417781114578\n",
      "Training batch 62 with loss 0.24907\n",
      "------------------------------------------\n",
      "L1 loss: 0.0583854503929615\n",
      "Training batch 63 with loss 0.24446\n",
      "------------------------------------------\n",
      "L1 loss: 0.06963330507278442\n",
      "Training batch 64 with loss 0.29002\n",
      "------------------------------------------\n",
      "L1 loss: 0.060198456048965454\n",
      "Training batch 65 with loss 0.22834\n",
      "------------------------------------------\n",
      "L1 loss: 0.06729035824537277\n",
      "Training batch 66 with loss 0.27992\n",
      "------------------------------------------\n",
      "L1 loss: 0.06709659844636917\n",
      "Training batch 67 with loss 0.26690\n",
      "------------------------------------------\n",
      "L1 loss: 0.07551684230566025\n",
      "Training batch 68 with loss 0.31608\n",
      "------------------------------------------\n",
      "L1 loss: 0.05012351647019386\n",
      "Training batch 69 with loss 0.26324\n",
      "------------------------------------------\n",
      "L1 loss: 0.07668796926736832\n",
      "Training batch 70 with loss 0.29355\n",
      "------------------------------------------\n",
      "L1 loss: 0.07139210402965546\n",
      "Training batch 71 with loss 0.32859\n",
      "------------------------------------------\n",
      "L1 loss: 0.08237379044294357\n",
      "Training batch 72 with loss 0.30024\n",
      "------------------------------------------\n",
      "L1 loss: 0.07089639455080032\n",
      "Training batch 73 with loss 0.30347\n",
      "------------------------------------------\n",
      "L1 loss: 0.0951233059167862\n",
      "Training batch 74 with loss 0.33019\n",
      "------------------------------------------\n",
      "L1 loss: 0.0732026919722557\n",
      "Training batch 75 with loss 0.28275\n",
      "------------------------------------------\n",
      "L1 loss: 0.08551231771707535\n",
      "Training batch 76 with loss 0.30719\n",
      "------------------------------------------\n",
      "L1 loss: 0.08041650056838989\n",
      "Training batch 77 with loss 0.27388\n",
      "------------------------------------------\n",
      "L1 loss: 0.08127563446760178\n",
      "Training batch 78 with loss 0.30387\n",
      "------------------------------------------\n",
      "L1 loss: 0.10915874689817429\n",
      "Training batch 79 with loss 0.37938\n",
      "------------------------------------------\n",
      "L1 loss: 0.07609668374061584\n",
      "Training batch 80 with loss 0.26410\n"
     ]
    }
   ],
   "source": [
    "trainer.run(dl_train, 10000, path_record=RECORDS_PATH, path_model=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28d949d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.opt = torch.optim.Adam(trainer.model.parameters(), lr=1e-4)\n",
    "trainer.opt.param_groups[0]['lr'] = 1e-5\n",
    "#trainer.opt.param_groups[0]['lr'] = 9.3e-6\n",
    "trainer.scheduler.step_size = 5\n",
    "trainer.scheduler.gamma = .97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a532ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'step_size': 5,\n",
       " 'gamma': 0.97,\n",
       " 'base_lrs': [2.8242953648100006e-06],\n",
       " 'last_epoch': 124,\n",
       " 'verbose': False,\n",
       " '_step_count': 125,\n",
       " '_get_lr_called_within_step': False,\n",
       " '_last_lr': [8.160945513890403e-07]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainer.opt.param_groups[0]['lr'])\n",
    "trainer.scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6487e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "trainer.load(f'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\ESRGAN_Scratch\\model_L1_0_295.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739be1e2",
   "metadata": {},
   "source": [
    "- We trained the model on 400 images with a learning rate of 1e-4 for 200 epochs. The images are crops of the high resolution image.\n",
    "\n",
    "- Add a probability of 4/5 to resize the full image then crop it instead of cropping immediately, until epoch 362\n",
    "- Dropped learning rate to 1e-5 until epoch 630, saw a decrease in 0.05 in epoch loss. Epoch and batch losses are still very unstable.\n",
    "- Dropped learning rate to 1e-6 for 50 epochs. Epoch and batch losses are still very unstable. I see minor changes\n",
    "- Dropped learning rate to 1e-7 for 50 epochs. Epoch and batch losses are still very unstable. I see minor changes\n",
    "- Dropped learning rate to 1e-12 for 50 epochs. Epoch and batch losses are still very unstable. I see minor changes\n",
    "- (learning rate to 1e-3 made loss reach 1e5 => divergence) (run 8)\n",
    "- Resized the full image to the *crop_size* instead of 3 * *crop_size*. Also increased learning rate back to 1e-4. Model keeps learning steadily, might have a relatively small lr, maybe multiply it by 2 (we currently see an average decrease of 0.1/h). Saved the model (run 9)\n",
    "- initialized the training with l1 loss added to the vgg loss (run 10)\n",
    "- Error! Model save corrupted so we restart with the new combined loss from the old save (see \"Resized the full...\") (second part run 10)\n",
    "- carried on with the training\n",
    "\n",
    "- epoch 3597, dropped learning rate to 9e-5 (run 14)\n",
    "- epoch 3907, dropped lr to 8e-5 (run 15)\n",
    "- epoch 3975, implemented StepLR(step_size = 15, gamma=.9), dropped lr to 7e-5 (run 19)\n",
    "- epoch 3997, changed StepLR(step_size = 30, gamma=.9), back to lr 7e-5 (I put the scheduler in the batch loop, took it out in the epoch loop) (run 20)\n",
    "- epoch 4069, changed StepLR(step_size = 15, gamma=.9), back to lr 7e-5 (run 21)\n",
    "- run 24, increased lr back to 3e-5\n",
    "- run 25, dropped lr to 2e-5\n",
    "- run 26, dropped lr to 1e-5\n",
    "- run 27, increased gamma to .99 and lr to 9.9e-6\n",
    "- run 28, reset to post run 26, increased gamma to .99, dropped step size to 15 and lr to 9.8e-6\n",
    "- run 29, reset to post run 26, increased gamma to .99, dropped step size to 15 and lr to 9.6e-6\n",
    "- run 30, dropped step size to 5 and lr to 9.6e-7 (checking for a stabilisation of loss?)\n",
    "- run 31, lr to 1e-5\n",
    "\n",
    "\n",
    "Next step: add l1 or l2 norm to the global loss.\n",
    "\n",
    "\n",
    "ideas:\n",
    "- train this model on bigger learning rate (x2 first and we save and check)\n",
    "- increase the size of input images\n",
    "- use l1 or l2 norm\n",
    "- add l1 or l2 norm to the global loss.\n",
    "- sgd\n",
    "- modify momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335d647a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=48x48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=192x192>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=192x192>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o = data_train[89*4+2]\n",
    "display(torchvision.transforms.ToPILImage()(o[0]))\n",
    "display(torchvision.transforms.ToPILImage()(o[1]))\n",
    "trainer.display_result(data_train, 89*4+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "492b3464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=192x192>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.display_result(data_train, 89*4+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1636dea3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAICAIAAABChommAAABM0lEQVR4nAEoAdf+AXBHIjY3DL7P8frw//8XBuL8Bf39Hv31/yEBAU5dQuzmBvn45wQTEQL019IC7Q0VFwn6E/XX5QQB5PkM4/xndVAUIS/z9O4HJA4C5Nv64tkABAIW89n4Ce4EFwj+/uzu9g4BweoKyMn3ABQFARADBPz6+B0wFezsABXwFBEE+vcBCvf3/w4sFvv4ABwb9O0aCQnx/AQfExH83ugI8Aoi8/Dv4f4IPkQiM+ofPCX7CA3a5+f96hfx6tkE8vsLCfAPAgAVCu7uFkox/zU8HQzsESwQ5OXc+dzX1+EGDxH2Aufn8gUCBy7+4xoL7wv6CRnzAycnDAYQ+RknBdjW4KzE0N/38gQHCATu/An/GAj8FgT2/gYPAvn46vLt6gPl7+/j9BM8Qxn+EPKvDJC0aMHAwAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=12x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test = DIV2KEval(FOLDER_HR_TRAIN, 2)\n",
    "\n",
    "trainer.display_result(data_test, 89*4+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "447d2c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2040</th>\n",
       "      <th>1404</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2040</td>\n",
       "      <td>1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2040</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2040</td>\n",
       "      <td>1344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1608</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1356</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2040  1404\n",
       "0  2040  1848\n",
       "1  2040  1356\n",
       "2  2040  1344\n",
       "3  1608  2040\n",
       "4  1356  2040"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('hr_dims.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "14d54584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2040    1116\n",
       "1404     648\n",
       "dtype: int64"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mins = df.min(axis=1)\n",
    "df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "88e7850a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mins.hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
