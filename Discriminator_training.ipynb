{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPhdBA/uNJaS4Q2ze9kcFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincentbriat/Super-resolution-investigation/blob/main/Discriminator_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmqGDR8CNiU7"
      },
      "outputs": [],
      "source": [
        "# Checks if the code is in a colab notebook\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run on Colab notebook"
      ],
      "metadata": {
        "id": "5voJ6roJN27A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "  !pip install basicsr\n",
        "  drive.mount('/content/drive/')\n",
        "  !unzip /content/drive/MyDrive/Datasets/DIV2K_valid_HR.zip\n",
        "  !unzip /content/drive/MyDrive/Datasets/DIV2K_valid_LR_clean.zip\n",
        "  FOLDER_LR_TEST = 'DIV2K_valid_LR_clean'\n",
        "  FOLDER_HR_TEST = 'DIV2K_valid_HR'\n",
        "\n",
        "  STUDENT_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/student.pth'\n",
        "  STUDENT_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/student.csv'\n",
        "  GENERATOR_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/generator.pth'\n",
        "  GENERATOR_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/generator.csv'\n",
        "\n",
        "  TEACHER_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/ESRGAN_models/RealESRGAN_x4plus.pth'"
      ],
      "metadata": {
        "id": "DB4gYxJZN31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run on my Windows desktop"
      ],
      "metadata": {
        "id": "y3keGCgTN5-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not IN_COLAB:\n",
        "  FOLDER_LR_TEST = 'D:\\Downloads\\Div2k\\DIV2K_valid_LR_clean'\n",
        "  FOLDER_HR_TEST = 'D:\\Downloads\\Div2k\\DIV2K_valid_HR'\n",
        "  \n",
        "  STUDENT_MODEL_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\student.pth'\n",
        "  STUDENT_RECORDS_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\student.csv'\n",
        "  GENERATOR_MODEL_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\generator.pth'\n",
        "  GENERATOR_RECORDS_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\generator.csv'\n",
        "\n",
        "  TEACHER_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\ESRGAN_models\\RealESRGAN_x4plus.pth'"
      ],
      "metadata": {
        "id": "WnGM39h8N53v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "mxBzg_aBOANA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import cv2\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import queue\n",
        "import threading\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "import torchvision\n",
        "from os import listdir, environ, path\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Making sure to use the gpu, if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# torch.cuda.set_device(torch.device(0))"
      ],
      "metadata": {
        "id": "RAnDybsWOAHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DIV2KLoader(object):\n",
        "  def __init__(self, low_res_folder, high_res_folder = None, preprocessing_input = None, preprocessing_output = None, include_targets = True):\n",
        "    self.low_res_folder = low_res_folder\n",
        "    self.high_res_folder = high_res_folder\n",
        "    self.img_names = sorted(listdir(low_res_folder))\n",
        "    self.include_targets = include_targets\n",
        "    self.len = len(self.img_names)\n",
        "    \n",
        "    if include_targets or high_res_folder is not None:\n",
        "      self.target_names = sorted(listdir(high_res_folder))\n",
        "\n",
        "    if preprocessing_input is None:\n",
        "      self.preprocessing_input = torchvision.transforms.ToTensor()\n",
        "    else:\n",
        "      self.preprocessing_input = preprocessing_input\n",
        "\n",
        "    if preprocessing_output is None:\n",
        "      self.preprocessing_output = torchvision.transforms.ToTensor()\n",
        "    else:\n",
        "      self.preprocessing_output = preprocessing_output\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    # Get the right image\n",
        "    img = Image.open(Path(self.low_res_folder).joinpath(self.img_names[i]))\n",
        "    if self.include_targets or self.high_res_folder is not None:\n",
        "      target = Image.open(Path(self.high_res_folder).joinpath(self.target_names[i]))\n",
        "      return self.preprocessing_input(img), self.preprocessing_output(target)\n",
        "    else:\n",
        "      return self.preprocessing_input(img)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "  \n",
        "  def restrict_size(self, size):\n",
        "    if size < len(self.img_names) and size > 0:\n",
        "      self.len = size\n",
        "    else:\n",
        "      self.len = len(self.img_names)\n",
        "      print(f\"Size must be between 0 and {len(self.img_names)}\")"
      ],
      "metadata": {
        "id": "LAlAXNiVOCsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 is resize\n",
        "# 1 is crop center\n",
        "# 2 is crop random (requires finish implementation)\n",
        "\n",
        "MODE = 1\n",
        "IM_SIZE = 50\n",
        "\n",
        "def resize_ratio(img, size=IM_SIZE):\n",
        "  width, height = img.size\n",
        "  if width > height:\n",
        "    width, height = size, int(height * size / width)\n",
        "  else:\n",
        "    width, height = int(width * size / height), size\n",
        "  return width, height\n",
        "\n",
        "def preprocessing_output(img):\n",
        "  im_w, im_h = img.size\n",
        "  if MODE == 1:\n",
        "    img = img.crop(((im_w - 400)/2, (im_h - 400)/2, (im_w + 400)/2, (im_h + 400)/2))\n",
        "  elif MODE == 2:\n",
        "    width, height = resize_ratio(img)\n",
        "    origin_x, origin_y = 0, 0\n",
        "    img = img.crop((origin_x, origin_y, origin_x + width*4, origin_y + height*4))\n",
        "  return torchvision.transforms.ToTensor()(img).to(device)\n",
        "\n",
        "def preprocessing_input(img):\n",
        "  im_w, im_h = img.size\n",
        "  if MODE == 1:\n",
        "    img = img.crop(((im_w - 400)/2, (im_h - 400)/2, (im_w + 400)/2, (im_h + 400)/2)).resize((100, 100))\n",
        "  elif MODE == 2:\n",
        "    width, height = resize_ratio(img)\n",
        "    origin_x, origin_y = 0, 0\n",
        "    img = img.crop((origin_x, origin_y, origin_x + width*4, origin_y + height*4)).resize((width, height))\n",
        "  return torchvision.transforms.ToTensor()(img).to(device)\n",
        "\n",
        "data_test = DIV2KLoader(FOLDER_HR_TEST, FOLDER_HR_TEST, preprocessing_input=preprocessing_input, preprocessing_output=preprocessing_output)\n",
        "data_test.restrict_size(10)\n",
        "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "zUn-cvdGOF0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "vd6trYshOGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, use_act: bool, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
        "    self.activation = nn.LeakyReLU(1, inplace=True) if use_act else nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.activation(self.conv(x))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels, middle_channels = 32, residual_scale = .2):\n",
        "    super().__init__()\n",
        "    self.block = nn.ModuleList([ConvBlock(in_channels + i * middle_channels,\n",
        "                                  middle_channels if i<4 else in_channels,\n",
        "                                  3,\n",
        "                                  stride=1,\n",
        "                                  padding=1,\n",
        "                                  use_act=i<4) for i in range(5)])\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    input = x\n",
        "    for conv in self.block:\n",
        "      out = conv(input)\n",
        "      input = torch.cat([input, out], dim=1)\n",
        "    return self.residual_scale * out + x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  "
      ],
      "metadata": {
        "id": "ejsr-lBJOMH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}