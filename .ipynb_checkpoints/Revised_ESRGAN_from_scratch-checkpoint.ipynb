{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741e828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the code is in a colab notebook\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f495c79",
   "metadata": {},
   "source": [
    "Run on Colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d565a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install basicsr\n",
    "  drive.mount('/content/drive/')\n",
    "  !unzip /content/drive/MyDrive/Datasets/DIV2K_valid_HR.zip\n",
    "  !unzip /content/drive/MyDrive/Datasets/DIV2K_valid_LR_clean.zip\n",
    "  !unzip /content/drive/MyDrive/Datasets/DIV2K_train_LR_clean.zip\n",
    "  #!unzip /content/drive/MyDrive/Datasets/DIV2K_train_HR.zip\n",
    "  !unzip /content/drive/MyDrive/ML/Indiv_Project/Second_Year/Pruning/80_percent/gen_images_0.zip\n",
    "  FOLDER_LR_TEST = 'DIV2K_valid_LR_clean'\n",
    "  FOLDER_HR_TEST = 'DIV2K_valid_HR'\n",
    "  FOLDER_LR_TRAIN = 'DIV2K_train_LR_clean'\n",
    "  #FOLDER_HR_TRAIN = 'DIV2K_train_HR'\n",
    "  FOLDER_GEN_IMAGES = 'gen_images_0'\n",
    "\n",
    "  STUDENT_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/student.pth'\n",
    "  STUDENT_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/student.csv'\n",
    "  GENERATOR_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/generator.pth'\n",
    "  GENERATOR_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/Models/generator.csv'\n",
    "  DISCRIMINATOR_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/Pruning/80_percent/discriminator.pth'\n",
    "  DISCRIMINATOR_RECORDS_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/Pruning/80_percent/discriminator.csv'\n",
    "  \n",
    "  PRUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son.pth'\n",
    "  FINE_TUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son_fine.pth'\n",
    "\n",
    "  TEACHER_MODEL_PATH = 'drive/MyDrive/ML/Indiv_Project/Second_Year/KD/ESRGAN_models/RealESRGAN_x4plus.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a512a8b",
   "metadata": {
    "id": "GqUMHJXSUzfi"
   },
   "source": [
    "Run on my Windows desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86f60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "  FOLDER_LR_TEST = 'D:\\Downloads\\Div2k\\DIV2K_valid_LR_clean'\n",
    "  FOLDER_HR_TEST = 'D:\\Downloads\\Div2k\\DIV2K_valid_HR'\n",
    "  FOLDER_LR_TRAIN = 'D:\\Downloads\\Div2k\\DIV2K_train_LR_clean'\n",
    "  FOLDER_HR_TRAIN = 'D:\\Downloads\\Div2k\\DIV2K_train_HR'\n",
    "  \n",
    "  STUDENT_MODEL_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\student.pth'\n",
    "  STUDENT_RECORDS_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\student.csv'\n",
    "  GENERATOR_MODEL_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\generator.pth'\n",
    "  GENERATOR_RECORDS_PATH='D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\Models\\generator.csv'\n",
    "  DISCRIMINATOR_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\discriminator.pth'\n",
    "  DISCRIMINATOR_MODEL_PATH_24 = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\discriminator_24.pth'\n",
    "  DISCRIMINATOR_RECORDS_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\discriminator.csv'\n",
    "  FOLDER_GEN_IMAGES = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\gen_images_0'\n",
    "  \n",
    "  PRUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son.pth'\n",
    "  FINE_TUNED_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\Pruning\\80_percent\\son_fine.pth'\n",
    "  \n",
    "  TEACHER_MODEL_PATH = 'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\KD\\ESRGAN_models\\RealESRGAN_x4plus.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dad9c",
   "metadata": {
    "id": "PLmP_9ta1bdM"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864e25d6",
   "metadata": {
    "id": "PLmP_9ta1bdM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\anaconda3\\envs\\ai-robotics\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "import torchvision\n",
    "from os import listdir, environ, path\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "# Making sure to use the gpu, if available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_device(torch.device(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, use_act: bool, **kwargs):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "    self.activation = nn.LeakyReLU(.2, inplace=True) if use_act else nn.Identity()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.activation(self.conv(x))\n",
    "\n",
    "class RDB(nn.Module):\n",
    "  def __init__(self, in_channels, middle_channels = 32, residual_scale = .2):\n",
    "    super().__init__()\n",
    "    self.residual_scale = residual_scale\n",
    "    self.block = nn.ModuleList([ConvBlock(in_channels + i * middle_channels,\n",
    "                                  middle_channels if i<4 else in_channels,\n",
    "                                  3,\n",
    "                                  stride=1,\n",
    "                                  padding=1,\n",
    "                                  use_act=i<4) for i in range(5)])\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = x\n",
    "    for conv in self.block:\n",
    "      out = conv(input)\n",
    "      input = torch.cat([input, out], dim=1)\n",
    "    return self.residual_scale * out + x\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "  def __init__(self, in_channels, mid_channels = 32, residual_scale = .2):\n",
    "    super().__init__()\n",
    "    self.residual_scale = residual_scale\n",
    "    self.model = nn.Sequential(*[RDB(in_channels, middle_channels = mid_channels, residual_scale = residual_scale) for _ in range(3)])\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.model(x) * self.residual_scale + x\n",
    "\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, out_channels = 64) -> None:\n",
    "    super().__init__()\n",
    "    self.model = nn.Conv2d(3, out_channels, 3, stride=1, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "class Tail(nn.Module):\n",
    "  def __init__(self, in_channels = 64) -> None:\n",
    "    super().__init__()\n",
    "    self.model = nn.Sequential(nn.Conv2d(in_channels, 256, 3, stride=1, padding=1),\n",
    "                               nn.Upsample(scale_factor=4, mode='nearest'),\n",
    "                               nn.LeakyReLU(.2, inplace=True),\n",
    "                               nn.Conv2d(256, 3, 3, stride=1, padding=1))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "# Original code for ESRGAN:\n",
    "# https://github.com/xinntao/ESRGAN/blob/master/RRDBNet_arch.py\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_layer(block, n_layers, **kwargs):\n",
    "    layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(block(**kwargs))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResidualDenseBlock_5C(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32, bias=True):\n",
    "        super(ResidualDenseBlock_5C, self).__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # initialization\n",
    "        # mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    '''Residual in Residual Dense Block'''\n",
    "\n",
    "    def __init__(self, nf, gc=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.RDB1 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB2 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB3 = ResidualDenseBlock_5C(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.RDB1(x)\n",
    "        out = self.RDB2(out)\n",
    "        out = self.RDB3(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=23, gc=32):\n",
    "        super(RRDBNet, self).__init__()\n",
    "\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
    "        self.RRDB_trunk = make_layer(RRDB, nb, nf=nf, gc=gc)\n",
    "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        #### upsampling\n",
    "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv_first(x)\n",
    "        trunk = self.trunk_conv(self.RRDB_trunk(feat))\n",
    "        feat = feat + trunk\n",
    "\n",
    "        feat = self.lrelu(self.upconv1(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        feat = self.lrelu(self.upconv2(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        out = self.conv_last(self.lrelu(self.HRconv(feat)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24742366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KLoader(object):\n",
    "  def __init__(self,  high_res_folder, low_res_size: int = 48):\n",
    "    self.high_res_folder = high_res_folder\n",
    "    self.target_names = sorted(listdir(high_res_folder))\n",
    "    self.len = len(self.target_names)\n",
    "    self.crop_size = low_res_size*4\n",
    "    self.resize = torchvision.transforms.Resize(low_res_size)\n",
    "\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    # Get the right target image\n",
    "    target = Image.open(Path(self.high_res_folder).joinpath(self.target_names[i]))\n",
    "    target = torchvision.transforms.ToTensor()(target)\n",
    "    \n",
    "    # Crop of target image\n",
    "    x, y, h, w = torchvision.transforms.RandomCrop.get_params(target, output_size=(self.crop_size, self.crop_size))\n",
    "    target = torchvision.transforms.functional.crop(target, x, y, h, w)\n",
    "    \n",
    "    # resizing for input image\n",
    "    img = self.resize(target)\n",
    "    \n",
    "    return img, target\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "  \n",
    "  def restrict_size(self, size):\n",
    "    if size < len(self.target_names) and size > 0:\n",
    "      self.len = size\n",
    "    else:\n",
    "      self.len = len(self.img_names)\n",
    "      print(f\"Size must be between 0 and {len(self.img_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "394466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.L1Loss()\n",
    "        self.vgg = vgg19(weights='DEFAULT').features[:35].eval().to(DEVICE)\n",
    "        \n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        vgg_input_features = self.vgg(X)\n",
    "        vgg_target_features = self.vgg(y)\n",
    "        return self.loss(vgg_input_features, vgg_target_features)\n",
    "    \n",
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = VGGLoss()\n",
    "        self.cur_epoch = 0\n",
    "        self.writer = SummaryWriter(f'runs/ESRGAN_Scratch/tensorboard')\n",
    "\n",
    "    def run(self, dataloader, epochs, path_record: str = None, path_model: str = None, load_epoch: bool = False):\n",
    "        # Initializes the records file if not existant\n",
    "\n",
    "        if path_record and not Path(path_record).is_file():\n",
    "            self.initialize_csv(path_record)\n",
    "            self.cur_epoch = 0\n",
    "        elif load_epoch:\n",
    "            # Loads the next epoch to continue training\n",
    "            self.load_cur_epoch(path_record)\n",
    "\n",
    "        while self.cur_epoch < epochs:\n",
    "            print(f'Epoch: {self.cur_epoch}')\n",
    "            loss = self.train_one_epoch(dataloader)\n",
    "            print('****************************************\\n')\n",
    "            print(f'Total Loss: {loss}')\n",
    "\n",
    "            self.writer.add_scalar('Training Loss - Epoch', loss, global_step = self.cur_epoch)\n",
    "            \n",
    "            if path_record:\n",
    "                self.record(path_record, [self.cur_epoch, loss])\n",
    "            if path_model:\n",
    "                self.save(path_model)\n",
    "            \n",
    "            self.cur_epoch += 1\n",
    "\n",
    "    def load_cur_epoch(self, path):\n",
    "        with open(path) as records_file:\n",
    "            epoch = records_file.readlines()[-1].split(',')[0]\n",
    "            self.cur_epoch = int(epoch) + 1\n",
    "\n",
    "    def load(self, path):\n",
    "        saved_info = torch.load(path, map_location=DEVICE)\n",
    "        self.model.load_state_dict(saved_info['model_state_dict'])\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=saved_info['lr'])\n",
    "        self.cur_epoch = saved_info['epoch'] + 1\n",
    "        # self.scheduler.load_state_dict(saved_info['lr_scheduler_state_dict'])\n",
    "        self.loss_fn = saved_info['loss_fn']\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({'model_state_dict': self.model.state_dict(),\n",
    "                'lr': self.opt.param_groups[0]['lr'],\n",
    "                'loss_fn': self.loss_fn,\n",
    "                'epoch': self.cur_epoch,\n",
    "                'lr_scheduler_state_dict': 0},\n",
    "               path)\n",
    "        print(\"Model saved successfully!\")\n",
    "\n",
    "    def initialize_csv(self, path):\n",
    "        self.record(path, ['Epoch', 'Loss'])\n",
    "        print(\"Records created Successfully!\")\n",
    "\n",
    "    def record(self, path, row):\n",
    "        with open(path, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow(row)\n",
    "        print(\"Model recorded successfully!\")\n",
    "\n",
    "    def train_one_epoch(self, dataloader):\n",
    "        cumu_loss = 0\n",
    "        self.model.train()\n",
    "        self.model.to(DEVICE)\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            out = self.model(X.to(DEVICE))\n",
    "            loss = self.learn(out, y)\n",
    "            cumu_loss += loss\n",
    "\n",
    "            if batch % 1 == 0:\n",
    "                print('------------------------------------------')\n",
    "                print(\n",
    "                    f'Training batch {batch} with loss {loss:.5f}')\n",
    "            \n",
    "            # Plot metrics to tensorboard\n",
    "            self.writer.add_scalar('Training Loss - Batches', loss, global_step = self.cur_epoch * len(dataloader) + batch)\n",
    "            out_grid = torchvision.utils.make_grid(out.clamp_(0.,1.))\n",
    "            self.writer.add_image(f'Images batch {batch}', out_grid, global_step=self.cur_epoch)\n",
    "            \n",
    "        return cumu_loss / len(dataloader)\n",
    "\n",
    "    def learn(self, out, y):\n",
    "        loss = self.loss_fn(out, y.to(DEVICE))\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9c12a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0a50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    RECORDS_PATH = f'drive/MyDrive/ML/Indiv_Project/Second_Year/ESRGAN_Scratch/training.csv'\n",
    "    MODEL_PATH = f'drive/MyDrive/ML/Indiv_Project/Second_Year/ESRGAN_Scratch/model.pth'\n",
    "else:\n",
    "    RECORDS_PATH = f'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\ESRGAN_Scratch\\\\training.csv'\n",
    "    MODEL_PATH = f'D:\\oldDrive\\ML\\Indiv_Project\\Second_Year\\ESRGAN_Scratch\\model.pth'\n",
    "\n",
    "model = RRDBNet()\n",
    "trainer = Trainer(model)\n",
    "\n",
    "data_train = DIV2KLoader(FOLDER_HR_TRAIN)\n",
    "data_train.restrict_size(400)\n",
    "dl_train = DataLoader(data_train, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcc4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model recorded successfully!\n",
      "Records created Successfully!\n",
      "Epoch: 0\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 1.11821\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 1.53194\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.22177\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 1.24798\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 1.22632\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.77997\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 1.22430\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.65233\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 1.18763\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 1.78387\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 1.16320\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 1.18051\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.43581\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 1.06198\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 1.39819\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 1.11825\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.92376\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.55109\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.15652\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 1.23769\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.30924\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.56723\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.01192\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.29973\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 1.15363\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.33408\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.79884\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.94265\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 1.07071\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 1.12608\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.82811\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 1.22271\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.38930\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 1.26122\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 1.04967\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.54531\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 2.14498\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.24400\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.97213\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.32284\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 1.34822\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.82141\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.65347\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 1.15303\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.95110\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.37739\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.14324\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.20294\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.98835\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.92956\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.23704\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.00041\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 1.08362\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 1.11271\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 1.13829\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.72094\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 1.20193\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 1.00650\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 1.03048\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.83608\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 1.16961\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.36943\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 1.06930\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.93581\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.61247\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.90368\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.86889\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 1.06284\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 1.21620\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 1.04544\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.83269\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.86883\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.99895\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.99741\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.16213\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.81570\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 1.00205\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 1.07275\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.93673\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.71741\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.25885\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 1.02653\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.09246\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.96921\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.00043\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.78533\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 1.46849\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.07992\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.78319\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 1.13979\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 1.00033\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.83816\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.83669\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 1.43522\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.28751\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.77597\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.25045\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.30334\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.77665\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 1.54754\n",
      "****************************************\n",
      "\n",
      "Total Loss: 1.170467904806137\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 1\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 1.02690\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 1.12516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 2 with loss 1.08733\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 1.07251\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.96057\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.21128\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 1.09274\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.17182\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.75072\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.88487\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.93780\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.95970\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.94801\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.71684\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.65029\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.82798\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.97699\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.39426\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.96461\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.84591\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.94737\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.93894\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.24518\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.18924\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.93330\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.25781\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.58288\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.56193\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.97241\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.68832\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.58205\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 1.06967\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.16093\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.89020\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 1.26333\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.75166\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.98093\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.91385\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 1.22210\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.92909\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.88224\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.57217\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.31227\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.95648\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.85456\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.97746\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.73542\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.03486\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.70372\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 1.16231\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.86920\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.02056\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.96496\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 1.03073\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.92295\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.86885\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.73415\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.87246\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 1.01278\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.66227\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.96752\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.74434\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.78552\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 1.11162\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.66962\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.99493\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.76069\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.53314\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 1.22014\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.86551\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.80637\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 1.11439\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.76419\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.75430\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.89912\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.86450\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.88079\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 1.13693\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.56139\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.35089\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.10381\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.89782\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.01214\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.86059\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.92353\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.52899\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.99779\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.14611\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.91921\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.91193\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.90399\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.75787\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.13201\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.76179\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.96552\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.91307\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.14980\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.55414\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.76806\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 1.07074\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.9690265077352523\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 2\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.88572\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.72919\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.92192\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.92360\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 1.14904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 5 with loss 1.18697\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.76302\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.20620\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 1.02562\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 1.28222\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.78138\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.72147\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.11710\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 1.02136\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.85113\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.87169\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.91056\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.83515\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.12290\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.86071\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.00638\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.15057\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.93049\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.04679\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.83114\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.06568\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.17544\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.77492\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 1.03519\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.58470\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.81506\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 1.02146\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.71915\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.64726\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.90645\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 0.87756\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 2.21878\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.21408\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 1.05375\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.11358\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 1.01797\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.83776\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.20689\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.74941\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.93784\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.07767\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.08941\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.12943\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.91142\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.72425\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.81832\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.35912\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.94534\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.89751\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 1.02885\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.53528\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.57344\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.61659\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.85865\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.59558\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.72232\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.32164\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.54125\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.99005\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.64442\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.50691\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.89421\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.72908\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.78726\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.73696\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.66878\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.94338\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.27357\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.44892\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.86256\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.87001\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.75887\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.66943\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.76957\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.19531\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.10576\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.82173\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.05059\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.86900\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.89512\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.79909\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 1.09191\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.34538\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.97396\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.67413\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.90204\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.81919\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.28807\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.74202\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.04011\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.95752\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.24230\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.53537\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.61612\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.96464\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.9333461907505989\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 3\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.74693\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.70697\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.99989\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.67732\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.83000\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.05484\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.96519\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.05071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 8 with loss 0.84360\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.83572\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 1.14172\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.79731\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.19488\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 1.28861\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.73137\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.57728\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.79324\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.26668\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.05498\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 1.04441\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.13294\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.16901\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.74793\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.96675\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.86368\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.92050\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.66833\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.55877\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.71650\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.85179\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.76919\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.50478\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.97331\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.96730\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 1.03002\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.42446\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.92641\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.86001\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.93154\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.80671\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.65341\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.58197\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.21722\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 1.13091\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.71232\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.12372\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.10724\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.01160\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.61131\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.97940\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.01673\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.21198\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.82962\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.93237\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.94704\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.93460\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.65893\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.77142\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.61263\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.15776\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 1.20183\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.78173\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.81584\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.84874\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.77798\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.66060\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.77031\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 1.09034\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 1.14145\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.54459\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.35924\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.96487\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.04669\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.64324\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.15664\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.91269\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.90528\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.71462\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.47391\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.11769\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.02770\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.55935\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.92362\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.73280\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.12653\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.76691\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.66053\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.62965\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 1.39701\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 1.28416\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.95656\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.73158\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.87906\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.93321\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.99711\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.44222\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.35959\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.02850\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.69864\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.92191\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.9157867601513863\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 4\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.75403\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.98462\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.04629\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.87344\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.79447\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.97406\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.71016\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.06991\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 1.21853\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.99364\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.67983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 11 with loss 0.92510\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.06870\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 1.00006\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.67499\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.75646\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.56404\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.15090\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.86105\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.73612\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.20297\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.80669\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.20758\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.12410\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.79563\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.81072\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.64023\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.63652\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.63596\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.81752\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.77926\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.59220\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.12295\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.88225\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.71887\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.05170\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.68831\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.58902\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.53014\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.61792\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.52577\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.62401\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.04528\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.87493\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.67936\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.82497\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.11337\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.00758\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 1.04812\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.67223\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.85216\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.40649\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.81174\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.74711\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.74056\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.76269\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.90844\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.59931\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.93083\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.91368\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.62256\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.77713\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.67334\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.55391\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.55592\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.82617\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 1.07300\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.87855\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 1.00655\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.55554\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.03945\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 1.07965\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.05885\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.83110\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.96877\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 1.31622\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.56809\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.84715\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.73729\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.42176\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.70277\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.72055\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.11811\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.92000\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.07707\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.73782\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.44431\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.22647\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.55023\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.77663\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.84848\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.92180\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.85084\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 1.05710\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.87182\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.62666\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.81794\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.27368\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.97166\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 1.01988\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8710041809082031\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 5\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.70028\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 1.07472\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.96772\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.57311\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.76244\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.26675\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.81926\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.87521\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.67667\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 1.01654\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.94562\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.72277\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.97517\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.78438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 14 with loss 0.97927\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.91419\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.79168\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.97907\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.90825\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.45928\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.07844\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.94745\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.87907\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.05229\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.80389\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.91922\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.59786\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.82227\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.40572\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.81751\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.58783\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.87195\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.22052\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.63571\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.78994\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 0.83494\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.05071\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.65075\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.50104\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.26764\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.97679\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.79938\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.34934\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.83633\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.62880\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.13154\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.92575\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.03086\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.40624\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.52108\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.99313\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.22088\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.60901\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.74481\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.54831\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.88738\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.82257\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.91790\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.36834\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.48304\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.98734\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.21611\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.68405\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.76292\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.45113\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.65405\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.72175\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.70627\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 1.11801\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.74024\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.95998\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.52383\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.85121\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.88670\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.98663\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 1.01649\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.87969\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 1.11235\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.84603\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.27544\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.06532\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.60650\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.96933\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.79737\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.14085\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.74003\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.64094\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.10110\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.72519\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.44762\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.77470\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.40191\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.04164\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.70414\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.09927\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.66281\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.75554\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.20765\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.57450\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 1.14541\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.841106063425541\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 6\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 1.10588\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.89533\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.54186\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 1.09414\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 1.02777\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.86884\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.66574\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.07132\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 1.29665\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.87173\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.89840\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.68363\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.94819\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.79400\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.41056\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.83500\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.81727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 17 with loss 0.97211\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.91511\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 1.02821\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.89646\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.88108\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.51324\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.02800\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.92134\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.95699\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.17001\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.78492\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.81932\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.93967\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.68265\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 1.01658\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.92107\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.87760\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.85328\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.39782\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.89725\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.32652\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.96011\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.11212\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 1.07676\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.92143\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.07410\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.78179\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.95576\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.94944\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.90016\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.28391\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.56865\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.80710\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.84469\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.99825\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.79829\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.94724\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.91822\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.48042\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.95396\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.53947\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.90678\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.99839\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.74265\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.05051\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.67778\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.88183\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.59828\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.76360\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.76799\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.65778\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.96049\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.55858\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.07539\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.80912\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.83804\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.57856\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.89797\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.95390\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.85046\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.86956\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.66538\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.11020\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.97482\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.61449\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.10912\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.91418\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.06084\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.59216\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.72502\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.24476\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.99365\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.58984\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.83634\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.79295\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.21834\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.65033\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.06031\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.75153\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.25706\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 0.98619\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.61550\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.70325\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8740136870741844\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 7\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.94196\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 1.14154\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.78579\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.69565\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.92782\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.16840\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.76136\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.06182\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.85850\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.76924\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.83190\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.51191\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.14074\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.97088\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.59757\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.43641\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.71741\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.97152\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.89860\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.67351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 20 with loss 1.17532\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.99360\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.05412\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.68508\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.70120\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.05652\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.69072\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.41149\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.57257\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.96481\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.77750\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.72087\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.67065\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.83895\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 1.06829\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.25240\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.87804\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.12168\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.62717\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.00612\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.90208\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.49184\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.42722\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.75506\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.54536\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.10634\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.71920\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.77133\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.76963\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.46707\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.75556\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.80407\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.81824\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.82461\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.72401\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.61526\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.49551\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.66151\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.49362\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.82736\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.88020\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.54427\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.70030\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.85428\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.39983\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.70018\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.44310\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.73021\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.60474\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.35998\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.01103\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.98225\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.98396\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.64889\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.83775\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.94704\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.77940\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.48813\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.54275\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.43350\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.01430\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.58570\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.05440\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.86855\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.03890\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.67866\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.52134\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.05591\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 1.02948\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.76133\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.82324\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.61380\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.84488\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.99690\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.99845\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.47667\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.91386\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.21791\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.59436\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.96429\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8102924972772598\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 8\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.69290\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.74564\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.00030\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.50428\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.93779\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.23956\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.73408\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.06679\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.99076\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 1.03027\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.83849\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.57594\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.89709\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.97249\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.47408\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.97739\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.85642\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.98591\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.14280\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.80543\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.78021\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.87437\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.05460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 23 with loss 0.95849\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.97901\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.72232\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.39690\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.94235\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.75919\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.85948\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.54344\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 1.04075\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.99289\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 1.03906\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.85633\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.10670\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.84970\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.21361\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.67812\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.12772\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.56790\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.62221\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.94049\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.89835\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.65450\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.08110\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.98691\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.03698\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.67468\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.60611\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.45686\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.55554\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.79147\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.68435\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.68927\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.72890\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.76543\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.86597\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.79038\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.66991\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.84938\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.09996\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.41305\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.70084\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.67590\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 1.01562\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.66453\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.62251\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.73037\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.43870\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.73961\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.82021\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.00837\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.46329\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.08292\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.67076\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.90416\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.82827\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.84657\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.03082\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.93996\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.55973\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.76721\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.54961\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.98576\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.70161\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.67471\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.86163\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 1.03660\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.79891\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.90888\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.52749\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.99381\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.97167\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.77938\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.73223\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.92275\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.24926\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.55515\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 1.02987\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8546296882629395\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 9\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.64823\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 1.06496\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.95937\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.84419\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.84139\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.10698\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.97703\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.04820\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.89268\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.65839\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.74227\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.53128\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.88228\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.78609\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.57062\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.63103\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.92160\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.95083\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.64688\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.85191\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.81331\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.02903\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.73057\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.78165\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.64462\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.11034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 26 with loss 0.87036\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.35792\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.59640\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.81287\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.74832\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.75341\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.74180\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.52774\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.82017\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 0.80106\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.74486\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.58629\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.76635\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.72730\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.93905\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.58357\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.07896\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.80762\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.72946\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.31417\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.50327\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.87244\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.84728\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.82843\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.69868\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.97212\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.83022\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.79812\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.85185\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.83888\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.38791\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.80354\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.97256\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.78584\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 1.01372\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.09281\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.61051\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.72461\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.62153\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 1.08625\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.53703\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.47737\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.89325\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.68638\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.85318\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.43604\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.77866\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.75439\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.72291\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.56899\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.76196\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.53301\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.68249\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.37603\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.06058\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.50905\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.95053\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 1.05406\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.71317\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.72448\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.88621\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.02379\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 1.07041\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.60658\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.78248\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.91210\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.25730\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.64840\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.77721\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.69162\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.56922\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.05053\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.56833\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.96008\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8199182251095771\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 10\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.67885\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.73011\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.91700\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.73023\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.90999\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.87519\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.96076\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.94186\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.76126\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.45665\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.97908\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 1.02042\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.87074\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.46760\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.86863\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.91081\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.60171\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.79459\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.15092\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.53571\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.03334\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.69999\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.71797\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.87693\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.86306\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.20900\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.88890\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.93373\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.99143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 29 with loss 0.55553\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.58443\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.64907\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.06964\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.64932\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.70225\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 0.85886\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.69167\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.42723\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.56637\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.54820\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.79956\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.73562\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.35785\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.70532\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.62965\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.72611\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.02467\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.97583\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.61041\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.78029\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.69573\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.16063\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.96575\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.60103\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.90196\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.94052\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.59732\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.45266\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.57557\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.16928\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.76031\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.72893\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.38569\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.80250\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.84451\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.67853\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.87848\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.47916\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.89531\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.68959\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.88695\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.81993\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.22399\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.55030\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.92205\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.83331\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.77622\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 1.03895\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.51205\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.25707\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.09397\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.87420\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.90748\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.72040\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.40663\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.36482\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.69877\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.42120\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.95894\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.84174\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.71657\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.69541\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.21427\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.67803\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.07688\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.38852\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.42053\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.39996\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.78209\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.84201\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8055102074146271\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 11\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.66922\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.94518\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.88586\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.78439\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.92897\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.32505\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.78296\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.05998\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.84504\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.90453\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.84543\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.80736\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.07384\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.96602\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.46882\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.79959\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.76029\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.95163\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.64225\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.60562\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.78029\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.09372\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.57694\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.02901\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.57446\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.83771\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.67337\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.75592\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.59821\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 1.07263\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.64057\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.87776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 32 with loss 0.82966\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.80540\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 1.05056\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.30152\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.16999\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.58618\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.71100\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.84725\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.82557\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.90881\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.97930\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 1.15170\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.84167\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.03520\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.40004\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.91869\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.75310\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.82120\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.91658\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.96540\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.91128\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.68419\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.73417\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.69672\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.60012\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.87116\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.63674\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.31592\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.77666\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.07849\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.38751\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 1.01339\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.78483\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.64278\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.68746\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.71528\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.93648\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.72381\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.79774\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.92974\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.83774\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.59952\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.05071\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.74062\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.82580\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 1.02141\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.85347\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.09208\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.98011\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.76795\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.09712\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.83481\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 1.26325\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.77412\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.60770\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.88914\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.75716\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.69285\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.76031\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.72557\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.32974\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.91428\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.60653\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.39269\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.84722\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.51429\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.53777\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.89196\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.835117730498314\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 12\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.96876\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.75471\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.11701\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.72824\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.84740\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.11998\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.61668\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.17742\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.66392\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.73429\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.99324\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.83203\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.97341\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.52082\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.39992\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.64927\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.92132\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.89227\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.99222\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.72516\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.11937\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.66678\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.14464\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.90597\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.53218\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.98412\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.40054\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.38213\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.35461\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.53858\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.76753\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.78198\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.01403\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.61350\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.73465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 35 with loss 1.29784\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.84982\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.54555\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 1.17434\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 1.28791\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.72680\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.55031\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.34467\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.97316\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.51048\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.98990\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.69904\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.75713\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.72666\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.80251\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.98485\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.25528\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 1.00229\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.95287\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.81899\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.83513\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.65415\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.61766\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.91756\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.58174\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.59160\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.27762\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.48136\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.91380\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.50246\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.29289\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.81881\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.83782\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.70162\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.52614\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.08017\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.82821\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.16468\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.75592\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.67141\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.84891\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.95026\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.80585\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.49596\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.13300\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.09808\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.71971\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.79594\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.76601\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.49993\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.88077\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.79566\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.90264\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.78569\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.98165\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.93165\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.54761\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.64203\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.71992\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.15290\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.84881\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.76149\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.45362\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.51276\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.80685\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8270749807357788\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 13\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.59207\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.76549\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.07915\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.47304\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.75172\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.73264\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.70548\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.25590\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.95162\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.93777\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.68625\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.57592\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.28306\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.70673\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.32292\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.59974\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.62730\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.11700\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.20717\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.70356\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.14027\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.41335\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.61298\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.05487\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.74520\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.52605\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.78553\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.72214\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.56341\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.91081\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.59696\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.85859\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.99822\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.78112\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.84474\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.22923\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.95773\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.81808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 38 with loss 0.20270\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.62050\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.68303\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.48071\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.99485\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.60515\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.57606\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.06240\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.00271\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.92454\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.85559\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.61009\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.74910\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.61042\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.86616\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.84518\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.76827\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.61924\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.63928\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.97131\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.63490\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.17542\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.56067\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.60594\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.41206\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 1.01679\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.68285\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.49240\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.46346\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.82756\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.98299\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.64879\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.84530\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.89815\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.83936\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.82222\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.11069\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.67477\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.71059\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.75612\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.69210\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 0.94903\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.11023\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.91525\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.10431\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.59329\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.82161\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.77235\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.96297\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.03036\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.89265\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.70293\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.69162\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.58320\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.19908\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.87827\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.64762\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.94813\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.95944\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 0.66552\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.46891\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.55283\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.7862384995818138\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 14\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.97664\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.72894\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.03515\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.62090\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.78331\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.80695\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.70682\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.02846\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.81557\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.67067\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.76631\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.85929\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.81680\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.77877\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.70871\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.60024\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.75422\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.85138\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.17026\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.93793\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.66231\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.09521\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.69609\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 1.06111\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.36102\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.73309\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.41644\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.44787\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.66422\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.52218\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.54695\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.81179\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.90824\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.64949\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.72571\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.47955\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.78073\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.62819\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 1.10969\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.62058\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.69454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 41 with loss 0.45781\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.10732\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.71301\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.48479\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.02241\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.71903\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.98896\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.88079\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.72534\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.73047\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.26788\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.86826\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.79509\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.71965\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.79921\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.56305\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.65605\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.60544\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.72043\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.35063\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.78817\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.52850\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.57356\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.62793\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.98059\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.64769\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.76601\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.92972\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.61473\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.86212\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.63603\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.91453\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.67108\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.64284\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.94601\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.69992\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.77648\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.81169\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.08529\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.93906\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.57630\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.65479\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.77253\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.95133\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 1.03868\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.71531\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.06734\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.76553\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.71210\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.72037\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.44513\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.71342\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.34320\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.11007\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.64285\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.59216\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.32040\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.57851\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 1.04909\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.7741898229718208\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 15\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.82474\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.98768\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.83082\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.98831\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.72341\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.17251\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.77986\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.57557\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.59733\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.58517\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.78371\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.54926\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.26643\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.92970\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.91738\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.68265\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.65007\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.25991\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.77778\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.81467\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.14556\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.62169\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.11705\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.91535\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.94833\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.83665\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.62166\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.72450\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.67858\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.70022\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.56701\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.51242\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.71202\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.81271\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.70491\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.46623\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.95958\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.31464\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.89530\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.76441\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.55097\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.56513\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.81338\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.93378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 44 with loss 0.65070\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.07267\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.31329\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.78727\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.83539\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.63980\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.37794\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.12799\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.84860\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.94202\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.78899\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.83072\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.48490\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.46691\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.94481\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.39868\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.88488\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.63369\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.80794\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.90327\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.73778\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.85816\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.23377\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.61074\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 1.03866\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.52448\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.08810\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.83377\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.28914\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.55137\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.82768\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.81406\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.78992\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.60073\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 1.04165\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 0.92823\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.78126\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.89741\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.85933\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.67733\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.72525\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.65907\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.61348\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.04744\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 1.19689\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.88807\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 1.00772\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.48881\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.89385\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.91012\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.92520\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.59054\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.04152\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 0.84412\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.63504\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.79054\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8156073959171772\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 16\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.62219\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.79781\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.02187\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.68187\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.98201\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.02074\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.54972\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.65843\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.71239\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.85373\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.52954\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.95959\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.83169\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.69203\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.80571\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.56576\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.75644\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.91274\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.31748\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.69942\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.10398\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.09864\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.41613\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.88065\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.69708\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.65007\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.53234\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.43867\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 1.00337\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.86643\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.54879\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.76987\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.94395\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.64431\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.78402\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.19423\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.71855\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.91026\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.66654\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.51295\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.69790\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.64115\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.93723\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.83309\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.68355\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.02185\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.99604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 47 with loss 1.12386\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.69261\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.65091\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.85985\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.82785\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.84682\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 1.20685\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.85861\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.84011\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.64423\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.81231\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.75928\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.23054\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.89070\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.15489\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.44333\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.83649\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.55257\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.72820\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.33801\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.84248\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.65776\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.74486\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.80494\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.88981\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.34541\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.86145\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.82639\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 1.01840\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.69933\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.57170\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.57462\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.60139\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.17787\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.92188\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.89921\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.70129\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.87717\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.73457\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.82808\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.09405\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.84601\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.57800\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.82703\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.41651\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.49037\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.71755\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.15186\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.73475\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.78873\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 0.70184\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.62503\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.84652\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.818576948940754\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 17\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.55508\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.74849\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.34680\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.67409\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.72164\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.91853\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.61559\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.93612\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.93142\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.70381\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.47970\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.54850\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.04268\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.60647\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.69554\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.71358\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.82161\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.76228\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.22600\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.68765\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.13318\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.96793\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.46232\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.89644\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.90797\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.05112\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.63807\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.77925\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.64221\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.71433\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.55687\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.73808\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.13565\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.89508\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.89833\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.09150\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.12666\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.42614\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.61275\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.91350\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.76685\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.53397\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.57424\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 1.10738\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.43849\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 1.00665\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.58312\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.05736\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.79353\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.54144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 50 with loss 1.02176\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.90969\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.97615\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 1.08811\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.77249\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.75637\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.70488\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.79518\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 1.06190\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.83052\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.54346\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.00662\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.59539\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.75847\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.51150\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.71877\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.62300\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.69760\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.68651\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.71683\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 1.04326\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.85248\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.71248\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.61431\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.98842\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.96899\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.64574\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.69229\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.42370\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.35225\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.91558\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.48318\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.74098\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.44354\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.84987\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.78818\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 1.32587\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.76512\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.92156\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.73428\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.89369\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.52260\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 0.78648\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 1.02062\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.76677\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.72380\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.06497\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 0.91207\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.67662\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.50622\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.816371111869812\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 18\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.61433\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.86050\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.85920\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.69252\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.99412\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.25407\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.75923\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.14479\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.76327\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.75609\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.89606\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.85603\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.22016\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.97459\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.92509\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.74080\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.72338\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.85866\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.00255\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.99188\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.86217\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.64186\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.62658\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.91120\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.56057\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.81850\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.75576\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.96485\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.83775\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.60186\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.47020\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.43669\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.04710\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.26674\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.61414\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.42505\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.44748\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.73159\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.74281\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.86103\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.52159\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.73320\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.25170\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 1.01393\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.78331\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.86186\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.86345\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.05846\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.68648\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.57040\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.15773\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.98373\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.75937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 53 with loss 1.04472\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.70054\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.79937\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.70619\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 1.05791\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.62048\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.75323\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.70430\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.31453\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.54507\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.84049\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.61783\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.51692\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.62902\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.63284\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.87131\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.76776\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.71789\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.74794\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.14408\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.49057\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.90253\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.97164\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.95405\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.65570\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.60580\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.15237\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.07384\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.52494\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.85082\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.70213\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.77406\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.74873\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 1.00790\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.54076\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.73502\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.65030\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.69505\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.38798\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.04633\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 1.13681\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 1.17591\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.93055\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.04280\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.26238\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.38984\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.94319\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8384085780382157\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 19\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 1.04250\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.77212\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 1.24420\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.74762\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.71502\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.17159\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.53128\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.02468\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.91176\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.74927\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.84984\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.53688\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.98716\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.89337\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.59224\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.68635\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.82657\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 1.04194\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.93354\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.61486\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.03540\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.98067\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.65927\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.82778\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 1.07235\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.78924\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.75769\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.46690\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.80887\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.95764\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.56763\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.89081\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.62694\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.94108\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.70781\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.37454\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.00344\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.57544\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.84392\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.80517\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.77191\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.51643\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.82906\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 1.24281\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.84761\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.77964\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.50792\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.87082\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.82951\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.74734\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.04845\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.21519\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.67643\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.69962\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.68424\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.94219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 56 with loss 0.43061\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.67762\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.27717\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.05559\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.53438\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.86167\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.76319\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.68706\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.44080\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.57228\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.59820\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.75561\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.87552\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.58887\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.42797\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.88810\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.16308\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.58377\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.73190\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.96554\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.71649\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.45215\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.57657\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.10489\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.22832\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.63132\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.83568\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.75110\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.82876\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.92055\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.73820\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.12372\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.61402\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.88315\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.69577\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.64021\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.04357\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.83061\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.84034\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.61101\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.44986\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.04023\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.63384\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.95495\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8079878824949265\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 20\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.64302\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.79096\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.95883\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.61483\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.70991\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.93390\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.71730\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.78344\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.67387\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 1.15119\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.64358\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.64492\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.10462\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.50544\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.66746\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.72295\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.87483\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.62449\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.82189\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.99961\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.74995\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.74935\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.65288\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.77193\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.64872\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.97993\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.14228\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.85163\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.87037\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.59314\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.47206\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.84000\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.65221\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.66512\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.89208\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.44894\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.00819\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.57427\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.47782\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.51371\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.83560\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.33161\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 0.95415\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.87361\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.60209\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.96287\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.15132\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.00674\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.63791\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.73877\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.75991\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.94142\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.70804\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 1.13681\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.97879\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.58897\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.59614\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.70565\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.98479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 59 with loss 0.85351\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.89104\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 0.92880\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.58238\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.90038\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.50423\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.67415\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.26781\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.66717\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.79381\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.53597\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.64929\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.49805\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.08520\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.67951\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.92053\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.72904\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.99283\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.86605\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.81335\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 1.18021\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.71544\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.62199\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.74144\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.81744\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.86242\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.91650\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.58419\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.15717\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 1.06838\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.95336\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.71211\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.47433\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.08761\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 1.01036\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.94082\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 1.07073\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.43357\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.00158\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.57354\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.53483\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8092791670560837\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 21\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.58077\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.77172\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.96002\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.73332\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.83352\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 1.10259\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.57941\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 0.86739\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.90336\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.79759\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.71198\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.91533\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.96651\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.54129\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.81480\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.63905\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.69040\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.96461\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.87687\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.79700\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.54412\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.97899\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.34542\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.91706\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.53156\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.74479\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.51548\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.85903\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.62587\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.42705\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.78859\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.68442\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.90692\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.71636\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.69485\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.29308\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 0.93482\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.36068\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.96409\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.55284\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.67340\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.75485\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.07777\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.93483\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.78275\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.85376\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 1.17214\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 1.17108\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.65689\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.79531\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 0.66055\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 1.29893\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 1.00513\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.67609\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.62971\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.93706\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.89069\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.49779\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.77907\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.00375\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.48385\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.06020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 62 with loss 0.70298\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.46790\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.77354\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.87143\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.74145\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.94395\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.83826\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.46365\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.83406\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 1.19612\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.97045\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.47664\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 0.91417\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 1.17114\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.82683\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.62785\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.91227\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 0.77694\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.83591\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.46136\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 1.00077\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.72988\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.96437\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 1.02271\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.67359\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 1.02587\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.69660\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 1.05760\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.68734\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.47560\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.19490\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.98613\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.84820\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.67099\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.87938\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 0.98737\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.52704\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.66993\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8213430425524711\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 22\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.79330\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.76922\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.92145\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.92775\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.93945\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.84296\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.59731\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.03671\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.96467\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.71425\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.75248\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.74288\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.27669\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.86015\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.44168\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.82692\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.57445\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.74052\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 0.98187\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 0.65063\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 0.47157\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 0.52392\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 0.53692\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.78196\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.57282\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 0.59514\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 0.67284\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.48975\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.52074\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.80323\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.83919\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 1.03280\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 0.94671\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.86924\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 1.12097\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.50825\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.29142\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 1.00859\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.66521\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.52679\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.98346\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.45345\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.21561\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.66894\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.50766\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.77097\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.71803\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.96716\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.58618\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.77958\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.02699\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.99099\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.64295\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 1.06727\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.66426\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.53895\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.56203\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.59407\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.35125\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 0.94661\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.73598\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.09647\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.46044\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.44520\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.79707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 65 with loss 0.61961\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.84293\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.69787\n",
      "------------------------------------------\n",
      "Training batch 68 with loss 0.84709\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.61825\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.92331\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 1.08433\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 0.87655\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.64161\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.01671\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.86542\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.92655\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.69764\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.38184\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 0.81999\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 0.77242\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.86743\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.83942\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.78888\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.91133\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.40331\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 0.95403\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.95300\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.97126\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.87806\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 1.11629\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.71380\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.13221\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.57036\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.84311\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.67215\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 0.79970\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.29938\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.80950\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.82790\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.7968817391991615\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 23\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.83853\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 1.03013\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.87860\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.61044\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.78190\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.95402\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.62464\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.15642\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 0.83669\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 1.05031\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.68464\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 0.63300\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 1.02131\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.93844\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.66764\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.55481\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.68335\n",
      "------------------------------------------\n",
      "Training batch 17 with loss 0.88237\n",
      "------------------------------------------\n",
      "Training batch 18 with loss 1.02423\n",
      "------------------------------------------\n",
      "Training batch 19 with loss 1.06377\n",
      "------------------------------------------\n",
      "Training batch 20 with loss 1.32234\n",
      "------------------------------------------\n",
      "Training batch 21 with loss 1.14286\n",
      "------------------------------------------\n",
      "Training batch 22 with loss 1.00504\n",
      "------------------------------------------\n",
      "Training batch 23 with loss 0.89870\n",
      "------------------------------------------\n",
      "Training batch 24 with loss 0.68887\n",
      "------------------------------------------\n",
      "Training batch 25 with loss 1.11711\n",
      "------------------------------------------\n",
      "Training batch 26 with loss 1.35811\n",
      "------------------------------------------\n",
      "Training batch 27 with loss 0.88237\n",
      "------------------------------------------\n",
      "Training batch 28 with loss 0.52056\n",
      "------------------------------------------\n",
      "Training batch 29 with loss 0.85627\n",
      "------------------------------------------\n",
      "Training batch 30 with loss 0.83926\n",
      "------------------------------------------\n",
      "Training batch 31 with loss 0.73218\n",
      "------------------------------------------\n",
      "Training batch 32 with loss 1.21817\n",
      "------------------------------------------\n",
      "Training batch 33 with loss 0.81958\n",
      "------------------------------------------\n",
      "Training batch 34 with loss 0.90484\n",
      "------------------------------------------\n",
      "Training batch 35 with loss 1.18807\n",
      "------------------------------------------\n",
      "Training batch 36 with loss 1.10551\n",
      "------------------------------------------\n",
      "Training batch 37 with loss 0.84543\n",
      "------------------------------------------\n",
      "Training batch 38 with loss 0.90393\n",
      "------------------------------------------\n",
      "Training batch 39 with loss 0.86957\n",
      "------------------------------------------\n",
      "Training batch 40 with loss 0.82923\n",
      "------------------------------------------\n",
      "Training batch 41 with loss 0.73740\n",
      "------------------------------------------\n",
      "Training batch 42 with loss 1.16478\n",
      "------------------------------------------\n",
      "Training batch 43 with loss 0.68111\n",
      "------------------------------------------\n",
      "Training batch 44 with loss 0.70058\n",
      "------------------------------------------\n",
      "Training batch 45 with loss 0.92017\n",
      "------------------------------------------\n",
      "Training batch 46 with loss 0.72274\n",
      "------------------------------------------\n",
      "Training batch 47 with loss 0.92286\n",
      "------------------------------------------\n",
      "Training batch 48 with loss 0.39841\n",
      "------------------------------------------\n",
      "Training batch 49 with loss 0.70001\n",
      "------------------------------------------\n",
      "Training batch 50 with loss 1.04616\n",
      "------------------------------------------\n",
      "Training batch 51 with loss 0.86689\n",
      "------------------------------------------\n",
      "Training batch 52 with loss 0.53462\n",
      "------------------------------------------\n",
      "Training batch 53 with loss 0.93385\n",
      "------------------------------------------\n",
      "Training batch 54 with loss 0.83220\n",
      "------------------------------------------\n",
      "Training batch 55 with loss 0.90757\n",
      "------------------------------------------\n",
      "Training batch 56 with loss 0.56024\n",
      "------------------------------------------\n",
      "Training batch 57 with loss 0.48264\n",
      "------------------------------------------\n",
      "Training batch 58 with loss 0.71260\n",
      "------------------------------------------\n",
      "Training batch 59 with loss 1.20927\n",
      "------------------------------------------\n",
      "Training batch 60 with loss 0.79720\n",
      "------------------------------------------\n",
      "Training batch 61 with loss 1.22145\n",
      "------------------------------------------\n",
      "Training batch 62 with loss 0.64472\n",
      "------------------------------------------\n",
      "Training batch 63 with loss 0.80528\n",
      "------------------------------------------\n",
      "Training batch 64 with loss 0.71774\n",
      "------------------------------------------\n",
      "Training batch 65 with loss 0.85050\n",
      "------------------------------------------\n",
      "Training batch 66 with loss 0.73488\n",
      "------------------------------------------\n",
      "Training batch 67 with loss 0.72369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training batch 68 with loss 0.89828\n",
      "------------------------------------------\n",
      "Training batch 69 with loss 0.71071\n",
      "------------------------------------------\n",
      "Training batch 70 with loss 0.75061\n",
      "------------------------------------------\n",
      "Training batch 71 with loss 0.80523\n",
      "------------------------------------------\n",
      "Training batch 72 with loss 1.02455\n",
      "------------------------------------------\n",
      "Training batch 73 with loss 0.82224\n",
      "------------------------------------------\n",
      "Training batch 74 with loss 1.11990\n",
      "------------------------------------------\n",
      "Training batch 75 with loss 0.97948\n",
      "------------------------------------------\n",
      "Training batch 76 with loss 0.59972\n",
      "------------------------------------------\n",
      "Training batch 77 with loss 0.76717\n",
      "------------------------------------------\n",
      "Training batch 78 with loss 0.56173\n",
      "------------------------------------------\n",
      "Training batch 79 with loss 0.78738\n",
      "------------------------------------------\n",
      "Training batch 80 with loss 1.00530\n",
      "------------------------------------------\n",
      "Training batch 81 with loss 0.67211\n",
      "------------------------------------------\n",
      "Training batch 82 with loss 0.88373\n",
      "------------------------------------------\n",
      "Training batch 83 with loss 0.67633\n",
      "------------------------------------------\n",
      "Training batch 84 with loss 0.89286\n",
      "------------------------------------------\n",
      "Training batch 85 with loss 0.63939\n",
      "------------------------------------------\n",
      "Training batch 86 with loss 1.21348\n",
      "------------------------------------------\n",
      "Training batch 87 with loss 0.77273\n",
      "------------------------------------------\n",
      "Training batch 88 with loss 0.87656\n",
      "------------------------------------------\n",
      "Training batch 89 with loss 0.66583\n",
      "------------------------------------------\n",
      "Training batch 90 with loss 0.82028\n",
      "------------------------------------------\n",
      "Training batch 91 with loss 0.51950\n",
      "------------------------------------------\n",
      "Training batch 92 with loss 1.13455\n",
      "------------------------------------------\n",
      "Training batch 93 with loss 0.72108\n",
      "------------------------------------------\n",
      "Training batch 94 with loss 0.93805\n",
      "------------------------------------------\n",
      "Training batch 95 with loss 0.53697\n",
      "------------------------------------------\n",
      "Training batch 96 with loss 1.11553\n",
      "------------------------------------------\n",
      "Training batch 97 with loss 1.51127\n",
      "------------------------------------------\n",
      "Training batch 98 with loss 0.52883\n",
      "------------------------------------------\n",
      "Training batch 99 with loss 0.94855\n",
      "****************************************\n",
      "\n",
      "Total Loss: 0.8539274650812149\n",
      "Model recorded successfully!\n",
      "Model saved successfully!\n",
      "Epoch: 24\n",
      "------------------------------------------\n",
      "Training batch 0 with loss 0.65014\n",
      "------------------------------------------\n",
      "Training batch 1 with loss 0.83320\n",
      "------------------------------------------\n",
      "Training batch 2 with loss 0.89708\n",
      "------------------------------------------\n",
      "Training batch 3 with loss 0.73959\n",
      "------------------------------------------\n",
      "Training batch 4 with loss 0.75691\n",
      "------------------------------------------\n",
      "Training batch 5 with loss 0.91277\n",
      "------------------------------------------\n",
      "Training batch 6 with loss 0.73243\n",
      "------------------------------------------\n",
      "Training batch 7 with loss 1.28065\n",
      "------------------------------------------\n",
      "Training batch 8 with loss 1.10429\n",
      "------------------------------------------\n",
      "Training batch 9 with loss 0.38484\n",
      "------------------------------------------\n",
      "Training batch 10 with loss 0.54474\n",
      "------------------------------------------\n",
      "Training batch 11 with loss 1.18146\n",
      "------------------------------------------\n",
      "Training batch 12 with loss 0.80096\n",
      "------------------------------------------\n",
      "Training batch 13 with loss 0.80441\n",
      "------------------------------------------\n",
      "Training batch 14 with loss 0.79642\n",
      "------------------------------------------\n",
      "Training batch 15 with loss 0.84409\n",
      "------------------------------------------\n",
      "Training batch 16 with loss 0.78894\n"
     ]
    }
   ],
   "source": [
    "trainer.run(dl_train, 1000, path_record=RECORDS_PATH, path_model=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.writer.add_scalar('Test', 12, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
